<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/yuzhouwan_logo_with_copyright.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/yuzhouwan_logo_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/yuzhouwan_logo_16x16.ico">
  <link rel="mask-icon" href="/yuzhouwan_logo_with_copyright.jpg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yuzhouwan.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":5,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":{"valine":{"text":"Valine：匿名","order":-1},"disqus":{"text":"Disqus：海外","order":-2},"gitalk":{"text":"Gitalk：可用 Github 账号登录","order":-3}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Kafka 是什么？　Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. 为什么要有 Kafka?分布式　具备经济、快速、可靠、易扩充、数据共享、设备共享、通">
<meta property="og:type" content="article">
<meta property="og:title" content="Apache Kafka 分布式消息队列框架">
<meta property="og:url" content="https://yuzhouwan.com/posts/26002/">
<meta property="og:site_name" content="宇宙湾">
<meta property="og:description" content="Kafka 是什么？　Kafka is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design. 为什么要有 Kafka?分布式　具备经济、快速、可靠、易扩充、数据共享、设备共享、通">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_consumer_groups.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_configured_clusters.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_dashboard.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_new_connector.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_new_connector_config.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_new_connector_created.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_new_connector_detail.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_ui_file_source_and_sink.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_topics_ui_list.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_schema_ui_list.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_architecture.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/kafka/kafka_connect_process_flow_kafka_2_hdfs.png">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg">
<meta property="article:published_time" content="2015-05-10T04:43:42.000Z">
<meta property="article:modified_time" content="2021-01-31T04:16:39.898Z">
<meta property="article:author" content="Benedict Jin">
<meta property="article:tag" content="Apache Storm">
<meta property="article:tag" content="Apache Kafka">
<meta property="article:tag" content="Docker">
<meta property="article:tag" content="Gradle">
<meta property="article:tag" content="HDFS">
<meta property="article:tag" content="Apache Druid">
<meta property="article:tag" content="ElasticSearch">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Message Queue">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuzhouwan.com/picture/kafka/kafka_consumer_groups.png">

<link rel="canonical" href="https://yuzhouwan.com/posts/26002/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Apache Kafka 分布式消息队列框架 | 宇宙湾</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97020848-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-97020848-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.null { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .null > span { position: relative; z-index: 10; }  .null img, .null .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .null img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .null-fallback { color: inherit; } .null-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="宇宙湾" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">宇宙湾</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">厚积薄发</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">147</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">54</span></a>

  </li>
        <li class="menu-item menu-item-书单">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>书单</a>

  </li>
        <li class="menu-item menu-item-影视">

    <a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>影视</a>

  </li>
        <li class="menu-item menu-item-友链">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuzhouwan.com/posts/26002/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/yuzhouwan_logo_128x128.ico">
      <meta itemprop="name" content="Benedict Jin">
      <meta itemprop="description" content="Benedict Jin's Blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="宇宙湾">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Apache Kafka 分布式消息队列框架
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2015-05-10 12:43:42" itemprop="dateCreated datePublished" datetime="2015-05-10T12:43:42+08:00">2015-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-31 12:16:39" itemprop="dateModified" datetime="2021-01-31T12:16:39+08:00">2021-01-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>73k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1:07</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Kafka-是什么？"><a href="#Kafka-是什么？" class="headerlink" title="Kafka 是什么？"></a>Kafka 是什么？</h2><p>　<strong>Kafka</strong> is a distributed, partitioned, replicated commit log service. It provides the functionality of a messaging system, but with a unique design.</p>
<h2 id="为什么要有-Kafka"><a href="#为什么要有-Kafka" class="headerlink" title="为什么要有 Kafka?"></a>为什么要有 Kafka?</h2><h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h3><p>　具备经济、快速、可靠、易扩充、数据共享、设备共享、通讯方便、灵活等，分布式所具备的特性</p>
<h3 id="高吞吐量"><a href="#高吞吐量" class="headerlink" title="高吞吐量"></a>高吞吐量</h3><p>　同时为数据生产者和消费者提高吞吐量</p>
<h3 id="高可靠性"><a href="#高可靠性" class="headerlink" title="高可靠性"></a>高可靠性</h3><p>　支持多个消费者，当某个消费者失败的时候，能够自动负载均衡</p>
<h3 id="离线-amp-实时性"><a href="#离线-amp-实时性" class="headerlink" title="离线 &amp; 实时性"></a>离线 &amp; 实时性</h3><p>　能将消息持久化，进行批量处理</p>
<h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>　作为各个系统连接的桥梁，避免系统之间的耦合</p>
<span id="more"></span>
<h2 id="Kafka-工作机制"><a href="#Kafka-工作机制" class="headerlink" title="Kafka 工作机制"></a>Kafka 工作机制</h2><h3 id="一些主要概念"><a href="#一些主要概念" class="headerlink" title="一些主要概念"></a>一些主要概念</h3><ul>
<li><p>Topic（主题）<br> A <strong>topic</strong> is a category or feed name to which messages are published.</p>
</li>
<li><p>Producers（发布者）<br> <strong>Producers</strong> publish data to the topics of their choice. The producer is responsible for choosing which message to assign to which partition within the topic.</p>
</li>
<li><p>Consumers（订阅者）<br> <strong>Consumers</strong> label themselves with a <em>consumer group</em> name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines.</p>
</li>
</ul>
<h3 id="示意图"><a href="#示意图" class="headerlink" title="示意图"></a>示意图</h3><p><img data-src="/picture/kafka/kafka_consumer_groups.png" alt="Kafka Consumer Groups"></p>
<center>（图片来源：<a href="https://kafka.apache.org/intro" target="_blank" rel="external nofollow noopener noreferrer">Kafka</a>™ 官网）</center>



<h2 id="Kafka-Connect"><a href="#Kafka-Connect" class="headerlink" title="Kafka Connect"></a>Kafka Connect</h2><h3 id="Kafka-Connect-是什么？"><a href="#Kafka-Connect-是什么？" class="headerlink" title="Kafka Connect 是什么？"></a>Kafka Connect 是什么？</h3><p>　<strong>Kafka Connect</strong> 是一款可扩展且稳定的、可在 Apache Kafka 和其他系统之间进行数据传输的框架。能够快速定义，将大量数据导入导出 Kafka 的连接器。Source Connector 可以接受整个数据库，将表转化为 Stream 更新到 Kafka Topic 中。也支持将应用服务器的指标收集到 Kafka Topic，使得数据可用于低延迟场景的<strong>流处理</strong>。Sink Connector 则可以将数据从 Kafka Topic 传送到搜索引擎（如 <a href="https://yuzhouwan.com/posts/22654/">ElasticSearch</a>）或<strong>离线分析</strong>系统（如 <a href="https://yuzhouwan.com/posts/60504/">Hadoop</a>）</p>
<p><img data-src="/picture/kafka/kafka_connect.png" alt="Kafka Connect"></p>
<center>（图片来源：<a href="https://kafka.apache.org/intro" target="_blank" rel="external nofollow noopener noreferrer">Kafka</a>™ 官网）</center>



<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li>Kafka Connector 通用框架，提供统一的 API 集成接口</li>
<li>支持单机和分布式模式</li>
<li>提供 <a href="https://yuzhouwan.com/posts/26002/#RESTful-接口">RESTful 接口</a>，用来查看和管理 Kafka Connectors</li>
<li>自动化的 Offset 管理</li>
<li>分布式、可扩展，基于现有的 Group 管理协议，可以通过增加 Task / Worker 实现动态扩展</li>
<li>更方便集成其他 流 / 批 处理系统</li>
<li>丰富的 Metrics 监控指标</li>
<li>支持 C / C++ / Go / <a href="https://yuzhouwan.com/posts/27328/">Java</a> / JMS / .Net / <a href="https://yuzhouwan.com/posts/43687/">Python</a> 等多种语言的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/clients/">客户端</a></li>
</ul>
<h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><ul>
<li>只专注于可靠、可扩展地<strong>同步数据</strong>（将转换、抽取等任务交给专门的数据处理框架）</li>
<li>尽可能地保持<strong>粗粒度</strong>（比如，同步数据库时，是将整个数据库作为默认的处理单元，而不是一张张表）</li>
<li><strong>并行</strong>地数据同步（支持数据处理能力的自动扩展）</li>
<li>支持 <strong>exactly-once</strong> 强语义（包括，类似 HDFS 这样没有主键用于区分重复的存储系统）</li>
<li>当源系统提供了数据结构和类型之后，需要在数据同步过程中保存<strong>元数据</strong></li>
<li>API 定义需要<strong>简洁</strong>、可重用、易于理解，方便实现自定义的 Connector</li>
<li>同时支持<strong>单机</strong>开发测试，也支持<strong>分布式</strong>生产应用</li>
</ul>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="Connector"><a href="#Connector" class="headerlink" title="Connector"></a>Connector</h4><p>　<strong>Connector</strong> 是为了方便协调数据流任务，而提出的高层抽象</p>
<h4 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h4><p>　具体实现数据从 Kafka 导入导出的功能</p>
<p>　同时具备 <strong>Task</strong> 自动容灾的能力（不过 Worker 本身<a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/userguide.html#deployment-considerations">不支持</a> 自动重启和扩容进程资源的功能）</p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>　执行 Connector 和 Task 的进程</p>
<h4 id="Converter"><a href="#Converter" class="headerlink" title="Converter"></a>Converter</h4><p>　用于转化 Kafka Connect 和 其他系统之间交互的数据格式</p>
<h4 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h4><p>　对 Connector 接收和发送的数据进行一些简单的处理逻辑</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/design.html#architecture">架构</a></h3><h4 id="Connector-model"><a href="#Connector-model" class="headerlink" title="Connector model"></a><strong>Connector model</strong></h4><p>　Connector 模型，定义了 Kafka Connector 与外部系统的操作接口，包括 Connector 和 Task</p>
<p>　Connector 是一个指定的 <code>Connector</code> 实现类，配置了需要拷贝哪些数据，以及如何处理数据的相关属性。Connector 实例由一组 Tasks 组成。Kafka Connect 实际管理 Tasks，Connector 只负责生成 Tasks，并在框架指定更新的时候，完成配置更改。<code>Source</code> 端 / <code>Sink</code> 端 的 <code>Connectors</code>/<code>Tasks</code> API 接口是独立定义的，主要为了能够保证 API 定义的简洁性</p>
<h4 id="Worker-model"><a href="#Worker-model" class="headerlink" title="Worker model"></a><strong>Worker model</strong></h4><p>　Worker 模型，管理 Connector 和 Task 的生命周期，包括 启动、暂停、恢复、重启、停止 等</p>
<p>　Kafka Connect 群集由一组 Worker 进程组成，这些进程是执行 Connector 和 Task 的容器。Worker 自动和其他分布式的 Worker 协调工作，提供可扩展性和容错性。同时 Worker 进程的<strong>资源管理</strong>可以托管于 Yarn / Mesos，<strong>配置管理</strong>可以与 Chef / Puppet 框架整合，<strong>生命周期管理</strong>也可以使用 Oozie / <a target="_blank" rel="external nofollow noopener noreferrer" href="http://falcon.apache.org/FalconDocumentation.html">Falcon</a> 等</p>
<h4 id="Data-model"><a href="#Data-model" class="headerlink" title="Data model"></a><strong>Data model</strong></h4><p>　数据模型，定义了 Kafka Connector 管理的数据结构、记录的序列化</p>
<p>　Connectors 复制数据流从一个 partitioned 输入流到一个 partitioned 输出流，其中输入输出端至少有一个总是 Kafka。每一个 Stream 都是一个有序的数据流，流里面的消息都有一个对应的偏移量。这些偏移量的格式和语义由 Connector 定义，以支持与各种系统的集成；然而，为了在故障的情况下实现某些传递语义，需要确保 Stream 内的偏移是唯一的，并且流可以 seek 任意的偏移。同时，Kafka Connect 支持插件式的转换器，以便支持各种序列化格式，来存储这些数据。另外，Schema 是内置的，使得关于数据格式的重要元数据，得以在复杂的数据管道中传播。但是，当 Schema 不可用时，也可以使用无模式的数据</p>
<h3 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h3><h4 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 增加用户，并赋予其密码</span></span><br><span class="line">$ adduser connect</span><br><span class="line">$ passwd connect            <span class="comment"># ur password for connect user</span></span><br><span class="line"><span class="comment"># 赋予用户可以 sudo 的权限</span></span><br><span class="line">$ chmod u+w /etc/sudoers</span><br><span class="line">$ vim /etc/sudoers</span><br><span class="line">  <span class="comment"># 找到 `root ALL=(ALL) ALL` 这行，并在下面添加 connect 用户</span></span><br><span class="line">  connect    ALL=(ALL)    ALL</span><br><span class="line"></span><br><span class="line">$ chmod u-w /etc/sudoers</span><br><span class="line"><span class="comment"># 切换到 connect 用户</span></span><br><span class="line">$ su - connect</span><br><span class="line">$ <span class="built_in">cd</span> /home/connect</span><br><span class="line"><span class="comment"># 存放软件目录 &amp; 安装目录 &amp; 日志目录</span></span><br><span class="line">$ mkdir install &amp;&amp; mkdir software &amp;&amp; mkdir logs</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Confluent"><a href="#Confluent" class="headerlink" title="Confluent"></a>Confluent</h4><h5 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h5><p>　在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/download/">https://www.confluent.io/download/</a> 页面中下载 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://packages.confluent.io/archive/3.3/confluent-oss-3.3.1-2.11.tar.gz">confluent-oss-3.3.1-2.11.tar.gz</a> 安装包</p>
<h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/install/</span><br><span class="line">$ tar zxvf confluent-oss-3.3.1-2.11.tar.gz -C ~/software/</span><br><span class="line">$ <span class="built_in">cd</span> ~/software/</span><br><span class="line">$ ln -s confluent-3.3.1/ confluent</span><br><span class="line">$ vim ~/.bashrc</span><br><span class="line">  <span class="built_in">export</span> CONFLUENT_HOME=/home/connect/software/confluent</span><br><span class="line">  <span class="built_in">export</span> PATH=<span class="variable">$CONFLUENT_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></tbody></table></figure>
<h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动 ZooKeeper, Kafka, Schema Registry</span></span><br><span class="line">$ confluent start schema-registry</span><br><span class="line">  Starting zookeeper</span><br><span class="line">  zookeeper is [UP]</span><br><span class="line">  Starting kafka</span><br><span class="line">  kafka is [UP]</span><br><span class="line">  Starting schema-registry</span><br><span class="line">  schema-registry is [UP]</span><br><span class="line"></span><br><span class="line">$ jps -ml</span><br><span class="line">  32680 org.apache.zookeeper.server.quorum.QuorumPeerMain /tmp/confluent.A8TzcjSE/zookeeper/zookeeper.properties</span><br><span class="line">  348 io.confluent.support.metrics.SupportedKafka /tmp/confluent.A8TzcjSE/kafka/kafka.properties</span><br><span class="line">  483 io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain /tmp/confluent.A8TzcjSE/schema-registry/schema-registry.properties</span><br></pre></td></tr></tbody></table></figure>
<h5 id="发送-avro-数据"><a href="#发送-avro-数据" class="headerlink" title="发送 avro 数据"></a>发送 avro 数据</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/software/confluent/</span><br><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic <span class="built_in">test</span> --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 输入以下三行 JSON 串</span></span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value3"</span>}</span><br><span class="line">  <span class="comment"># Ctrl+C 停止进程</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="接受-avro-数据"><a href="#接受-avro-数据" class="headerlink" title="接受 avro 数据"></a>接受 avro 数据</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/kafka-avro-console-consumer --topic <span class="built_in">test</span> --zookeeper localhost:2181 --from-beginning</span><br><span class="line">  <span class="comment"># 接收到 producer 发送的三行 JSON 串</span></span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value3"</span>}</span><br><span class="line">  <span class="comment"># Ctrl+C 停止进程</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="发送数据格式不对的-avro-数据"><a href="#发送数据格式不对的-avro-数据" class="headerlink" title="发送数据格式不对的 avro 数据"></a>发送数据格式不对的 avro 数据</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic <span class="built_in">test</span> --property value.schema=<span class="string">'{"type":"int"}'</span></span><br><span class="line">  <span class="comment"># 输入以下一行 JSON 串</span></span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value1"</span>}</span><br><span class="line">  <span class="comment"># 获得如下报错</span></span><br><span class="line">  org.apache.kafka.common.errors.SerializationException: Error deserializing json {<span class="string">"f1"</span>:<span class="string">"value1"</span>} to Avro of schema <span class="string">"int"</span></span><br><span class="line">  Caused by: org.apache.avro.AvroTypeException: Expected int. Got START_OBJECT</span><br><span class="line">      at org.apache.avro.io.JsonDecoder.error(JsonDecoder.java:698)</span><br><span class="line">      at org.apache.avro.io.JsonDecoder.readInt(JsonDecoder.java:172)</span><br><span class="line">      at org.apache.avro.io.ValidatingDecoder.readInt(ValidatingDecoder.java:83)</span><br><span class="line">      at org.apache.avro.generic.GenericDatumReader.readInt(GenericDatumReader.java:503)</span><br><span class="line">      at org.apache.avro.generic.GenericDatumReader.readWithoutConversion(GenericDatumReader.java:183)</span><br><span class="line">      at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:153)</span><br><span class="line">      at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:145)</span><br><span class="line">      at io.confluent.kafka.formatter.AvroMessageReader.jsonToAvro(AvroMessageReader.java:191)</span><br><span class="line">      at io.confluent.kafka.formatter.AvroMessageReader.readMessage(AvroMessageReader.java:158)</span><br><span class="line">      at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:58)</span><br><span class="line">      at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ confluent stop schema-registry</span><br><span class="line">  Stopping connect</span><br><span class="line">  connect is [DOWN]</span><br><span class="line">  Stopping kafka-rest</span><br><span class="line">  kafka-rest is [DOWN]</span><br><span class="line">  Stopping schema-registry</span><br><span class="line">  schema-registry is [DOWN]</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Kafka-Connect-1"><a href="#Kafka-Connect-1" class="headerlink" title="Kafka Connect"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/connect-hdfs/docs/hdfs_connector.html">Kafka Connect</a></h4><h5 id="启动-confluent"><a href="#启动-confluent" class="headerlink" title="启动 confluent"></a>启动 confluent</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ confluent start</span><br><span class="line">  zookeeper is already running. Try restarting <span class="keyword">if</span> needed</span><br><span class="line">  kafka is already running. Try restarting <span class="keyword">if</span> needed</span><br><span class="line">  Starting schema-registry</span><br><span class="line">  schema-registry is [UP]</span><br><span class="line">  Starting kafka-rest</span><br><span class="line">  kafka-rest is [UP]</span><br><span class="line">  Starting connect</span><br><span class="line">  connect is [UP]</span><br></pre></td></tr></tbody></table></figure>
<h5 id="查看-connect-日志"><a href="#查看-connect-日志" class="headerlink" title="查看 connect 日志"></a>查看 connect 日志</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ confluent <span class="built_in">log</span> connect</span><br><span class="line">$ confluent current</span><br><span class="line">  /tmp/confluent.A8TzcjSE</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /tmp/confluent.A8TzcjSE</span><br><span class="line">$ less connect/connect.stderr</span><br></pre></td></tr></tbody></table></figure>
<h5 id="查看支持的-connect-类型"><a href="#查看支持的-connect-类型" class="headerlink" title="查看支持的 connect 类型"></a>查看支持的 connect 类型</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ confluent list connectors</span><br><span class="line">  Bundled Predefined Connectors (edit configuration under etc/):</span><br><span class="line">    elasticsearch-sink</span><br><span class="line">    file-source</span><br><span class="line">    file-sink</span><br><span class="line">    jdbc-source</span><br><span class="line">    jdbc-sink</span><br><span class="line">    hdfs-sink</span><br><span class="line">    s3-sink</span><br><span class="line"></span><br><span class="line">$ ll etc/</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 08:07 camus</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:41 confluent-common</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:28 kafka</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:50 kafka-connect-elasticsearch</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:58 kafka-connect-hdfs</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 08:06 kafka-connect-jdbc</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 08:04 kafka-connect-s3</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:52 kafka-connect-storage-common</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:48 kafka-rest</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:42 rest-utils</span><br><span class="line">  drwxr-xr-x 2 connect connect 4096 Jul 28 07:45 schema-registry</span><br></pre></td></tr></tbody></table></figure>
<h5 id="使用-file-source"><a href="#使用-file-source" class="headerlink" title="使用 file-source"></a>使用 file-source</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 file-source</span></span><br><span class="line">$ cat ./etc/kafka/connect-file-source.properties</span><br><span class="line">  name=<span class="built_in">local</span>-file-source</span><br><span class="line">  connector.class=FileStreamSource</span><br><span class="line">  tasks.max=1</span><br><span class="line">  file=test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不需要 Schema Registry 功能，需要同时制定 key.converter 和 value.converter 参数为org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 往 test.txt 里面写入测试数据</span></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> {1..3}; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"log line <span class="variable">$i</span>"</span>; <span class="keyword">done</span> &gt; test.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 装载 file-source 连接器，并确认 file-source 的配置</span></span><br><span class="line">$ confluent load file-source</span><br><span class="line">  {<span class="string">"name"</span>:<span class="string">"file-source"</span>,<span class="string">"config"</span>:{<span class="string">"connector.class"</span>:<span class="string">"FileStreamSource"</span>,<span class="string">"tasks.max"</span>:<span class="string">"1"</span>,<span class="string">"file"</span>:<span class="string">"test.txt"</span>,<span class="string">"topic"</span>:<span class="string">"connect-test"</span>,<span class="string">"name"</span>:<span class="string">"file-source"</span>},<span class="string">"tasks"</span>:[]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认 connecter 是否已经装载</span></span><br><span class="line">$ confluent status connectors</span><br><span class="line">  [<span class="string">"file-source"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 file-source 连接器的运行状态</span></span><br><span class="line">$ confluent status file-source</span><br><span class="line">  {<span class="string">"name"</span>:<span class="string">"file-source"</span>,<span class="string">"connector"</span>:{<span class="string">"state"</span>:<span class="string">"RUNNING"</span>,<span class="string">"worker_id"</span>:<span class="string">"192.168.1.101:8083"</span>},<span class="string">"tasks"</span>:[{<span class="string">"state"</span>:<span class="string">"RUNNING"</span>,<span class="string">"id"</span>:0,<span class="string">"worker_id"</span>:<span class="string">"192.168.1.101:8083"</span>}]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 Kafka 是否接收到数据</span></span><br><span class="line">$ kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic connect-test --from-beginning</span><br><span class="line">  <span class="string">"log line 1"</span></span><br><span class="line">  <span class="string">"log line 2"</span></span><br><span class="line">  <span class="string">"log line 3"</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="使用-file-sink"><a href="#使用-file-sink" class="headerlink" title="使用 file-sink"></a>使用 file-sink</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 file-sink</span></span><br><span class="line">$ cat ./etc/kafka/connect-file-sink.properties</span><br><span class="line">  name=<span class="built_in">local</span>-file-sink</span><br><span class="line">  connector.class=FileStreamSink</span><br><span class="line">  tasks.max=1</span><br><span class="line">  file=test.sink.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 装载 file-sink 连接器，并确认 file-sink 的配置</span></span><br><span class="line">$ confluent load file-sink</span><br><span class="line">  {<span class="string">"name"</span>:<span class="string">"file-sink"</span>,<span class="string">"config"</span>:{<span class="string">"connector.class"</span>:<span class="string">"FileStreamSink"</span>,<span class="string">"tasks.max"</span>:<span class="string">"1"</span>,<span class="string">"file"</span>:<span class="string">"test.sink.txt"</span>,<span class="string">"topics"</span>:<span class="string">"connect-test"</span>,<span class="string">"name"</span>:<span class="string">"file-sink"</span>},<span class="string">"tasks"</span>:[]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认 connecter 是否已经装载</span></span><br><span class="line">$ confluent status connectors</span><br><span class="line">  [<span class="string">"file-source"</span>,<span class="string">"file-sink"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 file-sink 连接器的运行状态</span></span><br><span class="line">$ confluent status file-sink</span><br><span class="line">  {<span class="string">"name"</span>:<span class="string">"file-sink"</span>,<span class="string">"connector"</span>:{<span class="string">"state"</span>:<span class="string">"RUNNING"</span>,<span class="string">"worker_id"</span>:<span class="string">"192.168.1.101:8083"</span>},<span class="string">"tasks"</span>:[{<span class="string">"state"</span>:<span class="string">"RUNNING"</span>,<span class="string">"id"</span>:0,<span class="string">"worker_id"</span>:<span class="string">"192.168.1.101:8083"</span>}]}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 Kafka 数据是否已经写入到文件</span></span><br><span class="line">$ tail -f test.sink.txt</span><br><span class="line">  <span class="built_in">log</span> line 1</span><br><span class="line">  <span class="built_in">log</span> line 2</span><br><span class="line">  <span class="built_in">log</span> line 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 另起窗口，并执行写入 test.txt 数据的脚本，可以看到对应 test.sink.txt 里面已经被写入数据</span></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> {4..1000}; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"log line <span class="variable">$i</span>"</span>; <span class="keyword">done</span> &gt;&gt; test.txt</span><br></pre></td></tr></tbody></table></figure>
<h5 id="清理工作"><a href="#清理工作" class="headerlink" title="清理工作"></a>清理工作</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卸载 file-source / file-sink connector</span></span><br><span class="line">$ confluent unload file-source</span><br><span class="line">$ confluent unload file-sink</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭 connect 进程</span></span><br><span class="line">$ confluent stop connect</span><br><span class="line">  Stopping connect</span><br><span class="line">  connect is [DOWN]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止所有 confluent 进程</span></span><br><span class="line">$ confluent stop</span><br><span class="line">  Stopping connect</span><br><span class="line">  connect is [DOWN]</span><br><span class="line">  Stopping kafka-rest</span><br><span class="line">  kafka-rest is [DOWN]</span><br><span class="line">  Stopping schema-registry</span><br><span class="line">  schema-registry is [DOWN]</span><br><span class="line">  Stopping kafka</span><br><span class="line">  kafka is [DOWN]</span><br><span class="line">  Stopping zookeeper</span><br><span class="line">  zookeeper is [DOWN]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止所有 confluent 进程，并删除临时文件</span></span><br><span class="line">$ confluent destroy</span><br><span class="line">  Stopping connect</span><br><span class="line">  connect is [DOWN]</span><br><span class="line">  Stopping kafka-rest</span><br><span class="line">  kafka-rest is [DOWN]</span><br><span class="line">  Stopping schema-registry</span><br><span class="line">  schema-registry is [DOWN]</span><br><span class="line">  Stopping kafka</span><br><span class="line">  kafka is [DOWN]</span><br><span class="line">  Stopping zookeeper</span><br><span class="line">  zookeeper is [DOWN]</span><br><span class="line">  Deleting: /tmp/confluent.A8TzcjSE</span><br></pre></td></tr></tbody></table></figure>
<h3 id="支持的组件"><a href="#支持的组件" class="headerlink" title="支持的组件"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/product/connectors/">支持的组件</a></h3><h4 id="Kafka-2-HDFS"><a href="#Kafka-2-HDFS" class="headerlink" title="Kafka 2 HDFS"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/kafka-connect-hdfs">Kafka 2 HDFS</a></h4><h5 id="特性-1"><a href="#特性-1" class="headerlink" title="特性"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/connect-hdfs/docs/hdfs_connector.html#features">特性</a></h5><h6 id="Exactly-Once-Delivery"><a href="#Exactly-Once-Delivery" class="headerlink" title="Exactly Once Delivery"></a><strong>Exactly Once Delivery</strong></h6><p>　Connector 使用 WAL 文件保证每条导入到 HDFS 的数据都是<strong>有且仅有一条</strong>的<br>　通过将 Kafka offset 信息编码到 WAL 文件中，就可以在 task 失败或者重启的时候，获取到最后一条提交的 offset 了</p>
<h6 id="Extensible-Data-Format"><a href="#Extensible-Data-Format" class="headerlink" title="Extensible Data Format"></a><strong>Extensible Data Format</strong></h6><p>　原生支持 <code>Avro</code> / <code>Parquet</code> 格式，其他数据格式，可以通过扩展 <code>Format</code> 类获得支持</p>
<h6 id="Hive-Integration"><a href="#Hive-Integration" class="headerlink" title="Hive Integration"></a><strong>Hive Integration</strong></h6><p>　支持 Hive 的整合<br>　激活该功能后，Connector 会自动给每一个导入 HDFS 的 topic 创建 Hive 的外部分区表</p>
<h6 id="Schema-Evolution"><a href="#Schema-Evolution" class="headerlink" title="Schema Evolution"></a><strong>Schema Evolution</strong></h6><p>　支持模式演进和不同的 Schema 兼容级别<br>　整合 Hive 时，支持给 <code>schema.compatibility</code> 配置 <code>BACKWARD</code> / <code>FORWARD</code> / <code>FULL</code> 三种模式<br>　Hive 表可以查询同一个 Topic 下，以不同 Schema 写入的所有表数据</p>
<h6 id="Secure-HDFS-and-Hive-Metastore-Support"><a href="#Secure-HDFS-and-Hive-Metastore-Support" class="headerlink" title="Secure HDFS and Hive Metastore Support"></a><strong>Secure HDFS and Hive Metastore Support</strong></h6><p>　支持 <code>Kerberos</code> 权限控制，所以可以和带权限控制的 HDFS 或 Hive metastore 进行整合</p>
<h6 id="Pluggable-Partitioner"><a href="#Pluggable-Partitioner" class="headerlink" title="Pluggable Partitioner"></a><strong>Pluggable Partitioner</strong></h6><p>　支持默认的 Partitioner、基于 field 的 Partitioner、基于时间的 Partitioner（包括 daily / hourly 等粒度）<br>　可以实现 <code>Partitioner</code> 类扩展自己的 Partitioner<br>　也可以实现 <code>TimeBasedPartitioner</code> 类扩展基于时间的 Partitioner</p>
<h5 id="实战-HDFS"><a href="#实战-HDFS" class="headerlink" title="实战 HDFS"></a>实战 HDFS</h5><h6 id="安装-HDFS"><a href="#安装-HDFS" class="headerlink" title="安装 HDFS"></a>安装 HDFS</h6><p>　此处略，详见我的另一篇博客《<a href="https://yuzhouwan.com/posts/39683/#Hadoop">Apache Eagle</a>》</p>
<h6 id="配置-Confluent"><a href="#配置-Confluent" class="headerlink" title="配置 Confluent"></a>配置 Confluent</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka-connect-hdfs/quickstart-hdfs.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name</span>=<span class="string">hdfs-sink</span></span><br><span class="line"><span class="meta">connector.class</span>=<span class="string">io.confluent.connect.hdfs.HdfsSinkConnector</span></span><br><span class="line"><span class="meta">tasks.max</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">topics</span>=<span class="string">test_hdfs</span></span><br><span class="line"><span class="meta">hdfs.url</span>=<span class="string">hdfs://localhost:9000</span></span><br><span class="line"><span class="meta">flush.size</span>=<span class="string">3</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="启用"><a href="#启用" class="headerlink" title="启用"></a>启用</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ confluent start</span><br><span class="line">  Starting zookeeper</span><br><span class="line">  zookeeper is [UP]</span><br><span class="line">  Starting kafka</span><br><span class="line">  kafka is [UP]</span><br><span class="line">  Starting schema-registry</span><br><span class="line">  schema-registry is [UP]</span><br><span class="line">  Starting kafka-rest</span><br><span class="line">  kafka-rest is [UP]</span><br><span class="line">  Starting connect</span><br><span class="line">  connect is [UP]</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/software/confluent</span><br><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic test_hdfs --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 输入一下三行数据</span></span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value3"</span>}</span><br><span class="line"></span><br><span class="line">$ confluent load hdfs-sink -d etc/kafka-connect-hdfs/quickstart-hdfs.properties</span><br><span class="line">  {</span><br><span class="line">      <span class="string">"name"</span>:<span class="string">"hdfs-sink"</span>,</span><br><span class="line">      <span class="string">"config"</span>:{</span><br><span class="line">          <span class="string">"connector.class"</span>:<span class="string">"io.confluent.connect.hdfs.HdfsSinkConnector"</span>,</span><br><span class="line">          <span class="string">"tasks.max"</span>:<span class="string">"1"</span>,</span><br><span class="line">          <span class="string">"topics"</span>:<span class="string">"test_hdfs"</span>,</span><br><span class="line">          <span class="string">"hdfs.url"</span>:<span class="string">"hdfs://localhost:9000"</span>,</span><br><span class="line">          <span class="string">"flush.size"</span>:<span class="string">"3"</span>,</span><br><span class="line">          <span class="string">"name"</span>:<span class="string">"hdfs-sink"</span></span><br><span class="line">      },</span><br><span class="line">      <span class="string">"tasks"</span>:[</span><br><span class="line">          {</span><br><span class="line">              <span class="string">"connector"</span>:<span class="string">"hdfs-sink"</span>,</span><br><span class="line">              <span class="string">"task"</span>:0</span><br><span class="line">          }</span><br><span class="line">      ]</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<h6 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls /topics/test_hdfs/partition=0</span><br><span class="line">  Found 1 items</span><br><span class="line">  -rw-r--r--   3 connect supergroup        197 2017-11-12 16:58 /topics/test_hdfs/partition=0/test_hdfs+0+0000000000+0000000002.avro</span><br><span class="line"></span><br><span class="line">$ wget http://mirror.metrocast.net/apache/avro/avro-1.8.2/java/avro-tools-1.8.2.jar</span><br><span class="line"><span class="comment"># 直接在线上执行 avro 2 json 操作</span></span><br><span class="line">$ hadoop jar avro-tools-1.8.2.jar tojson hdfs://localhost:9000/topics/test_hdfs/partition=0/test_hdfs+0+0000000000+0000000002.avro</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者，拷贝 avro 文件到本地，再执行 avro 2 json 操作</span></span><br><span class="line">$ hadoop fs -copyToLocal /topics/test_hdfs/partition=0/test_hdfs+0+0000000000+0000000002.avro /tmp/test_hdfs+0+0000000000+0000000002.avro</span><br><span class="line">$ java -jar avro-tools-1.8.2.jar tojson /tmp/test_hdfs+0+0000000000+0000000002.avro</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value3"</span>}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="实战-Hive"><a href="#实战-Hive" class="headerlink" title="实战 Hive"></a>实战 Hive</h5><h6 id="安装-Hive"><a href="#安装-Hive" class="headerlink" title="安装 Hive"></a>安装 Hive</h6><p>a) 下载 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz">apache-hive-2.1.1-bin.tar.gz</a></p>
<p>b) 修改环境变量</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bashrc</span><br><span class="line">  <span class="built_in">export</span> HIVE_HOME=/usr/<span class="built_in">local</span>/hadoop/hive</span><br><span class="line">  <span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$HIVE_HOME</span>/conf:<span class="variable">$PATH</span></span><br></pre></td></tr></tbody></table></figure>
<p>c) 修改配置文件</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vim hive-env.sh</span><br><span class="line">  <span class="built_in">export</span> HADOOP_HEAPSIZE=1024</span><br><span class="line">  <span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line">  HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop</span><br><span class="line">  <span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line">  <span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/hive/conf</span><br><span class="line">  <span class="comment"># Folder containing extra ibraries required for hive compilation/execution can be controlled by:</span></span><br><span class="line">  <span class="built_in">export</span> HIVE_AUX_JARS_PATH=/usr/<span class="built_in">local</span>/hadoop/hive/lib</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim hive-site.xml</span><br><span class="line">  <span class="comment"># 该参数指定了 Hive 的数据存储目录，默认位置在 HDFS 上面的 /user/hive/warehouse 路径下</span></span><br><span class="line">  hive.metastore.warehouse.dir</span><br><span class="line">  <span class="comment"># 该参数指定了 Hive 的数据临时文件目录，默认位置为 HDFS 上面的 /tmp/hive 路径下</span></span><br><span class="line">  hive.exec.scratchdir</span><br></pre></td></tr></tbody></table></figure>
<p>d) 初始化和启动运行</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">$ ./schematool -initSchema -dbType derby</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ nohup bin/hive --service metastore &gt; logs/metastore.log &amp;</span><br><span class="line">$ nohup bin/hive --service hiveserver2 &gt; logs/hiveserver2.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">$ bin/hive</span><br><span class="line">  &gt; show databases;</span><br></pre></td></tr></tbody></table></figure>
<h6 id="配置-Confluent-1"><a href="#配置-Confluent-1" class="headerlink" title="配置 Confluent"></a>配置 Confluent</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka-connect-hdfs/quickstart-hdfs.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hdfs 部分</span></span><br><span class="line"><span class="attr">name</span>=<span class="string">hdfs_sink_18</span></span><br><span class="line"><span class="meta">connector.class</span>=<span class="string">io.confluent.connect.hdfs.HdfsSinkConnector</span></span><br><span class="line"><span class="meta">topics.dir</span>=<span class="string">/user/connect/topics</span></span><br><span class="line"><span class="meta">flush.size</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">topics</span>=<span class="string">hdfs_sink_18</span></span><br><span class="line"><span class="meta">tasks.max</span>=<span class="string">1</span></span><br><span class="line"><span class="meta">hdfs.url</span>=<span class="string">hdfs://192.168.1.101:9000</span></span><br><span class="line"><span class="meta">logs.dir</span>=<span class="string">/user/connect/logs</span></span><br><span class="line"><span class="meta">schema.cache.size</span>=<span class="string">1</span></span><br><span class="line"><span class="meta">value.converter.schema.registry.url</span>=<span class="string">http://localhost:8081</span></span><br><span class="line"><span class="meta">format.class</span>=<span class="string">io.confluent.connect.hdfs.avro.AvroFormat</span></span><br><span class="line"><span class="meta">value.converter</span>=<span class="string">io.confluent.connect.avro.AvroConverter</span></span><br><span class="line"><span class="meta">key.converter</span>=<span class="string">io.confluent.connect.avro.AvroConverter</span></span><br><span class="line"><span class="meta">key.converter.schema.registry.url</span>=<span class="string">http://localhost:8081</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hive 部分</span></span><br><span class="line"><span class="meta">hive.integration</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">hive.metastore.uris</span>=<span class="string">thrift://192.168.1.101:9083</span></span><br><span class="line"><span class="meta">schema.compatibility</span>=<span class="string">BACKWARD</span></span><br><span class="line"><span class="meta">hive.database</span>=<span class="string">hive</span></span><br><span class="line"><span class="meta">hive.conf.dir</span>=<span class="string">/home/connect/software/hive/conf</span></span><br><span class="line"><span class="meta">hive.home</span>=<span class="string">/home/connect/software/hive</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="启用-1"><a href="#启用-1" class="headerlink" title="启用"></a>启用</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ confluent load hdfs-sink-18 -d etc/kafka-connect-hdfs/quickstart-schema.properties</span><br><span class="line">$ <span class="built_in">cd</span> ~/software/confluent</span><br><span class="line">$ ./bin/kafka-avro-console-producer --broker-list 192.168.1.101:9092,192.168.1.102:9092,192.168.1.103:9092 --topic hdfs_sink_18 --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 输入以下数据</span></span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value3"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value4"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value5"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>:<span class="string">"value6"</span>}</span><br></pre></td></tr></tbody></table></figure>
<h6 id="验证-1"><a href="#验证-1" class="headerlink" title="验证"></a>验证</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hive</span><br><span class="line">$ use hive;</span><br><span class="line">$ select * from hdfs_sink_18 <span class="built_in">limit</span> 10;</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">OK</span><br><span class="line">value4  0</span><br><span class="line">value4  0</span><br><span class="line">value6  0</span><br><span class="line">value2  1</span><br><span class="line">value4  1</span><br><span class="line">value4  1</span><br><span class="line">value5  1</span><br><span class="line">value1  2</span><br><span class="line">value4  2</span><br><span class="line">value4  2</span><br><span class="line">Time taken: 8.458 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Kafka-2-ElasticSearch"><a href="#Kafka-2-ElasticSearch" class="headerlink" title="Kafka 2 ElasticSearch"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/kafka-connect-elasticsearch">Kafka 2 ElasticSearch</a></h4><h5 id="特性-2"><a href="#特性-2" class="headerlink" title="特性"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/connect-elasticsearch/docs/elasticsearch_connector.html">特性</a></h5><h6 id="Exactly-Once-Delivery-1"><a href="#Exactly-Once-Delivery-1" class="headerlink" title="Exactly Once Delivery"></a><strong>Exactly Once Delivery</strong></h6><p>　Connect 使用 ElasticSearch 的幂等写入语句和设置 ES Document IDs，来确保写入 ES 的数据都是<strong>有且仅有一条</strong>的<br>　如果 Kafka 消息里面包含了 Key 值，那么这些 Key 值会自动转化为 ES Document IDs；相反，如果 Kafka 消息里面没有包含这些 Key 值，或者是明确指定不要使用 Kafka 消息里面的 Key 值，Kafka Connect 将会自动使用 <code>topic+partition+offset</code> 作为 Key 值，以确保每一条写入 ES 的数据，都有一个唯一对应的 Document</p>
<h6 id="Mapping-Inference"><a href="#Mapping-Inference" class="headerlink" title="Mapping Inference"></a><strong>Mapping Inference</strong></h6><p>　启用该功能时，Connector 可以<strong>自动</strong>依据 Schema Register 来<strong>推断</strong> ES <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html">mapping</a> 结构<br>　但是，该功能的推断受限于 field 类型和默认值。如果需要增加更多的限制（例如，用户自定义的解析器），则需要自己手动创建 mapping</p>
<h6 id="Schema-Evolution-1"><a href="#Schema-Evolution-1" class="headerlink" title="Schema Evolution"></a><strong>Schema Evolution</strong></h6><p>　支持<strong>模式演进</strong>和不同的 Schema 兼容级别<br>　可以处理一些模式不兼容的更改，比如</p>
<ul>
<li><p>增加字段</p>
<p>当增加一个或者多个 field 到 Kafka 消息中，并且 ES 开启了 <code>dynamic mapping</code> 功能，那么 ES 将会自动增加新的 field 到 mapping 中</p>
</li>
<li><p>移除字段</p>
<p>当从 Kafka 消息中移除一个或者多个 field 时，则这些缺失的值，将会被作为 <code>null</code> 值进行处理</p>
</li>
<li><p>更改字段类型</p>
<p>例如将一个 field 从 <code>string</code> 改成 <code>integer</code> 类型，ES 将会自动将 <code>string</code> 转化为 <code>integer</code></p>
</li>
</ul>
<h6 id="Delivery-Semantics"><a href="#Delivery-Semantics" class="headerlink" title="Delivery Semantics"></a><strong>Delivery Semantics</strong></h6><p>　Connector 支持 <strong>batch</strong> 和 <strong>pipeline</strong> 两种写入 ES 的模式，以此来增加吞吐量。batch 模式下，允许并行地处理多个 batch<br>　通过使用分区级 Kafka 偏移量作为 ES <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-versioning">文档版本</a>，并使用 <code>version_mode = external</code> 配置，来确保文档级（Document-level）更新顺序</p>
<h6 id="Reindexing-with-Zero-Downtime"><a href="#Reindexing-with-Zero-Downtime" class="headerlink" title="Reindexing with Zero Downtime"></a><strong>Reindexing with Zero Downtime</strong></h6><p>　利用 ES 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Index Aliases</a> 接口，可以完成零停机 reindexing 操作（ES 本身提供了 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html">Reindex</a> 接口，不过性能不高），具体对 ES 集群操作的步骤如下</p>
<ol>
<li>Create an alias for the index with the old mapping.</li>
<li>The applications that uses the index are pointed to the alias.</li>
<li>Create a new index with the updated mapping.</li>
<li>Move data from old to the new index.</li>
<li>Atomically move the alias to the new index.</li>
<li>Delete the old index.</li>
</ol>
<p>　为了保证用户无感知，需要在 reindexing 期间，仍然可以处理写数据的请求。但是别名是无法同时往新旧 index 写入数据的。为了解决这个问题，可以使用两个 ElasticSearch Connector 同时将相同的数据，写入到新旧 index 中，具体对 ES Connector 的操作步骤如下</p>
<ol>
<li>The connector job that ingest data to the old indices continue writing to the old indices.</li>
<li>Create a new connector job that writes to new indices. This will copy both some old data and new data to the new indices as long as the data is in Kafka.</li>
<li>Once the data in the old indices are moved to the new indices by the reindexing process, we can stop the old connector job.</li>
</ol>
<h5 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h5><h6 id="安装-ElasticSearch"><a href="#安装-ElasticSearch" class="headerlink" title="安装 ElasticSearch"></a>安装 ElasticSearch</h6><p>　此处略写，详见我的另一篇博客《<a href="https://yuzhouwan.com/posts/22654/#安装">ElasticSearch</a>》</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载</span></span><br><span class="line">$ <span class="built_in">cd</span> ~/install</span><br><span class="line">$ curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.4.tar.gz</span><br><span class="line">$ tar zxvf elasticsearch-5.6.4.tar.gz -C ~/software/</span><br><span class="line">$ <span class="built_in">cd</span> ~/software</span><br><span class="line">$ ln -s elasticsearch-5.6.4/ elasticsearch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境变量</span></span><br><span class="line">$ vim ~/.bashrc</span><br><span class="line">  <span class="built_in">export</span> JAVA_HOME=~/software/java</span><br><span class="line">  <span class="built_in">export</span> ELASTIC_SEARCH_HOME=~/software/elasticsearch</span><br><span class="line">  <span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$ELASTIC_SEARCH_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br><span class="line">$ elasticsearch -version</span><br><span class="line">  Version: 5.6.4, Build: 8bbedf5/2017-10-31T18:55:38.105Z, JVM: 1.8.0_131</span><br><span class="line"></span><br><span class="line"><span class="comment"># 后台启动</span></span><br><span class="line">$ elasticsearch -Ecluster.name=yuzhouwan -Enode.name=yuzhouwan_kafka_connect_test -d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line">$ jps -ml</span><br><span class="line">  25332 org.elasticsearch.bootstrap.Elasticsearch -Ecluster.name=yuzhouwan -Enode.name=yuzhouwan_kafka_connect_test -d</span><br><span class="line">$ netstat -nap | grep 9200</span><br><span class="line">  tcp        0      0 ::ffff:127.0.0.1:9200       :::*                        LISTEN      25332/java</span><br><span class="line">  tcp        0      0 ::1:9200                    :::*                        LISTEN      25332/java</span><br><span class="line"></span><br><span class="line">$ curl -XGET <span class="string">'http://localhost:9200/_cluster/health'</span> | jq</span><br><span class="line">  {</span><br><span class="line">    <span class="string">"cluster_name"</span>: <span class="string">"yuzhouwan"</span>,</span><br><span class="line">    <span class="string">"status"</span>: <span class="string">"yellow"</span>,</span><br><span class="line">    <span class="string">"timed_out"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"number_of_nodes"</span>: 1,</span><br><span class="line">    <span class="string">"number_of_data_nodes"</span>: 1,</span><br><span class="line">    <span class="string">"active_primary_shards"</span>: 5,</span><br><span class="line">    <span class="string">"active_shards"</span>: 5,</span><br><span class="line">    <span class="string">"relocating_shards"</span>: 0,</span><br><span class="line">    <span class="string">"initializing_shards"</span>: 0,</span><br><span class="line">    <span class="string">"unassigned_shards"</span>: 5,</span><br><span class="line">    <span class="string">"delayed_unassigned_shards"</span>: 0,</span><br><span class="line">    <span class="string">"number_of_pending_tasks"</span>: 0,</span><br><span class="line">    <span class="string">"number_of_in_flight_fetch"</span>: 0,</span><br><span class="line">    <span class="string">"task_max_waiting_in_queue_millis"</span>: 0,</span><br><span class="line">    <span class="string">"active_shards_percent_as_number"</span>: 50</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<h6 id="配置-Confluent-2"><a href="#配置-Confluent-2" class="headerlink" title="配置 Confluent"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/master/docs/configuration_options.rst">配置</a> Confluent</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka-connect-elasticsearch/quickstart-elasticsearch.properties</span><br><span class="line">  name=elasticsearch-sink</span><br><span class="line">  connector.class=io.confluent.connect.elasticsearch.ElasticsearchSinkConnector</span><br><span class="line">  tasks.max=1</span><br><span class="line">  topics=<span class="built_in">test</span>-elasticsearch-sink</span><br><span class="line">  key.ignore=<span class="literal">true</span></span><br><span class="line">  connection.url=http://localhost:9200</span><br><span class="line">  type.name=kafka-connect</span><br></pre></td></tr></tbody></table></figure>
<h6 id="启用-2"><a href="#启用-2" class="headerlink" title="启用"></a>启用</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/software/confluent</span><br><span class="line">$ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties</span><br><span class="line">$ ./bin/kafka-server-start ./etc/kafka/server.properties</span><br><span class="line">$ ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties</span><br><span class="line">$ ./bin/kafka-rest-start ./etc/kafka-rest/kafka-rest.properties</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/software/confluent</span><br><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic <span class="built_in">test</span>-elasticsearch-sink --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 输入一下三行数据</span></span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value3"</span>}</span><br><span class="line"></span><br><span class="line">$ ./bin/connect-standalone etc/schema-registry/connect-avro-standalone.properties etc/kafka-connect-elasticsearch/quickstart-elasticsearch.properties</span><br><span class="line"></span><br><span class="line">$ confluent load elasticsearch-sink | jq</span><br><span class="line">  {</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"elasticsearch-sink"</span>,</span><br><span class="line">    <span class="string">"config"</span>: {</span><br><span class="line">      <span class="string">"connector.class"</span>: <span class="string">"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector"</span>,</span><br><span class="line">      <span class="string">"tasks.max"</span>: <span class="string">"1"</span>,</span><br><span class="line">      <span class="string">"topics"</span>: <span class="string">"test-elasticsearch-sink"</span>,</span><br><span class="line">      <span class="string">"key.ignore"</span>: <span class="string">"true"</span>,</span><br><span class="line">      <span class="string">"connection.url"</span>: <span class="string">"http://localhost:9200"</span>,</span><br><span class="line">      <span class="string">"type.name"</span>: <span class="string">"kafka-connect"</span>,</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"elasticsearch-sink"</span></span><br><span class="line">    },</span><br><span class="line">    <span class="string">"tasks"</span>: [</span><br><span class="line">      {</span><br><span class="line">        <span class="string">"connector"</span>: <span class="string">"elasticsearch-sink"</span>,</span><br><span class="line">        <span class="string">"task"</span>: 0</span><br><span class="line">      }</span><br><span class="line">    ]</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<h6 id="验证-2"><a href="#验证-2" class="headerlink" title="验证"></a>验证</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">$ curl -XGET <span class="string">'http://localhost:9200/test-elasticsearch-sink/_search?pretty'</span></span><br><span class="line">  {</span><br><span class="line">    <span class="string">"took"</span> : 2,</span><br><span class="line">    <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"_shards"</span> : {</span><br><span class="line">      <span class="string">"total"</span> : 5,</span><br><span class="line">      <span class="string">"successful"</span> : 5,</span><br><span class="line">      <span class="string">"skipped"</span> : 0,</span><br><span class="line">      <span class="string">"failed"</span> : 0</span><br><span class="line">    },</span><br><span class="line">    <span class="string">"hits"</span> : {</span><br><span class="line">      <span class="string">"total"</span> : 3,</span><br><span class="line">      <span class="string">"max_score"</span> : 1.0,</span><br><span class="line">      <span class="string">"hits"</span> : [</span><br><span class="line">        {</span><br><span class="line">          <span class="string">"_index"</span> : <span class="string">"test-elasticsearch-sink"</span>,</span><br><span class="line">          <span class="string">"_type"</span> : <span class="string">"kafka-connect"</span>,</span><br><span class="line">          <span class="string">"_id"</span> : <span class="string">"test-elasticsearch-sink+0+0"</span>,</span><br><span class="line">          <span class="string">"_score"</span> : 1.0,</span><br><span class="line">          <span class="string">"_source"</span> : {</span><br><span class="line">            <span class="string">"f1"</span> : <span class="string">"value1"</span></span><br><span class="line">          }</span><br><span class="line">        },</span><br><span class="line">        {</span><br><span class="line">          <span class="string">"_index"</span> : <span class="string">"test-elasticsearch-sink"</span>,</span><br><span class="line">          <span class="string">"_type"</span> : <span class="string">"kafka-connect"</span>,</span><br><span class="line">          <span class="string">"_id"</span> : <span class="string">"test-elasticsearch-sink+0+2"</span>,</span><br><span class="line">          <span class="string">"_score"</span> : 1.0,</span><br><span class="line">          <span class="string">"_source"</span> : {</span><br><span class="line">            <span class="string">"f1"</span> : <span class="string">"value3"</span></span><br><span class="line">          }</span><br><span class="line">        },</span><br><span class="line">        {</span><br><span class="line">          <span class="string">"_index"</span> : <span class="string">"test-elasticsearch-sink"</span>,</span><br><span class="line">          <span class="string">"_type"</span> : <span class="string">"kafka-connect"</span>,</span><br><span class="line">          <span class="string">"_id"</span> : <span class="string">"test-elasticsearch-sink+0+1"</span>,</span><br><span class="line">          <span class="string">"_score"</span> : 1.0,</span><br><span class="line">          <span class="string">"_source"</span> : {</span><br><span class="line">            <span class="string">"f1"</span> : <span class="string">"value2"</span></span><br><span class="line">          }</span><br><span class="line">        }</span><br><span class="line">      ]</span><br><span class="line">    }</span><br><span class="line">  }</span><br></pre></td></tr></tbody></table></figure>
<h5 id="踩过的坑"><a href="#踩过的坑" class="headerlink" title="踩过的坑"></a>踩过的坑</h5><h6 id="max-file-descriptors-32768-for-elasticsearch-process-is-too-low-increase-to-at-least-65536"><a href="#max-file-descriptors-32768-for-elasticsearch-process-is-too-low-increase-to-at-least-65536" class="headerlink" title="max file descriptors [32768] for elasticsearch process is too low, increase to at least [65536]"></a>max file descriptors [32768] for elasticsearch process is too low, increase to at least [65536]</h6><ul>
<li>解决</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ su root</span><br><span class="line">$ vim /etc/security/limits.d/90-nproc.conf</span><br><span class="line">  *    -    nproc      20480</span><br><span class="line">  *    -    nofile     65536</span><br><span class="line">  *    -    memlock    unlimited</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Kafka-2-Druid"><a href="#Kafka-2-Druid" class="headerlink" title="Kafka 2 Druid"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/gianm/kafka-connect-druid">Kafka 2 Druid</a></h4><p>　官方<a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/gianm/kafka-connect-druid/issues/1">决定</a>使用 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://druid.io/docs/latest/development/extensions-core/kafka-ingestion.html">Kafka Indexing Service</a> 方案，方便构建自己的生态圈。不过，目前的 Kafka Indexing Service 仍然存在一定的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://groups.google.com/forum/#!topic/druid-user/S7UKNsSCMGI">缺陷</a>。因此，Kafka Connect 2 <a href="https://yuzhouwan.com/posts/5845/">Druid</a> 功能需要自己基于 Druid Tranquility 组件进行定制开发。同时，可以在关闭 Kafka Auto Commit 功能后，自己控制事务提交，来保证不丢不重，进而弥补了 Tranquility 组件没有 Exactly-once 特性的缺陷</p>
<p>Tips: <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/Landoop/stream-reactor">Stream Reactor</a> 中基于 Tranquility 实现了 K2D</p>
<h4 id="Kafka-2-Kafka"><a href="#Kafka-2-Kafka" class="headerlink" title="Kafka 2 Kafka"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/connect-replicator/docs/connect_replicator.html">Kafka 2 Kafka</a></h4><p>　Connect Replicator 可以轻松可靠地将 Topic 从一个 Kafka 集群复制到另一个。除了复制消息之外，此连接器还会根据需要创建主题，以保留源群集中的 Topic 配置。这包括保留分区数量，副本数以及为单个 Topic 指定对任何配置的覆盖</p>
<p>　下图显示了一个典型的<strong>多数据中心</strong>（<strong>D</strong>ata <strong>C</strong>enter）部署，其中来自位于不同数据中心的两个 Kafka 群集的数据聚合在位于另一个数据中心的单独群集中。这里，复制数据的来源称为<strong>源簇</strong>，而复制数据的目标称为<strong>目的地</strong></p>
<h5 id="特性-3"><a href="#特性-3" class="headerlink" title="特性"></a>特性</h5><ul>
<li>Topic 的选择，可以使用白名单、黑名单和正则表达式</li>
<li>支持使用匹配的 Partition 数量、 Replicaton 因子和 Topic 配置覆盖，在目标集群动态创建 Topic</li>
<li>当新的 partition 被加入到源集群之后，目标集群会自动扩容对应的 Topic</li>
<li>其他源集群的配置更改，都会被自动同步到目标集群</li>
</ul>
<h3 id="单机版-Worker"><a href="#单机版-Worker" class="headerlink" title="单机版 Worker"></a>单机版 Worker</h3><h4 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties ...]</span><br></pre></td></tr></tbody></table></figure>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 存储 connector 的 offset</span></span><br><span class="line">offset.storage.file.filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># RESTful 端口，接受 HTTP 请求</span></span><br><span class="line">rest.port</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: 单机模式下，偏移量 offset 保存在 <code>/tmp/connect.offset</code> 中</p>
<h3 id="分布式-Worker"><a href="#分布式-Worker" class="headerlink" title="分布式 Worker"></a>分布式 Worker</h3><h4 id="启动-Confluent"><a href="#启动-Confluent" class="headerlink" title="启动 Confluent"></a>启动 Confluent</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/software/confluent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 ZooKeeper</span></span><br><span class="line">$ nohup ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties &gt; zookeeper.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Kafka</span></span><br><span class="line">$ nohup ./bin/kafka-server-start ./etc/kafka/server.properties &gt; kafka.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Schema Registry</span></span><br><span class="line">$ nohup ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties &gt; schema.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Kafka REST</span></span><br><span class="line">$ nohup ./bin/kafka-rest-start ./etc/kafka-rest/kafka-rest.properties &gt; kafka-rest.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以分布式模式启动 Connect</span></span><br><span class="line">$ nohup ./bin/connect-distributed ./etc/kafka/connect-distributed.properties &gt; connect-distribute.log &amp;</span><br></pre></td></tr></tbody></table></figure>
<h4 id="创建-Topic"><a href="#创建-Topic" class="headerlink" title="创建 Topic"></a>创建 Topic</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取帮助文档</span></span><br><span class="line">$ bin/kafka-topics --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 connect-configs / connect-offsets / connect-status 三个 Topic</span></span><br><span class="line"><span class="comment"># connect-configs 存储 connector 和 task 的配置信息</span></span><br><span class="line">$ bin/kafka-topics --create --zookeeper localhost:2181 --topic connect-configs --replication-factor 3 --partitions 1 --config cleanup.policy=compact</span><br><span class="line"><span class="comment"># connect-offsets 存储 connector 和 task 的 offset 信息</span></span><br><span class="line">$ bin/kafka-topics --create --zookeeper localhost:2181 --topic connect-offsets --replication-factor 3 --partitions 50 --config cleanup.policy=compact</span><br><span class="line"><span class="comment"># connect-status 存储 connector 和 task 的状态变更信息</span></span><br><span class="line">$ bin/kafka-topics --create --zookeeper localhost:2181 --topic connect-status --replication-factor 3 --partitions 10 --config cleanup.policy=compact</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已存在的 topic</span></span><br><span class="line">$ bin/kafka-topics --list --zookeeper localhost:2181</span><br><span class="line">  __consumer_offsets</span><br><span class="line">  _schemas</span><br><span class="line">  connect-configs</span><br><span class="line">  connect-offsets</span><br><span class="line">  connect-statuses</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 topic 状态</span></span><br><span class="line">$ bin/kafka-topics --describe --zookeeper localhost:2181 --topic connect-configs</span><br><span class="line">  Topic:connect-configs  PartitionCount:1  ReplicationFactor:1   Configs:</span><br><span class="line">  Topic:connect-configs  Partition: 0      Leader: 0    Replicas: 0   Isr: 0</span><br></pre></td></tr></tbody></table></figure>
<h4 id="删除-Topic"><a href="#删除-Topic" class="headerlink" title="删除 Topic"></a>删除 Topic</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka/server.properties</span><br><span class="line">  <span class="comment"># 如果需要删除 topic，需要先设置 delete.topic.enable 为 true</span></span><br><span class="line">  delete.topic.enable=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">$ bin/kafka-topics --delete --zookeeper localhost:2181 --topic connect-configs</span><br></pre></td></tr></tbody></table></figure>
<h4 id="Connector-和-Task-状态"><a href="#Connector-和-Task-状态" class="headerlink" title="Connector 和 Task 状态"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/managing.html#connector-and-task-status">Connector 和 Task 状态</a></h4><p>　所有 Connector 和隶属于这些 Connector 的 Tasks，都通过 <code>status.storage.topic</code> 发布状态更新。因为 Worker 消费 <code>status.storage.topic</code> 是异步的，所以从 API 获取最新状态的时候，会存在一些短暂的延迟</p>
<p>　Connector 或者某个 Task 的状态，可能为：</p>
<ul>
<li><p><strong>UNASSIGNED</strong></p>
<p>Connector 或者 Task 尚未被分配到某个 Worker 上</p>
</li>
<li><p><strong>RUNNING</strong></p>
<p>运行中</p>
</li>
<li><p><strong>PAUSED</strong></p>
<p>Connector 或者 Task 被管理员暂停</p>
</li>
<li><p><strong>FAILED</strong></p>
<p>通常因为出现异常，导致 Connector 或者 Task 失败</p>
</li>
</ul>
<p>　一般 Pause / Resume API 的操作，适用的场景是 消费端系统需要维护，通过停止掉 Kafka Connector，来避免数据一直被积压着。同时，停止操作不是临时的，即便重启了集群，仍需要再次操作 Resume 接口，暂停的 Connector 才会恢复到 Running 状态。另外，FAILED 状态的任务是不允许执行 Pause 操作的，需要重启恢复到 Running 状态才行</p>
<h4 id="Worker-之间如何建立通讯"><a href="#Worker-之间如何建立通讯" class="headerlink" title="Worker 之间如何建立通讯"></a>Worker 之间如何建立通讯</h4><p>　只要保证各个 Worker 使用的是统一的 <code>group.id</code>（可以看做是 Cluster Name），还有 <code>config.storage.topic</code>、<code>offset.storage.topic</code> 和 <code>status.storage.topic</code> 三个 Topic 也需要保持一致，Worker 就会自动发现同一个集群中的其他 Worker 进程</p>
<h4 id="Relance-机制"><a href="#Relance-机制" class="headerlink" title="Relance 机制"></a>Relance 机制</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[2017-12-04 11:32:18,607] INFO Rebalance started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1187)</span><br><span class="line">[2017-12-04 11:32:18,608] INFO Finished stopping tasks <span class="keyword">in</span> preparation <span class="keyword">for</span> rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1217)</span><br><span class="line">[2017-12-04 11:32:18,608] INFO (Re-)joining group connect-cluster (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:442)</span><br><span class="line">[2017-12-04 11:32:18,612] INFO Successfully joined group connect-cluster with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:409)</span><br><span class="line">[2017-12-04 11:32:18,612] INFO Joined group and got assignment: Assignment{error=0, leader=<span class="string">'connect-1-efb9e92c-27a6-4062-9fcc-92480f8e9e03'</span>, leaderUrl=<span class="string">'http://192.168.1.101:8083/'</span>, offset=-1, connectorIds=[], taskIds=[]} (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1166)</span><br><span class="line">[2017-12-04 11:32:18,612] INFO Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:815)</span><br><span class="line">[2017-12-04 11:32:18,613] INFO Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:825)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="完全分布式"><a href="#完全分布式" class="headerlink" title="完全分布式"></a>完全分布式</h3><h4 id="替换原生的-ZooKeeper"><a href="#替换原生的-ZooKeeper" class="headerlink" title="替换原生的 ZooKeeper"></a>替换原生的 ZooKeeper</h4><p>　具体安装过程略，详见我的另一篇博客《<a href="https://yuzhouwan.com/posts/31915/#环境搭建">ZooKeeper 原理与优化</a>》</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/connect/software/confluent</span><br><span class="line">$ vim ./etc/kafka/server.properties</span><br><span class="line">  zookeeper.connect=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line">  zookeeper.connection.timeout.ms=6000</span><br><span class="line"></span><br><span class="line">$ vim ./etc/schema-registry/connect-avro-distributed.properties</span><br><span class="line">  id=kafka-rest-test-server</span><br><span class="line">  schema.registry.url=http://localhost:8081</span><br><span class="line">  zookeeper.connect=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line"></span><br><span class="line">$ zkServer.sh start</span><br></pre></td></tr></tbody></table></figure>
<h4 id="替换原生的-Kafka"><a href="#替换原生的-Kafka" class="headerlink" title="替换原生的 Kafka"></a>替换原生的 Kafka</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建日志目录</span></span><br><span class="line">$ mkdir -p /home/connect/kafka_log</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home/connect/software/kafka</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 Kafka Server</span></span><br><span class="line">$ vim config/server.properties</span><br><span class="line">  <span class="comment"># 每个 Kafka Broker 节点，配置不同的 broker.id</span></span><br><span class="line">  broker.id=0</span><br><span class="line">  <span class="comment"># 允许删除 topic</span></span><br><span class="line">  delete.topic.enable=<span class="literal">true</span></span><br><span class="line">  <span class="comment"># 配置成当前 Kafka Borker 节点的 IP</span></span><br><span class="line">  listeners=PLAINTEXT://192.168.1.101:9092</span><br><span class="line">  advertised.listeners=PLAINTEXT://192.168.1.101:9092</span><br><span class="line">  log.dirs=/home/connect/kafka_log</span><br><span class="line">  zookeeper.connect=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 Kafka ZooKeeper</span></span><br><span class="line">$ vim config/zookeeper.properties</span><br><span class="line">  dataDir=/home/connect/zkdata</span><br><span class="line">  clientPort=2181</span><br><span class="line">  maxClientCnxns=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 Kafka Consumer</span></span><br><span class="line">$ vim config/consumer.properties</span><br><span class="line">  zookeeper.connect=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line">  zookeeper.connection.timeout.ms=6000</span><br><span class="line">  group.id=<span class="built_in">test</span>-consumer-group</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Kafka</span></span><br><span class="line">$ nohup /home/connect/software/kafka/bin/kafka-server-start.sh /home/connect/software/kafka/config/server.properties &gt; /home/connect/kafka_log/kafka.server.log 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 Kafka 日志</span></span><br><span class="line">$ tail -f ~/kafka_log/kafka.server.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新 Confluent 配置</span></span><br><span class="line">$ <span class="built_in">cd</span> /home/connect/software/confluent</span><br><span class="line">$ vim etc/kafka/connect-distributed.properties</span><br><span class="line">  bootstrap.servers=192.168.1.101:9092,192.168.1.102:9092,192.168.1.103:9092</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 connect-configs / connect-offsets / connect-status 三个 Topic</span></span><br><span class="line"><span class="comment"># connect-configs 存储 connector 和 task 的配置信息</span></span><br><span class="line">$ bin/kafka-topics.sh --create --zookeeper 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 --topic connect-configs --replication-factor 3 --partitions 1 --config cleanup.policy=compact</span><br><span class="line"><span class="comment"># connect-offsets 存储 connector 和 task 的 offset 信息</span></span><br><span class="line">$ bin/kafka-topics.sh --create --zookeeper 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 --topic connect-offsets --replication-factor 3 --partitions 1 --config cleanup.policy=compact</span><br><span class="line"><span class="comment"># connect-status 存储 connector 和 task 的状态变更信息</span></span><br><span class="line">$ bin/kafka-topics.sh --create --zookeeper 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 --topic connect-status --replication-factor 3 --partitions 1 --config cleanup.policy=compact</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看已存在的 topic</span></span><br><span class="line">$ bin/kafka-topics.sh --list --zookeeper 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line">  __consumer_offsets</span><br><span class="line">  connect-configs</span><br><span class="line">  connect-offsets</span><br><span class="line">  connect-status</span><br><span class="line">  <span class="built_in">test</span>-elasticsearch-sink</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查 topic 状态</span></span><br><span class="line">$ bin/kafka-topics --describe --zookeeper 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181 --topic connect-configs</span><br><span class="line">  Topic:connect-configs   PartitionCount:1  ReplicationFactor:3   Configs:cleanup.policy=compact</span><br><span class="line">  Topic:connect-configs   Partition: 0      Leader: 2             Replicas: 0,2,1     Isr: 2,0,1</span><br></pre></td></tr></tbody></table></figure>
<h4 id="更新-Kafka-Rest-配置"><a href="#更新-Kafka-Rest-配置" class="headerlink" title="更新 Kafka Rest 配置"></a>更新 Kafka Rest 配置</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/connect/software/confluent</span><br><span class="line">$ vim etc/kafka-rest/kafka-rest.properties</span><br><span class="line">  id=kafka-rest-test-server</span><br><span class="line">  schema.registry.url=http://192.168.1.103:8081</span><br><span class="line">  zookeeper.connect=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line"></span><br><span class="line">$ nohup ./bin/kafka-rest-start ./etc/kafka-rest/kafka-rest.properties &amp;</span><br></pre></td></tr></tbody></table></figure>
<h4 id="更新-Schema-Register-配置"><a href="#更新-Schema-Register-配置" class="headerlink" title="更新 Schema Register 配置"></a>更新 Schema Register 配置</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/connect/software/confluent</span><br><span class="line">$ vim etc/schema-registry/schema-registry.properties</span><br><span class="line">  listeners=http://0.0.0.0:8081</span><br><span class="line">  kafkastore.connection.url=192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181</span><br><span class="line">  kafkastore.topic=_schemas</span><br><span class="line">  debug=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">$ nohup ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties &amp;</span><br><span class="line">$ tail -f logs/schema-registry.log</span><br></pre></td></tr></tbody></table></figure>
<h4 id="更新-Worker-配置"><a href="#更新-Worker-配置" class="headerlink" title="更新 Worker 配置"></a>更新 Worker 配置</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ vim /home/connect/software/confluent/etc/kafka/connect-distributed.properties</span><br><span class="line"></span><br><span class="line">  bootstrap.servers=192.168.1.101:9092,192.168.1.102:9092,192.168.1.103:9092</span><br><span class="line"></span><br><span class="line">  group.id=connect-cluster</span><br><span class="line">  key.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">  value.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">  <span class="comment"># 考虑到方便压测，可以关闭 schema 功能，将下面两个配置项置为 false</span></span><br><span class="line">  key.converter.schemas.enable=<span class="literal">true</span></span><br><span class="line">  value.converter.schemas.enable=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  internal.key.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">  internal.value.converter=org.apache.kafka.connect.json.JsonConverter</span><br><span class="line">  internal.key.converter.schemas.enable=<span class="literal">false</span></span><br><span class="line">  internal.value.converter.schemas.enable=<span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  key.converter=io.confluent.connect.avro.AvroConverter</span><br><span class="line">  key.converter.schema.registry.url=http://192.168.1.103:8081</span><br><span class="line">  value.converter=io.confluent.connect.avro.AvroConverter</span><br><span class="line">  value.converter.schema.registry.url=http://192.168.1.103:8081</span><br><span class="line"></span><br><span class="line">$ nohup /home/connect/software/confluent/bin/connect-distributed /home/connect/software/confluent/etc/kafka/connect-distributed.properties &gt; /home/connect/software/confluent/logs/connect-distributed.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分发配置，并在各节点启动 worker</span></span><br><span class="line">$ scp /home/connect/software/confluent/etc/schema-registry/connect-avro-distributed.properties 192.168.1.102:/home/connect/software/confluent/etc/schema-registry/connect-avro-distributed.properties</span><br><span class="line"></span><br><span class="line">$ scp /home/connect/software/confluent/etc/schema-registry/connect-avro-distributed.properties 192.168.1.103:/home/connect/software/confluent/etc/schema-registry/connect-avro-distributed.properties</span><br></pre></td></tr></tbody></table></figure>
<h4 id="踩过的坑-1"><a href="#踩过的坑-1" class="headerlink" title="踩过的坑"></a>踩过的坑</h4><h5 id="WARNING-REMOTE-HOST-IDENTIFICATION-HAS-CHANGED"><a href="#WARNING-REMOTE-HOST-IDENTIFICATION-HAS-CHANGED" class="headerlink" title="WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!"></a>WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!</h5><h6 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vim .ssh/known_hosts</span><br><span class="line">  <span class="comment"># 删除本机 IP 下的秘钥</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="Timed-out-while-checking-for-or-creating-topic-s-‘connect-offsets’"><a href="#Timed-out-while-checking-for-or-creating-topic-s-‘connect-offsets’" class="headerlink" title="Timed out while checking for or creating topic(s) ‘connect-offsets’"></a>Timed out while checking for or creating topic(s) ‘connect-offsets’</h5><h6 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h6><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2017</span>-<span class="number">12</span>-<span class="number">01</span> <span class="number">17</span>:<span class="number">39</span>:<span class="number">13</span>,<span class="number">503</span>] ERROR Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:<span class="number">206</span>)</span><br><span class="line">org.apache.kafka.connect.errors.ConnectException: <span class="function">Timed out <span class="keyword">while</span> checking <span class="keyword">for</span> or creating <span class="title">topic</span><span class="params">(s)</span> 'connect-offsets'. This could indicate a connectivity issue, unavailable topic partitions, or <span class="keyword">if</span> <span class="keyword">this</span> is your first use of the topic it may have taken too <span class="keyword">long</span> to create.</span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.util.TopicAdmin.<span class="title">createTopics</span><span class="params">(TopicAdmin.java:<span class="number">243</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.storage.KafkaOffsetBackingStore$1.<span class="title">run</span><span class="params">(KafkaOffsetBackingStore.java:<span class="number">99</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.util.KafkaBasedLog.<span class="title">start</span><span class="params">(KafkaBasedLog.java:<span class="number">126</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.storage.KafkaOffsetBackingStore.<span class="title">start</span><span class="params">(KafkaOffsetBackingStore.java:<span class="number">109</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.runtime.Worker.<span class="title">start</span><span class="params">(Worker.java:<span class="number">146</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.runtime.AbstractHerder.<span class="title">startServices</span><span class="params">(AbstractHerder.java:<span class="number">99</span>)</span></span></span><br><span class="line"><span class="function">	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.<span class="title">run</span><span class="params">(DistributedHerder.java:<span class="number">194</span>)</span></span></span><br><span class="line"><span class="function">	at java.util.concurrent.Executors$RunnableAdapter.<span class="title">call</span><span class="params">(Executors.java:<span class="number">511</span>)</span></span></span><br><span class="line"><span class="function">	at java.util.concurrent.FutureTask.<span class="title">run</span><span class="params">(FutureTask.java:<span class="number">266</span>)</span></span></span><br><span class="line"><span class="function">	at java.util.concurrent.ThreadPoolExecutor.<span class="title">runWorker</span><span class="params">(ThreadPoolExecutor.java:<span class="number">1142</span>)</span></span></span><br><span class="line"><span class="function">	at java.util.concurrent.ThreadPoolExecutor$Worker.<span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line"><span class="function">	at java.lang.Thread.<span class="title">run</span><span class="params">(Thread.java:<span class="number">748</span>)</span></span></span><br><span class="line"><span class="function">Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call.</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="Request-to-leader-to-reconfigure-connector-tasks-failed"><a href="#Request-to-leader-to-reconfigure-connector-tasks-failed" class="headerlink" title="Request to leader to reconfigure connector tasks failed"></a>Request to leader to reconfigure connector tasks failed</h5><h6 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h6><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">303</span>] ERROR Unexpected error during connector task reconfiguration:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:<span class="number">933</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">303</span>] ERROR Task reconfiguration <span class="keyword">for</span> FileStreamSinkConnector failed unexpectedly, <span class="keyword">this</span> connector will not be properly reconfigured unless manually triggered. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:<span class="number">934</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">306</span>] INFO <span class="number">192.168</span>.<span class="number">1.102</span> - - [<span class="number">13</span>/Mar/<span class="number">2018</span>:<span class="number">02</span>:<span class="number">30</span>:<span class="number">28</span> +<span class="number">0000</span>] <span class="string">"POST /connectors/FileStreamSinkConnector/tasks?forward=false HTTP/1.1"</span> <span class="number">409</span> <span class="number">113</span>  <span class="number">1</span> (org.apache.kafka.connect.runtime.rest.RestServer:<span class="number">60</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">307</span>] INFO <span class="number">192.168</span>.<span class="number">1.102</span> - - [<span class="number">13</span>/Mar/<span class="number">2018</span>:<span class="number">02</span>:<span class="number">30</span>:<span class="number">28</span> +<span class="number">0000</span>] <span class="string">"POST /connectors/FileStreamSinkConnector/tasks?forward=true HTTP/1.1"</span> <span class="number">409</span> <span class="number">113</span>  <span class="number">3</span> (org.apache.kafka.connect.runtime.rest.RestServer:<span class="number">60</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">307</span>] INFO <span class="number">192.168</span>.<span class="number">1.102</span> - - [<span class="number">13</span>/Mar/<span class="number">2018</span>:<span class="number">02</span>:<span class="number">30</span>:<span class="number">28</span> +<span class="number">0000</span>] <span class="string">"POST /connectors/FileStreamSinkConnector/tasks HTTP/1.1"</span> <span class="number">409</span> <span class="number">113</span>  <span class="number">4</span> (org.apache.kafka.connect.runtime.rest.RestServer:<span class="number">60</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">307</span>] <span class="function">ERROR Request to leader to reconfigure connector tasks <span class="title">failed</span> <span class="params">(org.apache.kafka.connect.runtime.distributed.DistributedHerder:<span class="number">996</span>)</span></span></span><br><span class="line"><span class="function">org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Cannot complete request because of a conflicting <span class="title">operation</span> <span class="params">(e.g. worker rebalance)</span></span></span><br><span class="line"><span class="function">        at org.apache.kafka.connect.runtime.rest.RestServer.<span class="title">httpRequest</span><span class="params">(RestServer.java:<span class="number">229</span>)</span></span></span><br><span class="line"><span class="function">        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$18.<span class="title">run</span><span class="params">(DistributedHerder.java:<span class="number">993</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.Executors$RunnableAdapter.<span class="title">call</span><span class="params">(Executors.java:<span class="number">511</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.FutureTask.<span class="title">run</span><span class="params">(FutureTask.java:<span class="number">266</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.ThreadPoolExecutor.<span class="title">runWorker</span><span class="params">(ThreadPoolExecutor.java:<span class="number">1142</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.ThreadPoolExecutor$Worker.<span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line"><span class="function">        at java.lang.Thread.<span class="title">run</span><span class="params">(Thread.java:<span class="number">748</span>)</span></span></span><br><span class="line"><span class="function">[2018-03-13 10:30:28,307] ERROR Failed to reconfigure connector´s tasks, retrying after backoff: <span class="params">(org.apache.kafka.connect.runtime.distributed.DistributedHerder:<span class="number">922</span>)</span></span></span><br><span class="line"><span class="function">org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Cannot complete request because of a conflicting <span class="title">operation</span> <span class="params">(e.g. worker rebalance)</span></span></span><br><span class="line"><span class="function">        at org.apache.kafka.connect.runtime.rest.RestServer.<span class="title">httpRequest</span><span class="params">(RestServer.java:<span class="number">229</span>)</span></span></span><br><span class="line"><span class="function">        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$18.<span class="title">run</span><span class="params">(DistributedHerder.java:<span class="number">993</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.Executors$RunnableAdapter.<span class="title">call</span><span class="params">(Executors.java:<span class="number">511</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.FutureTask.<span class="title">run</span><span class="params">(FutureTask.java:<span class="number">266</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.ThreadPoolExecutor.<span class="title">runWorker</span><span class="params">(ThreadPoolExecutor.java:<span class="number">1142</span>)</span></span></span><br><span class="line"><span class="function">        at java.util.concurrent.ThreadPoolExecutor$Worker.<span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line"><span class="function">        at java.lang.Thread.<span class="title">run</span><span class="params">(Thread.java:<span class="number">748</span>)</span></span></span><br><span class="line"><span class="function">[2018-03-13 10:30:28,557] INFO SinkConnectorConfig values:</span></span><br><span class="line"><span class="function">        connector.class </span>= org.apache.kafka.connect.file.FileStreamSinkConnector</span><br><span class="line">        key.converter = <span class="keyword">null</span></span><br><span class="line">        name = FileStreamSinkConnector</span><br><span class="line">        tasks.max = <span class="number">1</span></span><br><span class="line">        topics = [kafka-connect-ui-file-sink]</span><br><span class="line">        transforms = <span class="keyword">null</span></span><br><span class="line">        value.converter = <span class="keyword">null</span></span><br><span class="line"> (org.apache.kafka.connect.runtime.SinkConnectorConfig:<span class="number">223</span>)</span><br><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">30</span>:<span class="number">28</span>,<span class="number">558</span>] INFO EnrichedConnectorConfig values:</span><br><span class="line">        connector.class = org.apache.kafka.connect.file.FileStreamSinkConnector</span><br><span class="line">        key.converter = <span class="keyword">null</span></span><br><span class="line">        name = FileStreamSinkConnector</span><br><span class="line">        tasks.max = <span class="number">1</span></span><br><span class="line">        topics = [kafka-connect-ui-file-sink]</span><br><span class="line">        transforms = <span class="keyword">null</span></span><br><span class="line">        value.converter = <span class="keyword">null</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="解决-1"><a href="#解决-1" class="headerlink" title="解决"></a>解决</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka/connect-distributed.properties</span><br><span class="line">  <span class="comment"># These are provided to inform the user about the presence of the REST host and port configs </span></span><br><span class="line">  <span class="comment"># Hostname &amp; Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.</span></span><br><span class="line">  rest.host.name=0.0.0.0</span><br><span class="line">  rest.port=8083</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 增加如下两个配置</span></span><br><span class="line">  <span class="comment"># The Hostname &amp; Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.</span></span><br><span class="line">  rest.advertised.host.name=192.168.1.101  <span class="comment"># 当前机器 IP 地址</span></span><br><span class="line">  rest.advertised.port=8083</span><br></pre></td></tr></tbody></table></figure>
<h5 id="io-confluent-kafka-schemaregistry-client-rest-RestService-Connection-refused"><a href="#io-confluent-kafka-schemaregistry-client-rest-RestService-Connection-refused" class="headerlink" title="io.confluent.kafka.schemaregistry.client.rest.RestService Connection refused"></a>io.confluent.kafka.schemaregistry.client.rest.RestService Connection refused</h5><h6 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic kafka-connect-ui-file-sink --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 发送数据</span></span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value1"</span>}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">2018</span>-<span class="number">03</span>-<span class="number">13</span> <span class="number">10</span>:<span class="number">48</span>:<span class="number">24</span>,<span class="number">211</span>] ERROR Failed to send HTTP request to endpoint: http:<span class="comment">//localhost:8081/subjects/kafka-connect-ui-file-sink-value/versions (io.confluent.kafka.schemaregistry.client.rest.RestService:156)</span></span><br><span class="line">java.net.ConnectException: <span class="function">Connection <span class="title">refused</span> <span class="params">(Connection refused)</span></span></span><br><span class="line"><span class="function">	at java.net.PlainSocketImpl.<span class="title">socketConnect</span><span class="params">(Native Method)</span></span></span><br><span class="line"><span class="function">	at java.net.AbstractPlainSocketImpl.<span class="title">doConnect</span><span class="params">(AbstractPlainSocketImpl.java:<span class="number">350</span>)</span></span></span><br><span class="line"><span class="function">	at java.net.AbstractPlainSocketImpl.<span class="title">connectToAddress</span><span class="params">(AbstractPlainSocketImpl.java:<span class="number">206</span>)</span></span></span><br><span class="line"><span class="function">	at java.net.AbstractPlainSocketImpl.<span class="title">connect</span><span class="params">(AbstractPlainSocketImpl.java:<span class="number">188</span>)</span></span></span><br><span class="line"><span class="function">	at java.net.SocksSocketImpl.<span class="title">connect</span><span class="params">(SocksSocketImpl.java:<span class="number">392</span>)</span></span></span><br><span class="line"><span class="function">	at java.net.Socket.<span class="title">connect</span><span class="params">(Socket.java:<span class="number">589</span>)</span></span></span><br><span class="line"><span class="function">	at java.net.Socket.<span class="title">connect</span><span class="params">(Socket.java:<span class="number">538</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.NetworkClient.<span class="title">doConnect</span><span class="params">(NetworkClient.java:<span class="number">180</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.www.http.HttpClient.<span class="title">openServer</span><span class="params">(HttpClient.java:<span class="number">463</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.www.http.HttpClient.<span class="title">openServer</span><span class="params">(HttpClient.java:<span class="number">558</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.www.http.HttpClient.&lt;init&gt;<span class="params">(HttpClient.java:<span class="number">242</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.www.http.HttpClient.<span class="title">New</span><span class="params">(HttpClient.java:<span class="number">339</span>)</span></span></span><br><span class="line"><span class="function">	at sun.net.www.http.HttpClient.<span class="title">New</span><span class="params">(HttpClient.java:<span class="number">357</span>)</span></span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="解决-2"><a href="#解决-2" class="headerlink" title="解决"></a>解决</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查 schema register 进程是否存在</span></span><br><span class="line">$ nohup ./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties &amp;</span><br><span class="line">$ tail -f logs/schema-registry.log</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Docker-镜像"><a href="#Docker-镜像" class="headerlink" title="Docker 镜像"></a>Docker 镜像</h3><h4 id="Docker-环境安装"><a href="#Docker-环境安装" class="headerlink" title="Docker 环境安装"></a>Docker 环境安装</h4><h5 id="更新镜像源"><a href="#更新镜像源" class="headerlink" title="更新镜像源"></a>更新镜像源</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加国内 yum 源</span></span><br><span class="line">$ sudo yum-config-manager --add-repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"><span class="comment"># 更新 yum 软件源缓存</span></span><br><span class="line">$ sudo yum makecache fast</span><br></pre></td></tr></tbody></table></figure>
<h5 id="安装-Docker-CE"><a href="#安装-Docker-CE" class="headerlink" title="安装 Docker-CE"></a>安装 Docker-CE</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装 依赖包</span></span><br><span class="line">$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line"><span class="comment"># 安装 docker-ce</span></span><br><span class="line">$ sudo yum install docker-ce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接一键安装</span></span><br><span class="line">$ curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line">$ sudo sh get-docker.sh --mirror Aliyun</span><br></pre></td></tr></tbody></table></figure>
<h5 id="启动-2"><a href="#启动-2" class="headerlink" title="启动"></a>启动</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> docker</span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></tbody></table></figure>
<h5 id="建立-Docker-用户组"><a href="#建立-Docker-用户组" class="headerlink" title="建立 Docker 用户组"></a>建立 Docker 用户组</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 docker 组</span></span><br><span class="line">$ sudo groupadd docker</span><br><span class="line"><span class="comment"># 将当前用户加入 docker 组</span></span><br><span class="line">$ sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="MySQL-Kafka-Connect-HDFS-实战"><a href="#MySQL-Kafka-Connect-HDFS-实战" class="headerlink" title="MySQL - Kafka Connect - HDFS 实战"></a>MySQL - Kafka Connect - HDFS 实战</h4><h5 id="下载-1"><a href="#下载-1" class="headerlink" title="下载"></a>下载</h5><p>　<code>MySQL - Kafka Connect - HDFS</code> 的集成环境，官方已经提供了 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://s3-us-west-2.amazonaws.com/confluent-files/kafka_connect_blog.ova">kafka_connect_blog.ova</a> 镜像文件</p>
<h5 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h5><h6 id="VirtualBox"><a href="#VirtualBox" class="headerlink" title="VirtualBox"></a>VirtualBox</h6><p>　Vagrant 来管理和安装 VirtualBox 虚拟机，相关安装步骤，详见我的另一篇博客《<a href="https://yuzhouwan.com/posts/43687/#Vagrant">Python</a>》</p>
<h5 id="导入虚拟机镜像"><a href="#导入虚拟机镜像" class="headerlink" title="导入虚拟机镜像"></a>导入虚拟机镜像</h5><p>　选择合适的资源，导入 <code>kafka_connect_blog.ova</code> 文件即可<br>　默认用户名密码均为 <code>vagrant</code></p>
<h5 id="操作虚拟机镜像"><a href="#操作虚拟机镜像" class="headerlink" title="操作虚拟机镜像"></a>操作虚拟机镜像</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新下载源索引</span></span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ ./setup.sh</span><br><span class="line">$ ./start.sh</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: 机器配置过低 和 网络代理受阻 的原因，未完待续…</p>
<h4 id="踩过的坑-2"><a href="#踩过的坑-2" class="headerlink" title="踩过的坑"></a>踩过的坑</h4><h5 id="bridge-nf-call-iptables-is-disabled"><a href="#bridge-nf-call-iptables-is-disabled" class="headerlink" title="bridge-nf-call-iptables is disabled"></a>bridge-nf-call-iptables is disabled</h5><h6 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WARNING: bridge-nf-call-iptables is disabled</span><br><span class="line">WARNING: bridge-nf-call-ip6tables is disabled</span><br></pre></td></tr></tbody></table></figure>
<h6 id="解决-3"><a href="#解决-3" class="headerlink" title="解决"></a>解决</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加内核配置参数</span></span><br><span class="line">$ sudo tee -a /etc/sysctl.conf &lt;&lt;-EOF</span><br><span class="line">  net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">  net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">  EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新加载 sysctl.conf</span></span><br><span class="line">$ sudo sysctl -p</span><br></pre></td></tr></tbody></table></figure>
<h3 id="常用配置"><a href="#常用配置" class="headerlink" title="常用配置"></a>常用配置</h3><h4 id="Worker-通用配置"><a href="#Worker-通用配置" class="headerlink" title="Worker 通用配置"></a>Worker 通用配置</h4><h5 id="bootstrap-servers"><a href="#bootstrap-servers" class="headerlink" title="bootstrap.servers"></a>bootstrap.servers</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立到 Kafka 的初始连接</span></span><br><span class="line"><span class="comment"># Bootstrap Kafka servers. If multiple servers are specified, they should be comma-separated.</span></span><br><span class="line"><span class="meta">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="key-value-converter"><a href="#key-value-converter" class="headerlink" title="[key | value].converter"></a>[key | value].converter</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 Kafka 中的数据如何转为到 Connect</span></span><br><span class="line"><span class="comment"># The converters specify the format of data in Kafka and how to translate it into Connect data.</span></span><br><span class="line"><span class="comment"># Every Connect user will need to configure these based on the format they want their data in</span></span><br><span class="line"><span class="comment"># when loaded from or stored into Kafka</span></span><br><span class="line"><span class="meta">key.converter</span>=<span class="string">io.confluent.connect.avro.AvroConverter</span></span><br><span class="line"><span class="meta">key.converter.schema.registry.url</span>=<span class="string">http://localhost:8081</span></span><br><span class="line"><span class="meta">value.converter</span>=<span class="string">io.confluent.connect.avro.AvroConverter</span></span><br><span class="line"><span class="meta">value.converter.schema.registry.url</span>=<span class="string">http://localhost:8081</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="internal-key-value-converter"><a href="#internal-key-value-converter" class="headerlink" title="internal.[key | value].converter"></a>internal.[key | value].converter</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 Connect 内部的数据转化</span></span><br><span class="line"><span class="comment"># The offsets, status, and configurations are written to the topics using converters specified through the following required properties.</span></span><br><span class="line"><span class="comment"># Most users will always want to use the JSON converter without schemas. </span></span><br><span class="line"><span class="comment"># Offset and config data is never visible outside of Connect in this format.</span></span><br><span class="line"><span class="meta">internal.key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="meta">internal.value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="meta">internal.key.converter.schemas.enable</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">internal.value.converter.schemas.enable</span>=<span class="string">false</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="rest-host-name-amp-rest-port"><a href="#rest-host-name-amp-rest-port" class="headerlink" title="rest.host.name &amp; rest.port"></a>rest.host.name &amp; rest.port</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 RESTful 服务的 IP 和 Port</span></span><br><span class="line"><span class="comment"># These are provided to inform the user about the presence of the REST host and port configs</span></span><br><span class="line"><span class="comment"># Hostname &amp; Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.</span></span><br><span class="line"><span class="meta">rest.host.name</span>=<span class="string">0.0.0.0</span></span><br><span class="line"><span class="meta">rest.port</span>=<span class="string">8083</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="plugin-path"><a href="#plugin-path" class="headerlink" title="plugin.path"></a>plugin.path</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins</span></span><br><span class="line"><span class="comment"># (connectors, converters, transformations). The list should consist of top level directories that include </span></span><br><span class="line"><span class="comment"># any combination of: </span></span><br><span class="line"><span class="comment"># a) directories immediately containing jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># b) uber-jars with plugins and their dependencies</span></span><br><span class="line"><span class="comment"># c) directories immediately containing the package directory structure of classes of plugins and their dependencies</span></span><br><span class="line"><span class="comment"># Examples: </span></span><br><span class="line"><span class="comment"># plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors,</span></span><br><span class="line"><span class="meta">plugin.path</span>=<span class="string">/home/connect/plugins</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="分布式-Worker-配置"><a href="#分布式-Worker-配置" class="headerlink" title="分布式 Worker 配置"></a>分布式 Worker 配置</h4><h5 id="group-id"><a href="#group-id" class="headerlink" title="group.id"></a>group.id</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The group ID is a unique identifier for the set of workers that form a single Kafka Connect cluster</span></span><br><span class="line"><span class="meta">group.id</span>=<span class="string">connect-cluster</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="config-offset-status-storage-topic"><a href="#config-offset-status-storage-topic" class="headerlink" title="[config | offset | status].storage.topic"></a>[config | offset | status].storage.topic</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Internal Storage Topics.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Kafka Connect distributed workers store the connector and task configurations, connector offsets,</span></span><br><span class="line"><span class="comment"># and connector statuses in three internal topics. These topics MUST be compacted.</span></span><br><span class="line"><span class="comment"># When the Kafka Connect distributed worker starts, it will check for these topics and attempt to create them</span></span><br><span class="line"><span class="comment"># as compacted topics if they don't yet exist, using the topic name, replication factor, and number of partitions</span></span><br><span class="line"><span class="comment"># as specified in these properties, and other topic-specific settings inherited from your brokers'</span></span><br><span class="line"><span class="comment"># auto-creation settings. If you need more control over these other topic-specific settings, you may want to</span></span><br><span class="line"><span class="comment"># manually create these topics before starting Kafka Connect distributed workers.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The following properties set the names of these three internal topics for storing configs, offsets, and status.</span></span><br><span class="line"><span class="meta">config.storage.topic</span>=<span class="string">connect-configs</span></span><br><span class="line"><span class="meta">offset.storage.topic</span>=<span class="string">connect-offsets</span></span><br><span class="line"><span class="meta">status.storage.topic</span>=<span class="string">connect-statuses</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="config-offset-status-storage-replication-factor"><a href="#config-offset-status-storage-replication-factor" class="headerlink" title="[config | offset | status].storage.replication.factor"></a>[config | offset | status].storage.replication.factor</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The following properties set the replication factor for the three internal topics, defaulting to 3 for each</span></span><br><span class="line"><span class="comment"># and therefore requiring a minimum of 3 brokers in the cluster. Since we want the examples to run with</span></span><br><span class="line"><span class="comment"># only a single broker, we set the replication factor here to just 1. That´s okay for the examples, but</span></span><br><span class="line"><span class="comment"># ALWAYS use a replication factor of AT LEAST 3 for production environments to reduce the risk of </span></span><br><span class="line"><span class="comment"># losing connector offsets, configurations, and status.</span></span><br><span class="line"><span class="meta">config.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="meta">offset.storage.replication.factor</span>=<span class="string">1</span></span><br><span class="line"><span class="meta">status.storage.replication.factor</span>=<span class="string">1</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="offset-status-storage-partitions"><a href="#offset-status-storage-partitions" class="headerlink" title="[offset | status].storage.partitions"></a>[offset | status].storage.partitions</h5><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The config storage topic must have a single partition, and this cannot be changed via properties. </span></span><br><span class="line"><span class="comment"># Offsets for all connectors and tasks are written quite frequently and therefore the offset topic</span></span><br><span class="line"><span class="comment"># should be highly partitioned; by default it is created with 25 partitions, but adjust accordingly</span></span><br><span class="line"><span class="comment"># with the number of connector tasks deployed to a distributed worker cluster. Kafka Connect records</span></span><br><span class="line"><span class="comment"># the status less frequently, and so by default the topic is created with 5 partitions.</span></span><br><span class="line"><span class="meta">offset.storage.partitions</span>=<span class="string">25</span></span><br><span class="line"><span class="meta">status.storage.partitions</span>=<span class="string">5</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="Connector-配置"><a href="#Connector-配置" class="headerlink" title="Connector 配置"></a>Connector 配置</h4><h5 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unique name for the connector. Attempting to register again with the same name will fail.</span></span><br><span class="line">name</span><br><span class="line"><span class="comment"># The Java class for the connector</span></span><br><span class="line">connector.class</span><br><span class="line"><span class="comment"># The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</span></span><br><span class="line">tasks.max</span><br><span class="line"><span class="comment"># (optional) Override the default key converter class set by the worker.</span></span><br><span class="line">key.converter</span><br><span class="line"><span class="comment"># (optional) Override the default value converter class set by the worker.</span></span><br><span class="line">value.converter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sink connectors also have one additional option to control their input</span></span><br><span class="line"><span class="comment"># A list of topics to use as input for this connector</span></span><br><span class="line">topics</span><br></pre></td></tr></tbody></table></figure>
<h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h5><h6 id="单机版-Connector-配置"><a href="#单机版-Connector-配置" class="headerlink" title="单机版 Connector 配置"></a>单机版 Connector 配置</h6><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name</span>=<span class="string">local-file-sink</span></span><br><span class="line"><span class="meta">connector.class</span>=<span class="string">FileStreamSinkConnector</span></span><br><span class="line"><span class="meta">tasks.max</span>=<span class="string">1</span></span><br><span class="line"><span class="attr">file</span>=<span class="string">test.sink.txt</span></span><br><span class="line"><span class="attr">topics</span>=<span class="string">connect-test</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="分布式-Connector-配置"><a href="#分布式-Connector-配置" class="headerlink" title="分布式 Connector 配置"></a>分布式 Connector <a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/userguide.html#distributed-worker-configuration">配置</a></h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X POST -H <span class="string">"Content-Type: application/json"</span> --data <span class="string">'{"name": "local-file-sink", "config": {"connector.class":"FileStreamSinkConnector", "tasks.max":"1", "file":"test.sink.txt", "topics":"connect-test" }}'</span> http://localhost:8083/connectors</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者直接指定一个包含了 JSON 格式的配置文件</span></span><br><span class="line">$ curl -X POST -H <span class="string">"Content-Type: application/json"</span> --data @config.json http://localhost:8083/connectors</span><br></pre></td></tr></tbody></table></figure>
<h3 id="RESTful-接口"><a href="#RESTful-接口" class="headerlink" title="RESTful 接口"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="http://kafka.apache.org/documentation.html#connect_rest">RESTful</a> 接口</h3><h4 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h4><p>　Since Kafka Connect is intended to be run as a service, it also provides a REST API for managing connectors. By default, this service runs on port 8083.</p>
<p>　The following are the currently supported endpoints:</p>
<ul>
<li><code>GET /connectors</code> - return a list of active connectors</li>
<li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
<li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
<li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
<li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
<li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
<li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
<li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
<li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
<li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
<li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
<li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
<li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
</ul>
<p>　Kafka Connect also provides a REST API for getting information about connector plugins:</p>
<ul>
<li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
<li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
</ul>
<h4 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h4><h5 id="Worker-版本信息"><a href="#Worker-版本信息" class="headerlink" title="Worker 版本信息"></a>Worker 版本信息</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/ | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"version"</span>: <span class="string">"0.10.0.1-cp1"</span>,</span><br><span class="line">  <span class="attr">"commit"</span>: <span class="string">"ea5fcd28195f168b"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="Worker-支持的-Connector-插件"><a href="#Worker-支持的-Connector-插件" class="headerlink" title="Worker 支持的 Connector 插件"></a>Worker 支持的 Connector 插件</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/connector-plugins | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.elasticsearch.ElasticsearchSinkConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"sink"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.3.1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.hdfs.HdfsSinkConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"sink"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.3.1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.hdfs.tools.SchemaSourceConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"source"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.11.0.0-cp1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.jdbc.JdbcSinkConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"sink"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.3.1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.jdbc.JdbcSourceConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"source"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.3.1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.s3.S3SinkConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"sink"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"3.3.1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"io.confluent.connect.storage.tools.SchemaSourceConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"source"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.11.0.0-cp1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"org.apache.kafka.connect.file.FileStreamSinkConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"sink"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.11.0.0-cp1"</span></span><br><span class="line">  },</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"class"</span>: <span class="string">"org.apache.kafka.connect.file.FileStreamSourceConnector"</span>,</span><br><span class="line">    <span class="attr">"type"</span>: <span class="string">"source"</span>,</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.11.0.0-cp1"</span></span><br><span class="line">  }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<h5 id="Worker-上激活的-Connector-插件"><a href="#Worker-上激活的-Connector-插件" class="headerlink" title="Worker 上激活的 Connector 插件"></a>Worker 上激活的 Connector 插件</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/connectors | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  <span class="string">"file-source"</span>,</span><br><span class="line">  <span class="string">"file-sink"</span></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<h5 id="重启-Connector"><a href="#重启-Connector" class="headerlink" title="重启 Connector"></a>重启 Connector</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X POST localhost:8083/connectors/file-sink/restart</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>如果成功了，不会返回任何信息</li>
<li>如果失败了，会打印如下类似信息</li>
</ul>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"error_code"</span>: <span class="number">404</span>,</span><br><span class="line">  <span class="attr">"message"</span>: <span class="string">"Unknown connector: local-file-sink"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="获得某个-Connector上的所有-Tasks"><a href="#获得某个-Connector上的所有-Tasks" class="headerlink" title="获得某个 Connector上的所有 Tasks"></a>获得某个 Connector上的所有 Tasks</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/connectors/file-sink/tasks | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  {</span><br><span class="line">    <span class="attr">"id"</span>: {</span><br><span class="line">      <span class="attr">"connector"</span>: <span class="string">"file-sink"</span>,</span><br><span class="line">      <span class="attr">"task"</span>: <span class="number">0</span></span><br><span class="line">    },</span><br><span class="line">    <span class="attr">"config"</span>: {</span><br><span class="line">      <span class="attr">"topics"</span>: <span class="string">"connect-test"</span>,</span><br><span class="line">      <span class="attr">"file"</span>: <span class="string">"test.sink.txt"</span>,</span><br><span class="line">      <span class="attr">"task.class"</span>: <span class="string">"org.apache.kafka.connect.file.FileStreamSinkTask"</span></span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<h5 id="重启-Task"><a href="#重启-Task" class="headerlink" title="重启 Task"></a>重启 Task</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X POST localhost:8083/connectors/file-sink/tasks/0/restart | jq</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>如果成功了，不会返回任何信息</li>
<li>如果失败了，会打印如下类似信息</li>
</ul>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"error_code"</span>: <span class="number">404</span>,</span><br><span class="line">  <span class="attr">"message"</span>: <span class="string">"Unknown task: file-sink-1"</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="暂停-Connector"><a href="#暂停-Connector" class="headerlink" title="暂停 Connector"></a>暂停 Connector</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X PUT localhost:8083/connectors/file-sink/pause | jq</span><br></pre></td></tr></tbody></table></figure>
<h5 id="恢复-Connector"><a href="#恢复-Connector" class="headerlink" title="恢复 Connector"></a>恢复 Connector</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X PUT localhost:8083/connectors/file-sink/resume | jq</span><br></pre></td></tr></tbody></table></figure>
<h5 id="更新-Connector-配置信息"><a href="#更新-Connector-配置信息" class="headerlink" title="更新 Connector 配置信息"></a>更新 Connector 配置信息</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X PUT -H <span class="string">"Content-Type: application/json"</span> --data <span class="string">'{"connector.class":"FileStreamSinkConnector","file":"test.sink.txt","tasks.max":"2","topics":"connect-test","name":"local-file-sink"}'</span> localhost:8083/connectors/<span class="built_in">local</span>-file-sink/config</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"local-file-sink"</span>,</span><br><span class="line">  <span class="attr">"config"</span>: {</span><br><span class="line">    <span class="attr">"connector.class"</span>: <span class="string">"FileStreamSinkConnector"</span>,</span><br><span class="line">    <span class="attr">"file"</span>: <span class="string">"test.sink.txt"</span>,</span><br><span class="line">    <span class="attr">"tasks.max"</span>: <span class="string">"2"</span>,</span><br><span class="line">    <span class="attr">"topics"</span>: <span class="string">"connect-test"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"local-file-sink"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="attr">"tasks"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="attr">"connector"</span>: <span class="string">"local-file-sink"</span>,</span><br><span class="line">      <span class="attr">"task"</span>: <span class="number">0</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="attr">"connector"</span>: <span class="string">"local-file-sink"</span>,</span><br><span class="line">      <span class="attr">"task"</span>: <span class="number">1</span></span><br><span class="line">    }</span><br><span class="line">  ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="获取-Connector-状态"><a href="#获取-Connector-状态" class="headerlink" title="获取 Connector 状态"></a>获取 Connector 状态</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/connectors/file-sink/status | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"file-sink"</span>,</span><br><span class="line">  <span class="attr">"connector"</span>: {</span><br><span class="line">    <span class="attr">"state"</span>: <span class="string">"RUNNING"</span>,</span><br><span class="line">    <span class="attr">"worker_id"</span>: <span class="string">"192.168.1.101:8083"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="attr">"tasks"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="attr">"state"</span>: <span class="string">"RUNNING"</span>,</span><br><span class="line">      <span class="attr">"id"</span>: <span class="number">0</span>,</span><br><span class="line">      <span class="attr">"worker_id"</span>: <span class="string">"192.168.1.101:8083"</span></span><br><span class="line">    }</span><br><span class="line">  ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="获取-Connector-配置信息"><a href="#获取-Connector-配置信息" class="headerlink" title="获取 Connector 配置信息"></a>获取 Connector 配置信息</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8083/connectors/file-sink | jq</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"file-sink"</span>,</span><br><span class="line">  <span class="attr">"config"</span>: {</span><br><span class="line">    <span class="attr">"topics"</span>: <span class="string">"connect-test"</span>,</span><br><span class="line">    <span class="attr">"file"</span>: <span class="string">"test.sink.txt"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"file-sink"</span>,</span><br><span class="line">    <span class="attr">"tasks.max"</span>: <span class="string">"1"</span>,</span><br><span class="line">    <span class="attr">"connector.class"</span>: <span class="string">"FileStreamSink"</span></span><br><span class="line">  },</span><br><span class="line">  <span class="attr">"tasks"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="attr">"connector"</span>: <span class="string">"file-sink"</span>,</span><br><span class="line">      <span class="attr">"task"</span>: <span class="number">0</span></span><br><span class="line">    }</span><br><span class="line">  ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h5 id="删除-Connector"><a href="#删除-Connector" class="headerlink" title="删除 Connector"></a>删除 Connector</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -X DELETE localhost:8083/connectors/file-sink</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: 每个 Connector 进程在启动的时候，都会内置地启动一个 REST 服务端（默认端口 8083）</p>
<h3 id="Schema-Registry"><a href="#Schema-Registry" class="headerlink" title="Schema Registry"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/schema-registry/docs/index.html">Schema Registry</a></h3><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有 Schema</span></span><br><span class="line">$ curl -X GET http://localhost:8081/subjects | jq</span><br><span class="line">  [</span><br><span class="line">    <span class="string">"hdfs_sink_13-value"</span>,</span><br><span class="line">    <span class="string">"kafka-connect-ui-file-sink-value"</span>,</span><br><span class="line">    <span class="string">"hdfs_sink_16-value"</span>,</span><br><span class="line">    <span class="string">"hdfs_sink_17-value"</span>,</span><br><span class="line">    <span class="string">"hdfs_sink_18-value"</span></span><br><span class="line">  ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 hdfs_sink_13-value 的所有版本</span></span><br><span class="line">$ curl -X DELETE http://localhost:8081/subjects/hdfs_sink_13-value</span><br><span class="line">  [1]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 hdfs_sink_16-value 的第一个版本</span></span><br><span class="line">$ curl -X DELETE http://localhost:8081/subjects/hdfs_sink_16-value/versions/1</span><br><span class="line">  1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 hdfs_sink_17-value 最后一个版本</span></span><br><span class="line">$ curl -X DELETE http://localhost:8081/subjects/hdfs_sink_17-value/versions/latest</span><br><span class="line">  1</span><br></pre></td></tr></tbody></table></figure>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><h4 id="Confluent-UI"><a href="#Confluent-UI" class="headerlink" title="Confluent UI"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/4.0.0/cloud-quickstart.html">Confluent UI</a></h4><p>　未开源</p>
<h4 id="Landoop-UI"><a href="#Landoop-UI" class="headerlink" title="Landoop UI"></a>Landoop UI</h4><h5 id="kafka-connect-ui"><a href="#kafka-connect-ui" class="headerlink" title="kafka-connect-ui"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/Landoop/kafka-connect-ui">kafka-connect-ui</a></h5><h6 id="启动-3"><a href="#启动-3" class="headerlink" title="启动"></a>启动</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/Landoop/kafka-connect-ui.git</span><br><span class="line">$ <span class="built_in">cd</span> kafka-connect-ui</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ npm install -g bower http-server</span><br><span class="line">$ npm install</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ http-server -p 8080 .</span><br></pre></td></tr></tbody></table></figure>
<h6 id="开启-Confluent-Rest"><a href="#开启-Confluent-Rest" class="headerlink" title="开启 Confluent Rest"></a>开启 Confluent Rest</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开 REST 接口</span></span><br><span class="line">$ <span class="built_in">cd</span> software/confluent</span><br><span class="line">$ vim ./etc/kafka/connect-distributed.properties</span><br><span class="line">  <span class="comment"># These are provided to inform the user about the presence of the REST host and port configs</span></span><br><span class="line">  <span class="comment"># Hostname &amp; Port for the REST API to listen on. If this is set, it will bind to the interface used to listen to requests.</span></span><br><span class="line">  rest.host.name=0.0.0.0</span><br><span class="line">  rest.port=8083</span><br><span class="line">  <span class="comment"># 如果不设置这两个配置，Worker 节点之间将无法互相识别</span></span><br><span class="line">  <span class="comment"># The Hostname &amp; Port that will be given out to other workers to connect to i.e. URLs that are routable from other servers.</span></span><br><span class="line">  rest.advertised.host.name=192.168.1.101  <span class="comment"># 当前机器 IP 地址</span></span><br><span class="line">  rest.advertised.port=8083</span><br><span class="line">  <span class="comment"># 如果不设置这两个配置，页面上将会看到 prod http://xxxx:8083 N/A N/A N/A</span></span><br><span class="line">  access.control.allow.methods=GET,POST,PUT,DELETE,OPTIONS</span><br><span class="line">  access.control.allow.origin=*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新启动 Worker</span></span><br><span class="line">$ nohup ./bin/connect-distributed ./etc/kafka/connect-distributed.properties &gt; connect-distribute.log &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 浏览器访问，验证</span></span><br><span class="line">$ curl http://192.168.1.101:8083/</span><br><span class="line">  {<span class="string">"version"</span>:<span class="string">"0.11.0.1-cp1"</span>,<span class="string">"commit"</span>:<span class="string">"3735a6ca8b6432db"</span>}</span><br></pre></td></tr></tbody></table></figure>
<h6 id="创建-File-Sink"><a href="#创建-File-Sink" class="headerlink" title="创建 File Sink"></a>创建 File Sink</h6><ul>
<li><p>选择 Kafka Connect 集群<br><img data-src="/picture/kafka/kafka_connect_ui_configured_clusters.png" alt="Kafka Connector UI Configured Clusters"></p>
</li>
<li><p>查看 Dashboard<br><img data-src="/picture/kafka/kafka_connect_ui_dashboard.png" alt="Kafka Connector UI Dashboard"></p>
</li>
<li><p>点击 Create 按钮<br><img data-src="/picture/kafka/kafka_connect_ui_new_connector.png" alt="Kafka Connector UI New Connector"></p>
</li>
<li><p>配置 Connector<br><img data-src="/picture/kafka/kafka_connect_ui_new_connector_config.png" alt="Kafka Connector UI New Connector Config"></p>
</li>
<li><p>创建成功<br><img data-src="/picture/kafka/kafka_connect_ui_new_connector_created.png" alt="Kafka Connector UI Created"></p>
</li>
<li><p>查看 Connector 详情<br><img data-src="/picture/kafka/kafka_connect_ui_new_connector_detail.png" alt="Kafka Connector UI New Connector Detail"></p>
</li>
</ul>
<h6 id="发送数据"><a href="#发送数据" class="headerlink" title="发送数据"></a>发送数据</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic kafka-connect-ui-file-sink --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span></span><br><span class="line">  <span class="comment"># 发送数据</span></span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value1"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value2"</span>}</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"value3"</span>}</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 持续发送</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> {1..3}; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"{\"f1\": \"value<span class="variable">$i</span>\"}"</span> | ./bin/kafka-avro-console-producer --broker-list localhost:9092 --topic kafka-connect-ui-file-sink --property value.schema=<span class="string">'{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'</span>; <span class="keyword">done</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="接收成功"><a href="#接收成功" class="headerlink" title="接收成功"></a>接收成功</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat ./software/confluent-3.3.1/kafka-connect-ui-file-sink.txt</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Struct{f1=value1}</span><br><span class="line">Struct{f1=value2}</span><br><span class="line">Struct{f1=value3}</span><br></pre></td></tr></tbody></table></figure>
<h6 id="创建-File-Source"><a href="#创建-File-Source" class="headerlink" title="创建 File Source"></a>创建 File Source</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 具体操作步骤，参考上述 File Sink 创建过程，配置如下</span></span><br><span class="line">name=FileStreamSourceConnector</span><br><span class="line">connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector</span><br><span class="line">file=kafka-connect-ui-file-source.txt</span><br><span class="line">tasks.max=1</span><br><span class="line">topic=kafka-connect-ui-file-sink</span><br></pre></td></tr></tbody></table></figure>
<p>　查看 Dashboard 效果如下：</p>
<p><img data-src="/picture/kafka/kafka_connect_ui_file_source_and_sink.png" alt="Kafka Connector UI File Source and Sink"></p>
<h6 id="写入数据到文件"><a href="#写入数据到文件" class="headerlink" title="写入数据到文件"></a>写入数据到文件</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/connect/software/confluent</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"{\"f1\": \"400\"}"</span> &gt; kafka-connect-ui-file-source.txt</span><br><span class="line">$ tail -f kafka-connect-ui-file-sink.txt</span><br><span class="line">  {<span class="string">"f1"</span>: <span class="string">"400"</span>}</span><br><span class="line"><span class="comment"># 可以看到数据已经落入到 File Sink 配置的本地文件中了</span></span><br><span class="line"><span class="comment"># 但是，数据格式和直接发送带 Schema 的数据给 Kafka，然后直接传输给 File Sink 的数据（Struct{f1=397}），有所不同</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="踩过的坑-3"><a href="#踩过的坑-3" class="headerlink" title="踩过的坑"></a>踩过的坑</h6><p>描述</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.connect.errors.ConnectException: org.apache.hadoop.security.AccessControlException: Permission denied: user=connect, access=WRITE, inode=<span class="string">"/"</span>:bigdata:supergroup:drwxr-xr-x</span><br></pre></td></tr></tbody></table></figure>
<p>解决</p>
<p>　创建 Kafka 2 HDFS 任务时，指定 <code>logs.dir=/user/connect/logs</code> 参数</p>
<p>　a) Connector 和 HDFS 集群均未报错，但是数据无法落 HDFS</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 除了基本的配置外，</span></span><br><span class="line">connector.class=io.confluent.connect.hdfs.HdfsSinkConnector</span><br><span class="line">topics.dir=/user/connect/topics</span><br><span class="line">flush.size=1</span><br><span class="line">topics=hdfs_sink_16</span><br><span class="line">tasks.max=1</span><br><span class="line">hdfs.url=hdfs://192.168.1.101:9000</span><br><span class="line">logs.dir=/user/connect/logs</span><br><span class="line">schema.cache.size=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据格式相关配置，进行显式指定</span></span><br><span class="line">format.class=io.confluent.connect.hdfs.avro.AvroFormat</span><br><span class="line">key.converter=io.confluent.connect.avro.AvroConverter</span><br><span class="line">key.converter.schema.registry.url=http://localhost:8081</span><br><span class="line">value.converter=io.confluent.connect.avro.AvroConverter</span><br><span class="line">value.converter.schema.registry.url=http://localhost:8081</span><br></pre></td></tr></tbody></table></figure>
<p>　b) Missing required configuration “schema.registry.url” which has no default value</p>
<p>　需要指定 <code>schemas.enable=false</code> 配置项</p>
<p>Tips: Live demo is <a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka-connect-ui.landoop.com">here</a>.</p>
<h5 id="kafka-topic-ui"><a href="#kafka-topic-ui" class="headerlink" title="kafka-topic-ui"></a>kafka-topic-ui</h5><h6 id="启动-4"><a href="#启动-4" class="headerlink" title="启动"></a>启动</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/Landoop/kafka-topics-ui.git</span><br><span class="line">$ <span class="built_in">cd</span> kafka-topics-ui</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ npm install -g bower</span><br><span class="line">$ npm install -g http-server</span><br><span class="line">$ npm install</span><br><span class="line">$ bower install</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ http-server -p 8081 .</span><br></pre></td></tr></tbody></table></figure>
<h6 id="启动-Kafka-Rest"><a href="#启动-Kafka-Rest" class="headerlink" title="启动 Kafka Rest"></a>启动 Kafka Rest</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka-rest/kafka-rest.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">id</span>=<span class="string">kafka-rest-test-server</span></span><br><span class="line"><span class="meta">schema.registry.url</span>=<span class="string">http://localhost:8081</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">192.168.1.101:2015,192.168.1.102:2015,192.168.1.103:2015</span></span><br><span class="line"><span class="comment"># 显式地指定 bootstrap server</span></span><br><span class="line"><span class="meta">bootstrap.servers</span>=<span class="string">192.168.1.101:9092,192.168.1.102:9092,192.168.1.103:9092</span></span><br><span class="line"><span class="comment"># 开启 Web 访问的权限</span></span><br><span class="line"><span class="meta">access.control.allow.methods</span>=<span class="string">GET,POST,PUT,DELETE,OPTIONS</span></span><br><span class="line"><span class="meta">access.control.allow.origin</span>=<span class="string">*</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h6><p><img data-src="/picture/kafka/kafka_topics_ui_list.png" alt="Kafka Topics UI"></p>
<h6 id="踩过的坑-4"><a href="#踩过的坑-4" class="headerlink" title="踩过的坑"></a>踩过的坑</h6><p>描述</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARN Connection to node -1 could not be established. Broker may not be available.</span><br></pre></td></tr></tbody></table></figure>
<p>解决</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/kafka-rest/kafka-rest.properties</span><br><span class="line">  <span class="comment"># 显式地指定 bootstrap server</span></span><br><span class="line">  bootstrap.servers=192.168.1.101:9092,192.168.1.102:9092,192.168.1.103:9092</span><br></pre></td></tr></tbody></table></figure>
<h5 id="schema-registry-ui"><a href="#schema-registry-ui" class="headerlink" title="schema-registry-ui"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/landoop/schema-registry-ui">schema-registry-ui</a></h5><h6 id="启动-5"><a href="#启动-5" class="headerlink" title="启动"></a>启动</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/Landoop/schema-registry-ui.git</span><br><span class="line">$ <span class="built_in">cd</span> schema-registry-ui</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ npm install</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">$ npm start</span><br></pre></td></tr></tbody></table></figure>
<h6 id="启动-Schema-Registry"><a href="#启动-Schema-Registry" class="headerlink" title="启动 Schema Registry"></a>启动 Schema Registry</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim etc/schema-registry/schema-registry.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">listeners</span>=<span class="string">http://0.0.0.0:8081</span></span><br><span class="line"><span class="meta">kafkastore.connection.url</span>=<span class="string">192.168.1.101:2015,192.168.1.102:2015,192.168.1.103:2015</span></span><br><span class="line"><span class="meta">kafkastore.topic</span>=<span class="string">_schemas</span></span><br><span class="line"><span class="attr">debug</span>=<span class="string">false</span></span><br><span class="line"><span class="comment"># 开启 Web 访问的权限</span></span><br><span class="line"><span class="meta">access.control.allow.methods</span>=<span class="string">GET,POST,PUT,DELETE,OPTIONS</span></span><br><span class="line"><span class="meta">access.control.allow.origin</span>=<span class="string">*</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="效果图-1"><a href="#效果图-1" class="headerlink" title="效果图"></a>效果图</h6><p><img data-src="/picture/kafka/kafka_schema_ui_list.png" alt="Kafka Schema Register UI"></p>
<h6 id="二次开发"><a href="#二次开发" class="headerlink" title="二次开发"></a>二次开发</h6><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cmd 命令行中启动</span></span><br><span class="line">$ npm start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝浏览器中的 `http://localhost:8080/#/cluster/prod` 连接</span></span><br><span class="line"><span class="comment"># 并在 WebStorm 里，创建 `JavaScript Debug`，并粘贴 URL 连接，运行即可</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="使用社区-Connector"><a href="#使用社区-Connector" class="headerlink" title="使用社区 Connector"></a>使用社区 Connector</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">$ git <span class="built_in">clone</span> git@github.com:confluentinc/kafka-connect-hdfs.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切换至稳定版本，并打包编译</span></span><br><span class="line">$ <span class="built_in">cd</span> kafka-connect-hdfs; git checkout v3.0.1; mvn package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 plugins 目录下创建对应的 kafka-connect-hdfs 子目录</span></span><br><span class="line">$ mkdir -p /usr/<span class="built_in">local</span>/share/kafka/plugins/kafka-connect-hdfs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝编译出来的 jar 包到 plugins 目录</span></span><br><span class="line">$ cp target/kafka-connect-hdfs-3.0.1-package/share/java/kafka-connect-hdfs/* /usr/<span class="built_in">local</span>/share/kafka/plugins/kafka-connect-hdfs/</span><br></pre></td></tr></tbody></table></figure>
<h3 id="实现自己的-Connector"><a href="#实现自己的-Connector" class="headerlink" title="实现自己的 Connector"></a>实现自己的 Connector</h3><p>Tips: <a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.confluent.io/current/connect/devguide.html">Connector Developer Guide</a></p>
<h3 id="修复已知-Bug"><a href="#修复已知-Bug" class="headerlink" title="修复已知 Bug"></a>修复已知 Bug</h3><p>　使用 jira 语法<a target="_blank" rel="external nofollow noopener noreferrer" href="https://issues.apache.org/jira/browse/KAFKA-6253?jql=project%20%3D%20KAFKA%20AND%20issuetype%20%3D%20Bug%20AND%20component%20%3D%20KafkaConnect">查询</a>出 Kafka Connect 组件的已知 bug，具体语法如下</p>
<figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">project = KAFKA AND issuetype = Bug AND component = KafkaConnect</span><br></pre></td></tr></tbody></table></figure>
<h2 id="实用技巧"><a href="#实用技巧" class="headerlink" title="实用技巧"></a>实用技巧</h2><h3 id="zkCli"><a href="#zkCli" class="headerlink" title="zkCli"></a>zkCli</h3><div class="table-container">
<table>
<thead>
<tr>
<th>Command</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>get /consumers/<code>&lt;topic&gt;</code>/owners</td>
<td>查看 topic 实时消费的 group id</td>
</tr>
<tr>
<td>get /consumers/<code>&lt;topic&gt;</code>/offsets/<code>&lt;group id&gt;</code>/<code>&lt;partitionor&gt;</code></td>
<td>查看 offset 情况（ctime: 创建时间; mtime: 修改时间）</td>
</tr>
</tbody>
</table>
</div>
<h3 id="kafka-run-class"><a href="#kafka-run-class" class="headerlink" title="kafka-run-class"></a>kafka-run-class</h3><h4 id="删除-Topic-1"><a href="#删除-Topic-1" class="headerlink" title="删除 Topic"></a>删除 Topic</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper &lt;zk host&gt;:2181,&lt;zk host&gt;:2181,&lt;zk host&gt;:2181 --topic &lt;topic&gt;</span><br></pre></td></tr></tbody></table></figure>
<h3 id="Consumer-指定-offset-进行消费，从而达到补数的效果"><a href="#Consumer-指定-offset-进行消费，从而达到补数的效果" class="headerlink" title="Consumer 指定 offset 进行消费，从而达到补数的效果"></a>Consumer 指定 offset 进行消费，从而达到补数的效果</h3><h4 id="Java-Client"><a href="#Java-Client" class="headerlink" title="Java Client"></a>Java Client</h4><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, bootstrapServers);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, groupId);</span><br><span class="line">props.put(<span class="string">"enable.auto.commit"</span>, <span class="keyword">false</span>);</span><br><span class="line">props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">props.put(<span class="string">"session.timeout.ms"</span>, <span class="string">"30000"</span>);</span><br><span class="line">props.put(<span class="string">"max.poll.records"</span>, maxPollRecords);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"><span class="comment">// consumer.subscribe(Arrays.asList(topic));</span></span><br><span class="line"></span><br><span class="line">TopicPartition p = <span class="keyword">new</span> TopicPartition(topic, <span class="number">2</span>);</span><br><span class="line">consumer.assign(Arrays.asList(p));</span><br><span class="line">consumer.seek(p, <span class="number">1024</span>);</span><br><span class="line"><span class="comment">// while (true) {</span></span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) {</span><br><span class="line">    String V = record.value();</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  }</span><br><span class="line"><span class="comment">// }</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="Shell-Console"><a href="#Shell-Console" class="headerlink" title="Shell Console"></a>Shell Console</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定从哪个 offset 开始消费</span></span><br><span class="line">$ kafka-console-consumer --bootstrap-server localhost:9092 --topic topic-1 --consumer.config  config/consumer.properties --offset 90 --partition 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 offset 重置到某一个 offset</span></span><br><span class="line">$ kafka-consumer-groups --bootstrap-server kafka-host:9092 --group my-group --reset-offsets --to-offset 1024 --all-topics --execute</span><br></pre></td></tr></tbody></table></figure>
<h3 id="哪些配置可以在-Kafka-Connect-中进行指定"><a href="#哪些配置可以在-Kafka-Connect-中进行指定" class="headerlink" title="哪些配置可以在 Kafka Connect 中进行指定"></a>哪些配置可以在 Kafka Connect 中进行指定</h3><p>　<a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/documentation/#newconsumerconfigs">New Consumer Configs</a></p>
<h3 id="优雅地停止-Kafka-服务"><a href="#优雅地停止-Kafka-服务" class="headerlink" title="优雅地停止 Kafka 服务"></a>优雅地停止 Kafka 服务</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim config/server.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">controlled.shutdown.enable</span>=<span class="string">true</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-server-stop.sh</span><br></pre></td></tr></tbody></table></figure>
<h3 id="数据过期策略"><a href="#数据过期策略" class="headerlink" title="数据过期策略"></a>数据过期策略</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim config/server.properties</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log.cleanup.policy</span>=<span class="string">delete</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h2><h3 id="架构层面"><a href="#架构层面" class="headerlink" title="架构层面"></a>架构层面</h3><h4 id="RingBuffer-Lock-Free"><a href="#RingBuffer-Lock-Free" class="headerlink" title="RingBuffer + Lock Free"></a>RingBuffer + Lock Free</h4><p>　使用 <code>RingBuffer + Lock Free</code> 实现高效的 <code>Producer-Consumer</code> 设计模式，极大地提升 Kafka Producer 发送 Message 的性能</p>
<h4 id="Avro-压缩"><a href="#Avro-压缩" class="headerlink" title="Avro 压缩"></a>Avro 压缩</h4><p>　使用 Avro 压缩，保证了高效地解压缩数据的同时，可以减少网络传输的数据量</p>
<h4 id="多-Partitioner-连接池"><a href="#多-Partitioner-连接池" class="headerlink" title="多 Partitioner + 连接池"></a>多 Partitioner + 连接池</h4><p>　使用连接池可以动态地增减所需的 Kafka 连接，并自己实现 Kafka Partitioner 充分利用多 Partitioner 提高并发度</p>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/asdf2014/yuzhouwan/tree/master/yuzhouwan-bigdata/yuzhouwan-bigdata-kafka/src/main/java/com/yuzhouwan/bigdata/kafka/util">here</a>.</p>
<h3 id="参数层面"><a href="#参数层面" class="headerlink" title="参数层面"></a>参数层面</h3><h4 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h4><figure class="highlight properties"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">kafka.key.serializer.class</span>=<span class="string">kafka.serializer.StringEncoder</span></span><br><span class="line"><span class="meta">kafka.serializer.class</span>=<span class="string">kafka.serializer.DefaultEncoder</span></span><br><span class="line"><span class="meta">kafka.request.required.acks</span>=<span class="string">0</span></span><br><span class="line"><span class="meta">kafka.async</span>=<span class="string">async</span></span><br><span class="line"><span class="meta">kafka.queue.buffering.max.ms</span>=<span class="string">5000</span></span><br><span class="line"><span class="meta">kafka.queue.buffering.max.messages</span>=<span class="string">10000</span></span><br><span class="line"><span class="meta">kafka.queue.enqueue.timeout.ms</span>=<span class="string">-1</span></span><br><span class="line"><span class="meta">kafka.batch.num.messages</span>=<span class="string">200</span></span><br><span class="line"><span class="meta">kafka.send.buffer.bytes</span>=<span class="string">102400</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="踩到的坑"><a href="#踩到的坑" class="headerlink" title="踩到的坑"></a>踩到的坑</h3><h4 id="Avro-中-Decimal-字段反序列化之后，全部变成了-pos-0-lim-0-cap-0"><a href="#Avro-中-Decimal-字段反序列化之后，全部变成了-pos-0-lim-0-cap-0" class="headerlink" title="Avro 中 Decimal 字段反序列化之后，全部变成了 [pos=0 lim=0 cap=0]"></a>Avro 中 Decimal 字段反序列化之后，全部变成了 [pos=0 lim=0 cap=0]</h4><h5 id="描述-4"><a href="#描述-4" class="headerlink" title="描述"></a>描述</h5><figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 正常进行 double 转 bytebuffer，并从 bytebuffer 转为 double 都是可以的</span></span><br><span class="line"><span class="keyword">val</span> bb: <span class="type">ByteBuffer</span> = <span class="type">ByteBuffer</span>.allocate(<span class="number">8</span>).putDouble(<span class="string">"666.8"</span>.toDouble)</span><br><span class="line"><span class="keyword">val</span> dd: <span class="type">Double</span> = <span class="type">ByteBuffer</span>.wrap(bb.array()).getDouble()</span><br><span class="line"><span class="comment">// 但是，传入 avro 类中，作为 decimal 类型的字段值，然后进行序列化 和 反序列化，就会出现问题</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="解决-4"><a href="#解决-4" class="headerlink" title="解决"></a>解决</h5><ul>
<li>定义 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://avro.apache.org/docs/current/spec.html#Decimal">decimal</a> 类型字段</li>
</ul>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"namespace"</span>: <span class="string">"com.yuzhouwan.bean"</span>,</span><br><span class="line">  <span class="attr">"type"</span>: <span class="string">"record"</span>,</span><br><span class="line">  <span class="attr">"name"</span>: <span class="string">"BeanA"</span>,</span><br><span class="line">  <span class="attr">"fields"</span>: [</span><br><span class="line">    {<span class="attr">"name"</span>: <span class="string">"id"</span>, <span class="attr">"type"</span>: <span class="string">"int"</span>},</span><br><span class="line">    {<span class="attr">"name"</span>: <span class="string">"price"</span>, <span class="attr">"type"</span>: {<span class="attr">"type"</span>: <span class="string">"bytes"</span>, <span class="attr">"logicalType"</span>: <span class="string">"decimal"</span>, <span class="attr">"precision"</span>: <span class="number">8</span>, <span class="attr">"scale"</span>: <span class="number">4</span>}}</span><br><span class="line">  ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li><p>配置 Maven <a href="https://yuzhouwan.com/posts/2254/#Avro-插件">自动解析</a> Avro 类</p>
</li>
<li><p>初始化 Avro 类</p>
</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.avro.{<span class="type">Conversions</span>, <span class="type">LogicalTypes</span>, <span class="type">Schema</span>}</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> decimal: java.math.<span class="type">BigDecimal</span> = java.math.<span class="type">BigDecimal</span>.valueOf(<span class="string">"666.8"</span>.toDouble)</span><br><span class="line"><span class="keyword">val</span> logicalType: <span class="type">LogicalTypes</span>.<span class="type">Decimal</span> = <span class="type">LogicalTypes</span>.decimal(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> schema: <span class="type">Schema</span> = <span class="keyword">new</span> <span class="type">Schema</span>.<span class="type">Parser</span>().parse(</span><br><span class="line">  <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    |{</span></span><br><span class="line"><span class="string">    |  "</span><span class="string">namespace": "</span>com.yuzhouwan.<span class="string">bean",</span></span><br><span class="line"><span class="string">    |  "</span><span class="string">type": "</span><span class="string">record",</span></span><br><span class="line"><span class="string">    |  "</span><span class="string">name": "</span><span class="type">BeanA</span><span class="string">",</span></span><br><span class="line"><span class="string">    |  "</span><span class="string">fields": [</span></span><br><span class="line"><span class="string">    |    {"</span><span class="string">name": "</span><span class="string">id", "</span><span class="string">type": "</span><span class="string">int"},</span></span><br><span class="line"><span class="string">    |    {"</span><span class="string">name": "</span><span class="string">price", "</span><span class="string">type": {"</span><span class="string">type": "</span><span class="string">bytes", "</span>logicalT<span class="string">ype": "</span><span class="string">decimal", "</span><span class="string">precision": 8, "</span><span class="string">scale": 4}}</span></span><br><span class="line"><span class="string">    |  ]</span></span><br><span class="line"><span class="string">    |}</span></span><br><span class="line"><span class="string">  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line"><span class="keyword">val</span> conversion: <span class="type">ByteBuffer</span> = decimalConversion.toBytes(decimal, schema, logicalType)</span><br><span class="line"><span class="keyword">val</span> trans: <span class="type">Transaction</span> = <span class="keyword">new</span> <span class="type">Transaction</span>(<span class="number">1</span>, conversion)</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>反序列化 decimal 字段</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> precision = schema.getJsonProp(<span class="string">"precision"</span>)</span><br><span class="line"><span class="keyword">val</span> scale = schema.getJsonProp(<span class="string">"scale"</span>)</span><br><span class="line"><span class="keyword">val</span> logicalType = <span class="type">LogicalTypes</span>.decimal(precision.toString.toInt, scale.toString.toInt)</span><br><span class="line"><span class="keyword">val</span> d: java.math.<span class="type">BigDecimal</span> = decimalConversion.fromBytes(trans.getSale, schema, logicalType)</span><br></pre></td></tr></tbody></table></figure>
<h5 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h5><p>　看到这里，是不是能感觉出来 decimal 使用起来还是很麻烦的，而且踩到的坑还远不止这一个。那我们不禁要想，为啥非要发明这么一个东西呢，直接用 float（单精度浮点数）、double（双精度浮点数）岂不是很省事？</p>
<p>　其实不然，因为一旦遇到 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://0.30000000000000004.com/">0.30000000000000004</a> 问题，这些依照 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/IEEE_754">IEEE 754</a> 标准构建的浮点数（<a href="https://yuzhouwan.com/posts/27328/">Java</a> / <a href="https://yuzhouwan.com/posts/18651/">Scala</a> / <a href="https://yuzhouwan.com/posts/43687/">Python</a>…），都将束手无策。我们可以在命令行运行一个简单的例子，进行验证。这里以 Python 为例，运行 <code>.1 + .2</code> 后会发现，得到的结果不是预期的 <code>0.3</code> 而是 <code>0.30000000000000004</code>。如果是第一次看到这个结果，想必一定会三观崩塌，开始怀疑是不是下载了一个假 Python。然而，一旦理解其中原理，也就不足为奇了。而想要摸清门道也很简单，只需通过<strong>基数连除</strong>和<strong>连乘法</strong>，先将浮点数转换为二进制表示，再对其做加法操作即可。转换后，<code>0.1 + 0.2</code> 就变成了 <code>0.00011001100110011001100110011001100110011001100110011001</code> + <code>0.00110011001100110011001100110011001100110011001100110011</code>，运算后可以得到 <code>0.01001100110011001100110011001100110011001100110011001100</code>，再将二进制结果转换为十进制的浮点数，就是我们看到的 <code>0.30000000000000004</code> 了。从本质上来说，还是因为二进制是连续的，十进制是非连续的。为什么这么说呢？举个例子，小数点后四位用二进制表示时，数值范围为 <code>0.0000</code> 到 <code>0.1111</code> 之间。而对应到十进制，却只能表示出 <code>0.0625</code>、<code>0.125</code>、<code>0.25</code> 和 <code>0.5</code> 这四个数值的组合结果。另外，有些十进制的小数，转换为二进制之后，还会变成了循环小数。但是，计算机里面浮点数只能存有限的位数，所以，同样也无法精确地表示</p>
<p>　这时候，我们再来看，如果用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://docs.python.org/3/library/decimal.html">decimal</a> 则不会遇到这个问题：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> decimal <span class="keyword">import</span> *</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = Decimal(<span class="string">".1"</span>) + Decimal(<span class="string">".2"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">  Decimal(<span class="string">'0.3'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>float(d)</span><br><span class="line">  <span class="number">0.3</span></span><br></pre></td></tr></tbody></table></figure>
<p>　当然，除了上述精确计算的好处之外，decimal 还有便于控制精度、自动和输入保持精度一致，以及允许除以 0 得到 Infinity 等优点。如此看来，decimal 还真的不是一无是处呢 :D</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>Context(prec=<span class="number">6</span>, Emax=<span class="number">999</span>, clamp=<span class="number">1</span>).create_decimal(<span class="string">'1.23e999'</span>)</span><br><span class="line">  Decimal(<span class="string">'1.23000E+999'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">1.00</span> + <span class="number">0.10</span></span><br><span class="line">  <span class="number">1.1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Decimal(<span class="string">"1.00"</span>) + Decimal(<span class="string">"0.10"</span>)</span><br><span class="line">  Decimal(<span class="string">'1.10'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>setcontext(ExtendedContext)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Decimal(<span class="number">1</span>) / Decimal(<span class="number">0</span>)</span><br><span class="line">  Decimal(<span class="string">'Infinity'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Decimal(<span class="number">-1</span>) / Decimal(<span class="number">0</span>)</span><br><span class="line">  Decimal(<span class="string">'-Infinity'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>Decimal(<span class="number">1</span>) / Decimal(<span class="number">0</span>) + Decimal(<span class="number">-1</span>) / Decimal(<span class="number">0</span>)</span><br><span class="line">  Decimal(<span class="string">'NaN'</span>)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="Kafka-阅读环境搭建"><a href="#Kafka-阅读环境搭建" class="headerlink" title="Kafka 阅读环境搭建"></a>Kafka 阅读环境搭建</h3><h4 id="安装-gradle"><a href="#安装-gradle" class="headerlink" title="安装 gradle"></a>安装 gradle</h4><p>　在 gradle <a target="_blank" rel="external nofollow noopener noreferrer" href="https://gradle.org/releases/">下载页面</a>，下载 gradle-4.3.1-all.zip 文件，解压至 <code>D:\apps\gradle</code>，并添加环境变量 <code>PATH=D:\apps\gradle\gradle-4.3.1\bin</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查是否安装成功</span></span><br><span class="line">$ gradle -v</span><br><span class="line">  ------------------------------------------------------------</span><br><span class="line">  Gradle 4.3.1</span><br><span class="line">  ------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">  Build time:   2017-11-08 08:59:45 UTC</span><br><span class="line">  Revision:     e4f4804807ef7c2829da51877861ff06e07e006d</span><br><span class="line"></span><br><span class="line">  Groovy:       2.4.12</span><br><span class="line">  Ant:          Apache Ant(TM) version 1.9.6 compiled on June 29 2015</span><br><span class="line">  JVM:          1.8.0_111 (Oracle Corporation 25.111-b14)</span><br><span class="line">  OS:           Windows 7 6.1 amd64</span><br></pre></td></tr></tbody></table></figure>
<div class="note success">建议使用 gradlew 命令，可以避免手动安装</div>



<h4 id="gradle-代理设置"><a href="#gradle-代理设置" class="headerlink" title="gradle 代理设置"></a>gradle 代理设置</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ gradle xxx -Dhttp.proxyHost=127.0.0.1 -Dhttp.proxyPort=1080</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者修改 gradle.properties 配置文件</span></span><br><span class="line">$ vim gradle.properties</span><br><span class="line">  systemProp.http.proxyHost=192.168.1.101</span><br><span class="line">  systemProp.http.proxyPort=8080</span><br><span class="line">  systemProp.http.nonProxyHosts=*.nonproxyrepos.com|localhost</span><br><span class="line">  systemProp.https.proxyHost=192.168.1.101</span><br><span class="line">  systemProp.https.proxyPort=8080</span><br><span class="line">  systemProp.https.nonProxyHosts=*.nonproxyrepos.com|localhost</span><br><span class="line">  <span class="comment">#systemProp.http.proxyUser=userid</span></span><br><span class="line">  <span class="comment">#systemProp.http.proxyPassword=password</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 【注意】如果在 Kafka 源码目录下修改的 gradle.properties 无法生效，可以直接拷贝到 $USER_HOME/.gradle 目录下</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="编译源码"><a href="#编译源码" class="headerlink" title="编译源码"></a>编译源码</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载依赖</span></span><br><span class="line">$ ./gradlew</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加 `-x test` 参数，可跳过单元测试</span></span><br><span class="line">$ ./gradlew -x <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随后，生成可以用 Intellij Idea 打开的工程</span></span><br><span class="line">$ ./gradlew idea</span><br></pre></td></tr></tbody></table></figure>
<div class="note info">如果 gradle 的配置文件里面，没有设置 idea 插件，导致项目模块无法被识别。则需要删除 .idea 文件夹，并以 build.gradle 打开为项目，即可</div>



<h3 id="Kafka-Connect-2"><a href="#Kafka-Connect-2" class="headerlink" title="Kafka Connect"></a>Kafka Connect</h3><h4 id="阅读环境搭建"><a href="#阅读环境搭建" class="headerlink" title="阅读环境搭建"></a>阅读环境搭建</h4><p>　Kafka Connect 项目的 <strong>接口定义</strong> 和 <strong>组件模块化</strong> 做得相当到位，整个工程被拆成很多子项目。同时，搭建源码阅读环境的时候，会发现很多依赖在中央仓库中是找不到的，这时候需要下载源码进行本地编译安装。首先，下载 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/common"><strong>kafka-connect-common</strong></a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/kafka-connect-storage-common"><strong>kafka-connect-storage-common</strong></a> 两个父工程，以及相关的依赖子工程 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/rest-utils"><strong>rest-utils</strong></a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/confluentinc/schema-registry"><strong>schema-registry</strong></a> 的源码。通过 <code>git fetch --tags</code> 命令将 tags 全部下载下来之后，选定一个稳定版本 (这里以 v3.3.1 为例)，并创建新分支 <code>v3.3.1</code>，再在新分支下执行 <code>git reset --hard &lt;commit&gt;</code> 统一所有项目代码到 v3.3.1 版本 (这里还有一个依赖的子项目 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/julianhyde/aggdesigner"><strong>aggdesigner</strong></a> 也需要本地编译安装，对应版本为 <code>pentaho-aggdesigner-5.1.5-jhyde</code>)。另外，Kafka 也用了一个非常规的版本 <code>0.11.0.1-cp1</code>，这里只需要选择一个略高的版本 <code>0.11.0.1</code> 保证兼容性即可（当然也可以在 Kafka trunk 分支下执行 <code>gradlew installAll</code> 命令编译出对应版本），最后，依次执行 <code>mvn clean install</code> 命令完成打包编译</p>
<h4 id="主体架构"><a href="#主体架构" class="headerlink" title="主体架构"></a>主体架构</h4><h5 id="ConnectDistributed"><a href="#ConnectDistributed" class="headerlink" title="ConnectDistributed"></a>ConnectDistributed</h5><p>　Command line utility that runs Kafka Connect in distributed mode. In this mode, the process joints a group of other workers and work is distributed among them. This is useful for running Connect as a service, where connectors can be submitted to the cluster to be automatically executed in a scalable, distributed fashion. This also allows you to easily scale out horizontally, elastically adding or removing capacity simply by starting or stopping worker instances.</p>
<h5 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h5><p>　This class ties together all the components of a Kafka Connect process (herder, worker, storage, command interface), managing their lifecycle.</p>
<h5 id="Connector-1"><a href="#Connector-1" class="headerlink" title="Connector"></a>Connector</h5><p>　Connectors manage integration of Kafka Connect with another system, either as an input that ingests data into Kafka or an output that passes data to an external system. Implementations should not use this class directly; they should inherit from SourceConnector or SinkConnector.</p>
<p>　Connectors have two primary tasks. First, given some configuration, they are responsible for creating configurations for a set of <code>{@link Task}s</code> that split up the data processing. For example, a database Connector might create Tasks by dividing the set of tables evenly among tasks. Second, they are responsible for monitoring inputs for changes that require reconfiguration and notifying the Kafka Connect runtime via the ConnectorContext. Continuing the previous example, the connector might periodically check for new tables and notify Kafka Connect of additions and deletions. Kafka Connect will then request new configurations and update the running Tasks.</p>
<h6 id="SinkConnector"><a href="#SinkConnector" class="headerlink" title="SinkConnector"></a>SinkConnector</h6><p>　SourceConnectors implement the connector interface to pull data from another system and send it to Kafka.</p>
<h6 id="SourceConnector"><a href="#SourceConnector" class="headerlink" title="SourceConnector"></a>SourceConnector</h6><p>　SinkConnectors implement the Connector interface to send Kafka data to another system.</p>
<h5 id="Herder"><a href="#Herder" class="headerlink" title="Herder"></a>Herder</h5><p>　The herder interface tracks and manages workers and connectors. It is the main interface for external components to make changes to the state of the cluster. For example, in distributed mode, an implementation of this class knows how to accept a connector configuration, may need to route it to the current leader worker for the cluster so the config can be written to persistent storage, and then ensures the new connector is correctly instantiated on one of the workers.</p>
<p>　This class must implement all the actions that can be taken on the cluster (add/remove connectors, pause/resume tasks, get state of connectors and tasks, etc). The non-Java interfaces to the cluster (REST API and CLI) are very simple wrappers of the functionality provided by this interface.</p>
<p>　In standalone mode, this implementation of this class will be trivial because no coordination is needed. In that case, the implementation will mainly be delegating tasks directly to other components. For example, when creating a new connector in standalone mode, there is no need to persist the config and the connector and its tasks must run in the same process, so the standalone herder implementation can immediately instantiate and start the connector and its tasks.</p>
<h5 id="Worker-1"><a href="#Worker-1" class="headerlink" title="Worker"></a>Worker</h5><p>　Worker runs a (dynamic) set of tasks in a set of threads, doing the work of actually moving<br>data to/from Kafka.</p>
<p>　Since each task has a dedicated thread, this is mainly just a container for them.</p>
<h5 id="Task-1"><a href="#Task-1" class="headerlink" title="Task"></a>Task</h5><p>　Tasks contain the code that actually copies data to/from another system. They receive a configuration from their parent Connector, assigning them a fraction of a Kafka Connect job´s work. The Kafka Connect framework then pushes/pulls data from the Task. The Task must also be able to respond to reconfiguration requests.</p>
<p>　Task only contains the minimal shared functionality between<br><code>{@link org.apache.kafka.connect.source.SourceTask}</code> and<br><code>{@link org.apache.kafka.connect.sink.SinkTask}</code>.</p>
<h6 id="SourceTask"><a href="#SourceTask" class="headerlink" title="SourceTask"></a>SourceTask</h6><p>　SourceTask is a Task that pulls records from another system for storage in Kafka.</p>
<h6 id="SinkTask"><a href="#SinkTask" class="headerlink" title="SinkTask"></a>SinkTask</h6><p>　SinkTask is a Task that takes records loaded from Kafka and sends them to another system. Each task instance is assigned a set of partitions by the Connect framework and will handle all records received from those partitions. As records are fetched from Kafka, they will be passed to the sink task using the {@link #put(Collection)} API, which should either write them to the downstream system or batch them for later writing. Periodically, Connect will call {@link #flush(Map)} to ensure that batched records are actually pushed to the downstream system..</p>
<p>状态机</p>
<ul>
<li><p>Initialization</p>
<p>SinkTasks are first initialized using <code>{@link #initialize(SinkTaskContext)}</code> to prepare the task´s context and <code>{@link #start(Map)}</code> to accept configuration and start any services needed for processing.</p>
</li>
<li><p>Partition Assignment</p>
<p>After initialization, Connect will assign the task a set of partitions using <code>{@link #open(Collection)}</code>. These partitions are owned exclusively by this task until they have been closed with <code>{@link #close(Collection)}</code>.</p>
</li>
<li><p>Record Processing</p>
<p>Once partitions have been opened for writing, Connect will begin forwarding records from Kafka using the <code>{@link #put(Collection)}</code> API. Periodically, Connect will ask the task to flush records using <code>{@link #flush(Map)}</code> as described above.</p>
</li>
<li><p>Partition Rebalancing</p>
<p>Occasionally, Connect will need to change the assignment of this task. When this happens, the currently assigned partitions will be closed with <code>{@link #close(Collection)}</code> and the new assignment will be opened using <code>{@link #open(Collection)}</code>.</p>
</li>
<li><p>Shutdown</p>
<p>When the task needs to be shutdown, Connect will close active partitions (if there are any) and stop the task using <code>{@link #stop()}</code></p>
</li>
</ul>
<h5 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h5><h6 id="OffsetBackingStore"><a href="#OffsetBackingStore" class="headerlink" title="OffsetBackingStore"></a>OffsetBackingStore</h6><p>　OffsetBackingStore is an interface for storage backends that store key-value data. The backing store doesn´t need to handle serialization or deserialization. It only needs to support reading/writing bytes. Since it is expected these operations will require network operations, only bulk operations are supported.</p>
<p>　Since OffsetBackingStore is a shared resource that may be used by many OffsetStorage instances that are associated with individual tasks, the caller must be sure keys include information about the connector so that the shared namespace does not result in conflicting keys.</p>
<ul>
<li>KafkaOffsetBackingStore</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Implementation of OffsetBackingStore that uses a Kafka topic to store offset data.</span><br><span class="line"></span><br><span class="line">Internally, this implementation both produces to and consumes from a Kafka topic <span class="built_in">which</span> stores the offsets. It accepts producer and consumer overrides via its configuration but forces some settings to specific values to ensure correct behavior (e.g. acks, auto.offset.reset).</span><br></pre></td></tr></tbody></table></figure>
<h6 id="StatusBackingStore"><a href="#StatusBackingStore" class="headerlink" title="StatusBackingStore"></a>StatusBackingStore</h6><ul>
<li>KafkaStatusBackingStore</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">StatusBackingStore implementation <span class="built_in">which</span> uses a compacted topic <span class="keyword">for</span> storage of connector and task status information. When a state change is observed, the new state is written to the compacted topic. The new state will not be visible until it has been <span class="built_in">read</span> back from the topic.</span><br><span class="line"></span><br><span class="line">In spite of their names, the putSafe() methods cannot guarantee the safety of the write (since Kafka itself cannot provide such guarantees currently), but it can avoid specific unsafe conditions. In particular, we putSafe() allows writes <span class="keyword">in</span> the following conditions:</span><br><span class="line"></span><br><span class="line">　1) It is (probably) safe to overwrite the state <span class="keyword">if</span> there is no previous value.</span><br><span class="line">　2) It is (probably) safe to overwrite the state <span class="keyword">if</span> the previous value was <span class="built_in">set</span> by a worker with the same workerId.</span><br><span class="line">　3) It is (probably) safe to overwrite the previous state <span class="keyword">if</span> the current generation is higher than the previous .</span><br><span class="line"></span><br><span class="line">Basically all these conditions <span class="keyword">do</span> is reduce the window <span class="keyword">for</span> conflicts. They obviously cannot take into account <span class="keyword">in</span>-flight requests.</span><br></pre></td></tr></tbody></table></figure>
<h6 id="ConfigBackingStore"><a href="#ConfigBackingStore" class="headerlink" title="ConfigBackingStore"></a>ConfigBackingStore</h6><ul>
<li>KafkaConfigBackingStore</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Provides persistent storage of Kafka Connect connector configurations <span class="keyword">in</span> a Kafka topic.</span><br><span class="line"></span><br><span class="line">This class manages both connector and task configurations. It tracks three types of configuration entries:</span><br><span class="line"></span><br><span class="line">　1. Connector config: map of string -&gt; string configurations passed to the Connector class, with support <span class="keyword">for</span>　expanding this format <span class="keyword">if</span> necessary. (Kafka key: connector-[connector-id]).　These configs are *not* ephemeral. They represent the <span class="built_in">source</span> of truth. If the entire Connect　cluster goes down, this is all that is really needed to recover.</span><br><span class="line"></span><br><span class="line">　2. Task configs: map of string -&gt; string configurations passed to the Task class, with support <span class="keyword">for</span> expanding this format <span class="keyword">if</span> necessary. (Kafka key: task-[connector-id]-[task-id]). These configs are ephemeral; they are stored here to</span><br><span class="line">　　a) disseminate them to all workers <span class="keyword">while</span> ensuring agreement and</span><br><span class="line">　　b) to allow faster cluster/worker recovery since the common <span class="keyword">case</span> of recovery (restoring a connector) will simply result <span class="keyword">in</span> the same configuration as before the failure.</span><br><span class="line"></span><br><span class="line">　3. Task commit <span class="string">"configs"</span>: records indicating that previous task config entries should be committed and all task configs <span class="keyword">for</span> a connector can be applied. (Kafka key: commit-[connector-id]. This config has two effects. First, it records the number of tasks the connector is currently running (and can therefore increase/decrease parallelism). Second, because each task config is stored separately but they need to be applied together to ensure each partition is assigned to a single task, this record also indicates that task configs <span class="keyword">for</span> the specified connector can be <span class="string">"applied"</span> or <span class="string">"committed"</span>.</span><br><span class="line"></span><br><span class="line">This configuration is expected to be stored <span class="keyword">in</span> a *single partition* and *compacted* topic. Using a single partition ensures we can enforce ordering on messages, allowing Kafka to be used as a write ahead <span class="built_in">log</span>. Compaction allows us to clean up outdated configurations over time. However, this combination has some important implications <span class="keyword">for</span> the implementation of this class and the configuration state that it may expose.</span><br><span class="line"></span><br><span class="line">Connector configurations are independent of all other configs, so they are handled easily. Writing a single record is already atomic, so these can be applied as soon as they are <span class="built_in">read</span>. One connectors config does not affect any others, and they <span class="keyword">do</span> not need to coordinate with the connector<span class="string">'s task configuration at all.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">　The most obvious implication for task configs is the need for the commit messages. Because Kafka does not currently have multi-record transactions or support atomic batch record writes, task commit messages are required to ensure that readers do not end up using inconsistent configs. For example, consider if a connector wrote configs for its tasks, then was reconfigured and only managed to write updated configs for half its tasks. If task configs were applied immediately you could be using half the old configs and half the new configs. In that condition, some partitions may be double-assigned because the old config and new config may use completely different assignments. Therefore, when reading the log, we must buffer config updates for a connector'</span>s tasks and only apply atomically them once a commit message has been <span class="built_in">read</span>.</span><br><span class="line"></span><br><span class="line">However, there are also further challenges. This simple buffering approach would work fine as long as the entire <span class="built_in">log</span> was always available, but we would like to be able to <span class="built_in">enable</span> compaction so our configuration topic does not grow indefinitely. Compaction may <span class="built_in">break</span> a normal <span class="built_in">log</span> because old entries will suddenly go missing. A new worker reading from the beginning of the <span class="built_in">log</span> <span class="keyword">in</span> order to build up the full current configuration will see task commits, but some records required <span class="keyword">for</span> those commits will have been removed because the same keys have subsequently been rewritten. For example, <span class="keyword">if</span> you have a sequence of record keys <span class="string">"[connector-foo-config, task-foo-1-config, task-foo-2-config, commit-foo (2 tasks), task-foo-1-config, commit-foo (1 task)]"</span>, we can end up with a compacted <span class="built_in">log</span> containing <span class="string">"[connector-foo-config, task-foo-2-config, commit-foo (2 tasks), task-foo-1-config, commit-foo (1 task)]"</span>. When <span class="built_in">read</span> back, the first commit will see an invalid state because the first task-foo-1-config has been cleaned up.</span><br><span class="line"></span><br><span class="line">Compaction can further complicate things <span class="keyword">if</span> writing new task configs fails mid-write. Consider a similar scenario as the previous one, but <span class="keyword">in</span> this <span class="keyword">case</span> both the first and second update will write 2 task configs. However, the second write fails half of the way through: <span class="string">"[connector-foo-config, task-foo-1-config, task-foo-2-config, commit-foo (2 tasks), task-foo-1-config]"</span>. Now compaction occurs and we are left with <span class="string">"[connector-foo-config, task-foo-2-config, commit-foo (2 tasks), task-foo-1-config]"</span>. At the first commit, we donot have a complete <span class="built_in">set</span> of configs. And because of the failure, there is no second commit. We are left <span class="keyword">in</span> an inconsistent state with no obvious way to resolve the issue -- we can try to keep on reading, but the failed node may never recover and write the updated config. Meanwhile, other workers may have seen the entire <span class="built_in">log</span>; they will see the second task-foo-1-config waiting to be applied, but will otherwise think everything is ok -- they have a valid <span class="built_in">set</span> of task configs <span class="keyword">for</span> connector <span class="string">"foo"</span>.</span><br><span class="line"></span><br><span class="line">Because we can encounter these inconsistencies and addressing them requires support from the rest of the system (resolving the task configuration inconsistencies requires support from the connector instance to regenerate updated configs), this class exposes not only the current <span class="built_in">set</span> of configs, but also <span class="built_in">which</span> connectors have inconsistent data. This allows users of this class (i.e., Herder implementations) to take action to resolve any inconsistencies. These inconsistencies should be rare (as described above, due to compaction combined with leader failures <span class="keyword">in</span> the middle of updating task configurations).</span><br><span class="line"></span><br><span class="line">Note that the expectation is that this config storage system has only a single writer at a time. The <span class="built_in">caller</span> (Herder) must ensure this is the <span class="keyword">case</span>. In distributed mode this will require forwarding config change requests to the leader <span class="keyword">in</span> the cluster (i.e. the worker group coordinated by the Kafka broker).</span><br><span class="line"></span><br><span class="line">Since processing of the config <span class="built_in">log</span> occurs <span class="keyword">in</span> a background thread, callers must take care when using accessors. To simplify handling this correctly, this class only exposes a mechanism to snapshot the current state of the cluster. Updates may <span class="built_in">continue</span> to be applied (and callbacks invoked) <span class="keyword">in</span> the background. Callers must take care that they are using a consistent snapshot and only update when it is safe. In particular, <span class="keyword">if</span> task configs are updated <span class="built_in">which</span> require synchronization across workers to commit offsets and update the configuration, callbacks and updates during the rebalance must be deferred.</span><br></pre></td></tr></tbody></table></figure>
<h5 id="RestServer"><a href="#RestServer" class="headerlink" title="RestServer"></a>RestServer</h5><p>　Embedded server for the REST API that provides the control plane for Kafka Connect workers.</p>
<h5 id="Plugin"><a href="#Plugin" class="headerlink" title="Plugin"></a>Plugin</h5><h6 id="PluginDesc"><a href="#PluginDesc" class="headerlink" title="PluginDesc"></a>PluginDesc</h6><h6 id="DelegatingClassLoader"><a href="#DelegatingClassLoader" class="headerlink" title="DelegatingClassLoader"></a>DelegatingClassLoader</h6><h4 id="Confluent-开源版本"><a href="#Confluent-开源版本" class="headerlink" title="Confluent 开源版本"></a>Confluent 开源版本</h4><h5 id="模块架构图"><a href="#模块架构图" class="headerlink" title="模块架构图"></a>模块架构图</h5><p><img data-src="/picture/kafka/kafka_connect_architecture.png" alt="Kafka Connector Architecture"></p>
<center>（利用 <a href="https://www.axure.com.cn/" target="_blank" rel="external nofollow noopener noreferrer">Axure</a>™ 绘制而成）</center>


<h5 id="kafka-connect-common"><a href="#kafka-connect-common" class="headerlink" title="kafka-connect-common"></a>kafka-connect-common</h5><p>　该项目是整个 Kafka Connect 工程的<strong>根目录级</strong>父工程，其中主要包含了，common 接口定义、配置读取、Metrics /  JMX 指标设计、公共工具类，以及如何 build 编译、package 打包哪些文件、license 著作权标识 等工程类的部分</p>
<h5 id="kafka-connect-storage-common"><a href="#kafka-connect-storage-common" class="headerlink" title="kafka-connect-storage-common"></a>kafka-connect-storage-common</h5><p>　官方对该项目定位描述为简单的一句话：”Kafka Connect common packages for connectors transferring data between Kafka and distributed filesystems or cloud storage”。实际包含了很多实质性内容，Common 模块、Core 模块、Format 模块、Partitioner 模块、WAL 模块、Hive 模块 等</p>
<h6 id="Common-模块"><a href="#Common-模块" class="headerlink" title="Common 模块"></a>Common 模块</h6><p>　主要定义了 ComposableConfig / SchemaGenerator 接口，分别用于 配置组合 和 Schema 生成</p>
<h6 id="Core-模块"><a href="#Core-模块" class="headerlink" title="Core 模块"></a>Core 模块</h6><p>　主要定义了 Storage 接口，用于分布式存储。基于 Storage 接口的不同实现，一个 Storage 对象可以是分布式文件系统里面的一个文件或者一个文件夹，也可以是对象存储系统中的一个存储对象。类似地，路径就对应于分布式文件系统中的实际路径和对象存储库中的查找键</p>
<h6 id="Format-模块"><a href="#Format-模块" class="headerlink" title="Format 模块"></a>Format 模块</h6><p>　主要定义了 Format / SchemaFileReader / RecordWriter 三个接口，分别用于 存储类型格式化（e.g. Path in HDFS, String in S3）、从 Storage 中读取 Schema 和 如何将记录写入 Storage</p>
<h6 id="Partitioner-模块"><a href="#Partitioner-模块" class="headerlink" title="Partitioner 模块"></a>Partitioner 模块</h6><p>　该模块主要定义了 Partitioner 接口，用来对接收到的消息进行分区，生成对应的目录和文件名。支持 Default / Hourly / Daily / TimeBase / Field 和自定义的 Partitioner，另外 TimeBase 的 Partitioner 还同时支持 ingestion time / event time 两种时间类型</p>
<h6 id="WAL-模块"><a href="#WAL-模块" class="headerlink" title="WAL 模块"></a>WAL 模块</h6><p>　定义了 WAL 接口，用于规范 Kafka offset 信息如何编码到 WAL</p>
<h6 id="Hive-模块"><a href="#Hive-模块" class="headerlink" title="Hive 模块"></a>Hive 模块</h6><p>　主要定义了 HiveUtil 抽象类、HiveFactory 接口（HiveUtil 的工厂类） 和 Schema / Config / Exception / MetaStore 等相关实现类</p>
<h5 id="kafka-connect-hdfs"><a href="#kafka-connect-hdfs" class="headerlink" title="kafka-connect-hdfs"></a>kafka-connect-hdfs</h5><p>　首先可以看出 State 枚举类中给出了 <code>RECOVERY_STARTED</code>,  <code>RECOVERY_PARTITION_PAUSED</code>,  <code>WAL_APPLIED</code>,  <code>WAL_TRUNCATED</code>,  <code>OFFSET_RESET</code>,  <code>WRITE_STARTED</code>,  <code>WRITE_PARTITION_PAUSED</code>,  <code>SHOULD_ROTATE</code>,  <code>TEMP_FILE_CLOSED,  WAL_APPENDED</code>,  <code>FILE_COMMITTED</code> 一共 11 种任务运行状态，写入的逻辑全都在 <code>TopicPartitionWriter</code> 的 <code>write</code> 方法里面了，而异常恢复的逻辑，则在 <code>recover</code> 方法中实现</p>
<p><img data-src="/picture/kafka/kafka_connect_process_flow_kafka_2_hdfs.png" alt="Kafka Connector Process Flow about Kafka to HDFS"></p>
<center>（利用 <a href="https://www.axure.com.cn/" target="_blank" rel="external nofollow noopener noreferrer">Axure</a>™ 绘制而成）</center>





<h4 id="落脚点"><a href="#落脚点" class="headerlink" title="落脚点"></a>落脚点</h4><h5 id="Kafka-Connect-组件的-ClassLoader-隔离"><a href="#Kafka-Connect-组件的-ClassLoader-隔离" class="headerlink" title="Kafka Connect 组件的 ClassLoader 隔离"></a>Kafka Connect 组件的 ClassLoader 隔离</h5><p>　Worker <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L181">启动</a>一个 Task</p>
<h5 id="分布式情况下，如何做到在一个节点上，更新了-ClassLoader，广播到整个集群"><a href="#分布式情况下，如何做到在一个节点上，更新了-ClassLoader，广播到整个集群" class="headerlink" title="分布式情况下，如何做到在一个节点上，更新了 ClassLoader，广播到整个集群"></a>分布式情况下，如何做到在一个节点上，更新了 ClassLoader，广播到整个集群</h5><p>　不需要广播，滚动升级，灰度发布，即可</p>
<h2 id="社区跟进"><a href="#社区跟进" class="headerlink" title="社区跟进"></a>社区跟进</h2><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/apache/kafka/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+author%3Aasdf2014">Kafka Pull Request</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/Landoop/kafka-connect-ui/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+author%3Aasdf2014">Kafka-Connect-UI Pull Request</a></li>
</ul>
<p>　详见：《<a href="https://yuzhouwan.com/posts/19631/">如何成为 Apache 的 PMC</a>》</p>
<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://hub.docker.com/r/confluentinc/cp-kafka-connect/">Kafka Connect 镜像</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://yeasy.gitbooks.io/docker_practice/content/install/mirror.html">国内镜像源配置</a></li>
</ul>
<h3 id="Book"><a href="#Book" class="headerlink" title="Book"></a>Book</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/Apache-Kafka-Garg-Nishant/dp/B00FYS7OUW">Apache Kafka</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/Apache-Kafka-Cookbook-Minni-Saurabh/dp/B015EHCTES">Apache Kafka Cookbook</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/Kafka-The-Definitive-Guide-Narkhede-Neha/dp/1491936169">Kafka: The Definitive Guide</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/dp/B077698DBP">Kafka 技术内幕：图文详解 Kafka 源码设计与实现</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/dp/B072HQC5KB">Apache Kafka 源码剖析</a></li>
</ul>
<h2 id="欢迎加入我们的技术群，一起交流学习"><a href="#欢迎加入我们的技术群，一起交流学习" class="headerlink" title="欢迎加入我们的技术群，一起交流学习"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/asdf2014/yuzhouwan#technical-discussion-group">欢迎加入我们的技术群，一起交流学习</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">群名称</th>
<th style="text-align:center">群号</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">人工智能（高级）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=71c6bd3fb0ff01d93abca654140387d99d3be752f92a53c1fbfd27f2dd4b4247"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">人工智能（进阶）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=deb268f65589a1a0a1dbaf7b72c849ed45298697805bef81e0c613dea40cd05e"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">BigData</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=f86b3c8de20da1658a3bb42df17a2fc4eee0d75c4a130a63585fdd257e3565ed"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">算法</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=bfbcf1453371a0810fd6be235ace47147f6fb9d262fb768b497c861f50af0af4"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg" alt></a></td>
</tr>
</tbody>
</table>
</div>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/31915/" rel="bookmark">ZooKeeper 原理与优化</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/39683/" rel="bookmark">Apache Eagle 深度调研</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/200926/" rel="bookmark">Helm 实战</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/200906/" rel="bookmark">Presto：分布式 SQL 查询引擎</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/5845/" rel="bookmark">Apache Druid：一款高效的 OLAP 引擎</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Benedict Jin
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuzhouwan.com/posts/26002/" title="Apache Kafka 分布式消息队列框架">https://yuzhouwan.com/posts/26002/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-ND</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Apache-Storm/" rel="tag"># Apache Storm</a>
              <a href="/tags/Apache-Kafka/" rel="tag"># Apache Kafka</a>
              <a href="/tags/Docker/" rel="tag"># Docker</a>
              <a href="/tags/Gradle/" rel="tag"># Gradle</a>
              <a href="/tags/HDFS/" rel="tag"># HDFS</a>
              <a href="/tags/Apache-Druid/" rel="tag"># Apache Druid</a>
              <a href="/tags/ElasticSearch/" rel="tag"># ElasticSearch</a>
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Message-Queue/" rel="tag"># Message Queue</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/17444/" rel="prev" title="Qcon 2015 见闻之一：猿题库">
      <i class="fa fa-chevron-left"></i> Qcon 2015 见闻之一：猿题库
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/25015/" rel="next" title="Apache Storm 与 Kafka 的整合应用">
      Apache Storm 与 Kafka 的整合应用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-gitalk">Gitalk：可用 Github 账号登录</a></li>
            <li class="tab"><a href="#comment-disqus">Disqus：海外</a></li>
            <li class="tab"><a href="#comment-valine">Valine：匿名</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments" id="gitalk-container"></div>
            </div>
            <div class="tab-pane disqus" id="comment-disqus">
              
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
            </div>
            <div class="tab-pane valine" id="comment-valine">
              <div class="comments" id="valine-comments"></div>
            </div>
        </div>
      </div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">1.</span> <span class="nav-text">Kafka 是什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89-Kafka"><span class="nav-number">2.</span> <span class="nav-text">为什么要有 Kafka?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F"><span class="nav-number">2.1.</span> <span class="nav-text">分布式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F"><span class="nav-number">2.2.</span> <span class="nav-text">高吞吐量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-number">2.3.</span> <span class="nav-text">高可靠性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A6%BB%E7%BA%BF-amp-%E5%AE%9E%E6%97%B6%E6%80%A7"><span class="nav-number">2.4.</span> <span class="nav-text">离线 &amp; 实时性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E8%80%A6"><span class="nav-number">2.5.</span> <span class="nav-text">解耦</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">3.</span> <span class="nav-text">Kafka 工作机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5"><span class="nav-number">3.1.</span> <span class="nav-text">一些主要概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E6%84%8F%E5%9B%BE"><span class="nav-number">3.2.</span> <span class="nav-text">示意图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-Connect"><span class="nav-number">4.</span> <span class="nav-text">Kafka Connect</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Connect-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">Kafka Connect 是什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%80%A7"><span class="nav-number">4.2.</span> <span class="nav-text">特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="nav-number">4.3.</span> <span class="nav-text">设计目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">4.4.</span> <span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Connector"><span class="nav-number">4.4.1.</span> <span class="nav-text">Connector</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Task"><span class="nav-number">4.4.2.</span> <span class="nav-text">Task</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker"><span class="nav-number">4.4.3.</span> <span class="nav-text">Worker</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Converter"><span class="nav-number">4.4.4.</span> <span class="nav-text">Converter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Transform"><span class="nav-number">4.4.5.</span> <span class="nav-text">Transform</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84"><span class="nav-number">4.5.</span> <span class="nav-text">架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Connector-model"><span class="nav-number">4.5.1.</span> <span class="nav-text">Connector model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker-model"><span class="nav-number">4.5.2.</span> <span class="nav-text">Worker model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-model"><span class="nav-number">4.5.3.</span> <span class="nav-text">Data model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quick-Start"><span class="nav-number">4.6.</span> <span class="nav-text">Quick Start</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83"><span class="nav-number">4.6.1.</span> <span class="nav-text">基础环境</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Confluent"><span class="nav-number">4.6.2.</span> <span class="nav-text">Confluent</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD"><span class="nav-number">4.6.2.1.</span> <span class="nav-text">下载</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">4.6.2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8"><span class="nav-number">4.6.2.3.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%91%E9%80%81-avro-%E6%95%B0%E6%8D%AE"><span class="nav-number">4.6.2.4.</span> <span class="nav-text">发送 avro 数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A5%E5%8F%97-avro-%E6%95%B0%E6%8D%AE"><span class="nav-number">4.6.2.5.</span> <span class="nav-text">接受 avro 数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E4%B8%8D%E5%AF%B9%E7%9A%84-avro-%E6%95%B0%E6%8D%AE"><span class="nav-number">4.6.2.6.</span> <span class="nav-text">发送数据格式不对的 avro 数据</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2"><span class="nav-number">4.6.2.7.</span> <span class="nav-text">停止</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-Connect-1"><span class="nav-number">4.6.3.</span> <span class="nav-text">Kafka Connect</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-confluent"><span class="nav-number">4.6.3.1.</span> <span class="nav-text">启动 confluent</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B-connect-%E6%97%A5%E5%BF%97"><span class="nav-number">4.6.3.2.</span> <span class="nav-text">查看 connect 日志</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E6%94%AF%E6%8C%81%E7%9A%84-connect-%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.6.3.3.</span> <span class="nav-text">查看支持的 connect 类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-file-source"><span class="nav-number">4.6.3.4.</span> <span class="nav-text">使用 file-source</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-file-sink"><span class="nav-number">4.6.3.5.</span> <span class="nav-text">使用 file-sink</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B8%85%E7%90%86%E5%B7%A5%E4%BD%9C"><span class="nav-number">4.6.3.6.</span> <span class="nav-text">清理工作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="nav-number">4.7.</span> <span class="nav-text">支持的组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-2-HDFS"><span class="nav-number">4.7.1.</span> <span class="nav-text">Kafka 2 HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-1"><span class="nav-number">4.7.1.1.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Exactly-Once-Delivery"><span class="nav-number">4.7.1.1.1.</span> <span class="nav-text">Exactly Once Delivery</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Extensible-Data-Format"><span class="nav-number">4.7.1.1.2.</span> <span class="nav-text">Extensible Data Format</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Hive-Integration"><span class="nav-number">4.7.1.1.3.</span> <span class="nav-text">Hive Integration</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Schema-Evolution"><span class="nav-number">4.7.1.1.4.</span> <span class="nav-text">Schema Evolution</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Secure-HDFS-and-Hive-Metastore-Support"><span class="nav-number">4.7.1.1.5.</span> <span class="nav-text">Secure HDFS and Hive Metastore Support</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Pluggable-Partitioner"><span class="nav-number">4.7.1.1.6.</span> <span class="nav-text">Pluggable Partitioner</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-HDFS"><span class="nav-number">4.7.1.2.</span> <span class="nav-text">实战 HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-HDFS"><span class="nav-number">4.7.1.2.1.</span> <span class="nav-text">安装 HDFS</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Confluent"><span class="nav-number">4.7.1.2.2.</span> <span class="nav-text">配置 Confluent</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E7%94%A8"><span class="nav-number">4.7.1.2.3.</span> <span class="nav-text">启用</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81"><span class="nav-number">4.7.1.2.4.</span> <span class="nav-text">验证</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E6%88%98-Hive"><span class="nav-number">4.7.1.3.</span> <span class="nav-text">实战 Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Hive"><span class="nav-number">4.7.1.3.1.</span> <span class="nav-text">安装 Hive</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Confluent-1"><span class="nav-number">4.7.1.3.2.</span> <span class="nav-text">配置 Confluent</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-1"><span class="nav-number">4.7.1.3.3.</span> <span class="nav-text">启用</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81-1"><span class="nav-number">4.7.1.3.4.</span> <span class="nav-text">验证</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-2-ElasticSearch"><span class="nav-number">4.7.2.</span> <span class="nav-text">Kafka 2 ElasticSearch</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-2"><span class="nav-number">4.7.2.1.</span> <span class="nav-text">特性</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Exactly-Once-Delivery-1"><span class="nav-number">4.7.2.1.1.</span> <span class="nav-text">Exactly Once Delivery</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Mapping-Inference"><span class="nav-number">4.7.2.1.2.</span> <span class="nav-text">Mapping Inference</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Schema-Evolution-1"><span class="nav-number">4.7.2.1.3.</span> <span class="nav-text">Schema Evolution</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Delivery-Semantics"><span class="nav-number">4.7.2.1.4.</span> <span class="nav-text">Delivery Semantics</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Reindexing-with-Zero-Downtime"><span class="nav-number">4.7.2.1.5.</span> <span class="nav-text">Reindexing with Zero Downtime</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E6%88%98"><span class="nav-number">4.7.2.2.</span> <span class="nav-text">实战</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-ElasticSearch"><span class="nav-number">4.7.2.2.1.</span> <span class="nav-text">安装 ElasticSearch</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-Confluent-2"><span class="nav-number">4.7.2.2.2.</span> <span class="nav-text">配置 Confluent</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E7%94%A8-2"><span class="nav-number">4.7.2.2.3.</span> <span class="nav-text">启用</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81-2"><span class="nav-number">4.7.2.2.4.</span> <span class="nav-text">验证</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91"><span class="nav-number">4.7.2.3.</span> <span class="nav-text">踩过的坑</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#max-file-descriptors-32768-for-elasticsearch-process-is-too-low-increase-to-at-least-65536"><span class="nav-number">4.7.2.3.1.</span> <span class="nav-text">max file descriptors [32768] for elasticsearch process is too low, increase to at least [65536]</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-2-Druid"><span class="nav-number">4.7.3.</span> <span class="nav-text">Kafka 2 Druid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kafka-2-Kafka"><span class="nav-number">4.7.4.</span> <span class="nav-text">Kafka 2 Kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-3"><span class="nav-number">4.7.4.1.</span> <span class="nav-text">特性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E7%89%88-Worker"><span class="nav-number">4.8.</span> <span class="nav-text">单机版 Worker</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-1"><span class="nav-number">4.8.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE"><span class="nav-number">4.8.2.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-Worker"><span class="nav-number">4.9.</span> <span class="nav-text">分布式 Worker</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-Confluent"><span class="nav-number">4.9.1.</span> <span class="nav-text">启动 Confluent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Topic"><span class="nav-number">4.9.2.</span> <span class="nav-text">创建 Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A0%E9%99%A4-Topic"><span class="nav-number">4.9.3.</span> <span class="nav-text">删除 Topic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Connector-%E5%92%8C-Task-%E7%8A%B6%E6%80%81"><span class="nav-number">4.9.4.</span> <span class="nav-text">Connector 和 Task 状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker-%E4%B9%8B%E9%97%B4%E5%A6%82%E4%BD%95%E5%BB%BA%E7%AB%8B%E9%80%9A%E8%AE%AF"><span class="nav-number">4.9.5.</span> <span class="nav-text">Worker 之间如何建立通讯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Relance-%E6%9C%BA%E5%88%B6"><span class="nav-number">4.9.6.</span> <span class="nav-text">Relance 机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F"><span class="nav-number">4.10.</span> <span class="nav-text">完全分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%BF%E6%8D%A2%E5%8E%9F%E7%94%9F%E7%9A%84-ZooKeeper"><span class="nav-number">4.10.1.</span> <span class="nav-text">替换原生的 ZooKeeper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%BF%E6%8D%A2%E5%8E%9F%E7%94%9F%E7%9A%84-Kafka"><span class="nav-number">4.10.2.</span> <span class="nav-text">替换原生的 Kafka</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0-Kafka-Rest-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.10.3.</span> <span class="nav-text">更新 Kafka Rest 配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0-Schema-Register-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.10.4.</span> <span class="nav-text">更新 Schema Register 配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0-Worker-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.10.5.</span> <span class="nav-text">更新 Worker 配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91-1"><span class="nav-number">4.10.6.</span> <span class="nav-text">踩过的坑</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#WARNING-REMOTE-HOST-IDENTIFICATION-HAS-CHANGED"><span class="nav-number">4.10.6.1.</span> <span class="nav-text">WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3"><span class="nav-number">4.10.6.1.1.</span> <span class="nav-text">解决</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Timed-out-while-checking-for-or-creating-topic-s-%E2%80%98connect-offsets%E2%80%99"><span class="nav-number">4.10.6.2.</span> <span class="nav-text">Timed out while checking for or creating topic(s) ‘connect-offsets’</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0"><span class="nav-number">4.10.6.2.1.</span> <span class="nav-text">描述</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Request-to-leader-to-reconfigure-connector-tasks-failed"><span class="nav-number">4.10.6.3.</span> <span class="nav-text">Request to leader to reconfigure connector tasks failed</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-1"><span class="nav-number">4.10.6.3.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-1"><span class="nav-number">4.10.6.3.2.</span> <span class="nav-text">解决</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#io-confluent-kafka-schemaregistry-client-rest-RestService-Connection-refused"><span class="nav-number">4.10.6.4.</span> <span class="nav-text">io.confluent.kafka.schemaregistry.client.rest.RestService Connection refused</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-2"><span class="nav-number">4.10.6.4.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-2"><span class="nav-number">4.10.6.4.2.</span> <span class="nav-text">解决</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Docker-%E9%95%9C%E5%83%8F"><span class="nav-number">4.11.</span> <span class="nav-text">Docker 镜像</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Docker-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="nav-number">4.11.1.</span> <span class="nav-text">Docker 环境安装</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0%E9%95%9C%E5%83%8F%E6%BA%90"><span class="nav-number">4.11.1.1.</span> <span class="nav-text">更新镜像源</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-Docker-CE"><span class="nav-number">4.11.1.2.</span> <span class="nav-text">安装 Docker-CE</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-2"><span class="nav-number">4.11.1.3.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BB%BA%E7%AB%8B-Docker-%E7%94%A8%E6%88%B7%E7%BB%84"><span class="nav-number">4.11.1.4.</span> <span class="nav-text">建立 Docker 用户组</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MySQL-Kafka-Connect-HDFS-%E5%AE%9E%E6%88%98"><span class="nav-number">4.11.2.</span> <span class="nav-text">MySQL - Kafka Connect - HDFS 实战</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD-1"><span class="nav-number">4.11.2.1.</span> <span class="nav-text">下载</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-1"><span class="nav-number">4.11.2.2.</span> <span class="nav-text">安装</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#VirtualBox"><span class="nav-number">4.11.2.2.1.</span> <span class="nav-text">VirtualBox</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F"><span class="nav-number">4.11.2.3.</span> <span class="nav-text">导入虚拟机镜像</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%95%9C%E5%83%8F"><span class="nav-number">4.11.2.4.</span> <span class="nav-text">操作虚拟机镜像</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91-2"><span class="nav-number">4.11.3.</span> <span class="nav-text">踩过的坑</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#bridge-nf-call-iptables-is-disabled"><span class="nav-number">4.11.3.1.</span> <span class="nav-text">bridge-nf-call-iptables is disabled</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-3"><span class="nav-number">4.11.3.1.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-3"><span class="nav-number">4.11.3.1.2.</span> <span class="nav-text">解决</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.</span> <span class="nav-text">常用配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Worker-%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.1.</span> <span class="nav-text">Worker 通用配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#bootstrap-servers"><span class="nav-number">4.12.1.1.</span> <span class="nav-text">bootstrap.servers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#key-value-converter"><span class="nav-number">4.12.1.2.</span> <span class="nav-text">[key | value].converter</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#internal-key-value-converter"><span class="nav-number">4.12.1.3.</span> <span class="nav-text">internal.[key | value].converter</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rest-host-name-amp-rest-port"><span class="nav-number">4.12.1.4.</span> <span class="nav-text">rest.host.name &amp; rest.port</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#plugin-path"><span class="nav-number">4.12.1.5.</span> <span class="nav-text">plugin.path</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-Worker-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.2.</span> <span class="nav-text">分布式 Worker 配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#group-id"><span class="nav-number">4.12.2.1.</span> <span class="nav-text">group.id</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#config-offset-status-storage-topic"><span class="nav-number">4.12.2.2.</span> <span class="nav-text">[config | offset | status].storage.topic</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#config-offset-status-storage-replication-factor"><span class="nav-number">4.12.2.3.</span> <span class="nav-text">[config | offset | status].storage.replication.factor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#offset-status-storage-partitions"><span class="nav-number">4.12.2.4.</span> <span class="nav-text">[offset | status].storage.partitions</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Connector-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.3.</span> <span class="nav-text">Connector 配置</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%B4%E6%98%8E"><span class="nav-number">4.12.3.1.</span> <span class="nav-text">说明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">4.12.3.2.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8D%95%E6%9C%BA%E7%89%88-Connector-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.3.2.1.</span> <span class="nav-text">单机版 Connector 配置</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F-Connector-%E9%85%8D%E7%BD%AE"><span class="nav-number">4.12.3.2.2.</span> <span class="nav-text">分布式 Connector 配置</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RESTful-%E6%8E%A5%E5%8F%A3"><span class="nav-number">4.13.</span> <span class="nav-text">RESTful 接口</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%B4%E6%98%8E-1"><span class="nav-number">4.13.1.</span> <span class="nav-text">说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B-1"><span class="nav-number">4.13.2.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Worker-%E7%89%88%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="nav-number">4.13.2.1.</span> <span class="nav-text">Worker 版本信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Worker-%E6%94%AF%E6%8C%81%E7%9A%84-Connector-%E6%8F%92%E4%BB%B6"><span class="nav-number">4.13.2.2.</span> <span class="nav-text">Worker 支持的 Connector 插件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Worker-%E4%B8%8A%E6%BF%80%E6%B4%BB%E7%9A%84-Connector-%E6%8F%92%E4%BB%B6"><span class="nav-number">4.13.2.3.</span> <span class="nav-text">Worker 上激活的 Connector 插件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%90%AF-Connector"><span class="nav-number">4.13.2.4.</span> <span class="nav-text">重启 Connector</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%8E%B7%E5%BE%97%E6%9F%90%E4%B8%AA-Connector%E4%B8%8A%E7%9A%84%E6%89%80%E6%9C%89-Tasks"><span class="nav-number">4.13.2.5.</span> <span class="nav-text">获得某个 Connector上的所有 Tasks</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%90%AF-Task"><span class="nav-number">4.13.2.6.</span> <span class="nav-text">重启 Task</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9A%82%E5%81%9C-Connector"><span class="nav-number">4.13.2.7.</span> <span class="nav-text">暂停 Connector</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%81%A2%E5%A4%8D-Connector"><span class="nav-number">4.13.2.8.</span> <span class="nav-text">恢复 Connector</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9B%B4%E6%96%B0-Connector-%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="nav-number">4.13.2.9.</span> <span class="nav-text">更新 Connector 配置信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96-Connector-%E7%8A%B6%E6%80%81"><span class="nav-number">4.13.2.10.</span> <span class="nav-text">获取 Connector 状态</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%8E%B7%E5%8F%96-Connector-%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="nav-number">4.13.2.11.</span> <span class="nav-text">获取 Connector 配置信息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%A0%E9%99%A4-Connector"><span class="nav-number">4.13.2.12.</span> <span class="nav-text">删除 Connector</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Schema-Registry"><span class="nav-number">4.14.</span> <span class="nav-text">Schema Registry</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">4.14.1.</span> <span class="nav-text">常用命令</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">4.15.</span> <span class="nav-text">可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Confluent-UI"><span class="nav-number">4.15.1.</span> <span class="nav-text">Confluent UI</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Landoop-UI"><span class="nav-number">4.15.2.</span> <span class="nav-text">Landoop UI</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka-connect-ui"><span class="nav-number">4.15.2.1.</span> <span class="nav-text">kafka-connect-ui</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-3"><span class="nav-number">4.15.2.1.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%BC%80%E5%90%AF-Confluent-Rest"><span class="nav-number">4.15.2.1.2.</span> <span class="nav-text">开启 Confluent Rest</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-File-Sink"><span class="nav-number">4.15.2.1.3.</span> <span class="nav-text">创建 File Sink</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE"><span class="nav-number">4.15.2.1.4.</span> <span class="nav-text">发送数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%8E%A5%E6%94%B6%E6%88%90%E5%8A%9F"><span class="nav-number">4.15.2.1.5.</span> <span class="nav-text">接收成功</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-File-Source"><span class="nav-number">4.15.2.1.6.</span> <span class="nav-text">创建 File Source</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0%E6%96%87%E4%BB%B6"><span class="nav-number">4.15.2.1.7.</span> <span class="nav-text">写入数据到文件</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91-3"><span class="nav-number">4.15.2.1.8.</span> <span class="nav-text">踩过的坑</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka-topic-ui"><span class="nav-number">4.15.2.2.</span> <span class="nav-text">kafka-topic-ui</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-4"><span class="nav-number">4.15.2.2.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-Kafka-Rest"><span class="nav-number">4.15.2.2.2.</span> <span class="nav-text">启动 Kafka Rest</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%95%88%E6%9E%9C%E5%9B%BE"><span class="nav-number">4.15.2.2.3.</span> <span class="nav-text">效果图</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91-4"><span class="nav-number">4.15.2.2.4.</span> <span class="nav-text">踩过的坑</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#schema-registry-ui"><span class="nav-number">4.15.2.3.</span> <span class="nav-text">schema-registry-ui</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-5"><span class="nav-number">4.15.2.3.1.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-Schema-Registry"><span class="nav-number">4.15.2.3.2.</span> <span class="nav-text">启动 Schema Registry</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%95%88%E6%9E%9C%E5%9B%BE-1"><span class="nav-number">4.15.2.3.3.</span> <span class="nav-text">效果图</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E5%BC%80%E5%8F%91"><span class="nav-number">4.15.2.3.4.</span> <span class="nav-text">二次开发</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BE%E5%8C%BA-Connector"><span class="nav-number">4.16.</span> <span class="nav-text">使用社区 Connector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%B7%B1%E7%9A%84-Connector"><span class="nav-number">4.17.</span> <span class="nav-text">实现自己的 Connector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%AE%E5%A4%8D%E5%B7%B2%E7%9F%A5-Bug"><span class="nav-number">4.18.</span> <span class="nav-text">修复已知 Bug</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="nav-number">5.</span> <span class="nav-text">实用技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#zkCli"><span class="nav-number">5.1.</span> <span class="nav-text">zkCli</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-run-class"><span class="nav-number">5.2.</span> <span class="nav-text">kafka-run-class</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%A0%E9%99%A4-Topic-1"><span class="nav-number">5.2.1.</span> <span class="nav-text">删除 Topic</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Consumer-%E6%8C%87%E5%AE%9A-offset-%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%B4%B9%EF%BC%8C%E4%BB%8E%E8%80%8C%E8%BE%BE%E5%88%B0%E8%A1%A5%E6%95%B0%E7%9A%84%E6%95%88%E6%9E%9C"><span class="nav-number">5.3.</span> <span class="nav-text">Consumer 指定 offset 进行消费，从而达到补数的效果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Java-Client"><span class="nav-number">5.3.1.</span> <span class="nav-text">Java Client</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shell-Console"><span class="nav-number">5.3.2.</span> <span class="nav-text">Shell Console</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%93%AA%E4%BA%9B%E9%85%8D%E7%BD%AE%E5%8F%AF%E4%BB%A5%E5%9C%A8-Kafka-Connect-%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%8C%87%E5%AE%9A"><span class="nav-number">5.4.</span> <span class="nav-text">哪些配置可以在 Kafka Connect 中进行指定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E9%9B%85%E5%9C%B0%E5%81%9C%E6%AD%A2-Kafka-%E6%9C%8D%E5%8A%A1"><span class="nav-number">5.5.</span> <span class="nav-text">优雅地停止 Kafka 服务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%BF%87%E6%9C%9F%E7%AD%96%E7%95%A5"><span class="nav-number">5.6.</span> <span class="nav-text">数据过期策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E5%B1%82%E9%9D%A2"><span class="nav-number">6.1.</span> <span class="nav-text">架构层面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RingBuffer-Lock-Free"><span class="nav-number">6.1.1.</span> <span class="nav-text">RingBuffer + Lock Free</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Avro-%E5%8E%8B%E7%BC%A9"><span class="nav-number">6.1.2.</span> <span class="nav-text">Avro 压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A-Partitioner-%E8%BF%9E%E6%8E%A5%E6%B1%A0"><span class="nav-number">6.1.3.</span> <span class="nav-text">多 Partitioner + 连接池</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%B1%82%E9%9D%A2"><span class="nav-number">6.2.</span> <span class="nav-text">参数层面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Producer"><span class="nav-number">6.2.1.</span> <span class="nav-text">Producer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B8%A9%E5%88%B0%E7%9A%84%E5%9D%91"><span class="nav-number">6.3.</span> <span class="nav-text">踩到的坑</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Avro-%E4%B8%AD-Decimal-%E5%AD%97%E6%AE%B5%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%85%A8%E9%83%A8%E5%8F%98%E6%88%90%E4%BA%86-pos-0-lim-0-cap-0"><span class="nav-number">6.3.1.</span> <span class="nav-text">Avro 中 Decimal 字段反序列化之后，全部变成了 [pos&#x3D;0 lim&#x3D;0 cap&#x3D;0]</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-4"><span class="nav-number">6.3.1.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-4"><span class="nav-number">6.3.1.2.</span> <span class="nav-text">解决</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A1%A5%E5%85%85"><span class="nav-number">6.3.1.3.</span> <span class="nav-text">补充</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB"><span class="nav-number">7.</span> <span class="nav-text">源码阅读</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-number">7.1.</span> <span class="nav-text">Kafka 阅读环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-gradle"><span class="nav-number">7.1.1.</span> <span class="nav-text">安装 gradle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#gradle-%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE"><span class="nav-number">7.1.2.</span> <span class="nav-text">gradle 代理设置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81"><span class="nav-number">7.1.3.</span> <span class="nav-text">编译源码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka-Connect-2"><span class="nav-number">7.2.</span> <span class="nav-text">Kafka Connect</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-number">7.2.1.</span> <span class="nav-text">阅读环境搭建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="nav-number">7.2.2.</span> <span class="nav-text">主体架构</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ConnectDistributed"><span class="nav-number">7.2.2.1.</span> <span class="nav-text">ConnectDistributed</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Connect"><span class="nav-number">7.2.2.2.</span> <span class="nav-text">Connect</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Connector-1"><span class="nav-number">7.2.2.3.</span> <span class="nav-text">Connector</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#SinkConnector"><span class="nav-number">7.2.2.3.1.</span> <span class="nav-text">SinkConnector</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#SourceConnector"><span class="nav-number">7.2.2.3.2.</span> <span class="nav-text">SourceConnector</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Herder"><span class="nav-number">7.2.2.4.</span> <span class="nav-text">Herder</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Worker-1"><span class="nav-number">7.2.2.5.</span> <span class="nav-text">Worker</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Task-1"><span class="nav-number">7.2.2.6.</span> <span class="nav-text">Task</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#SourceTask"><span class="nav-number">7.2.2.6.1.</span> <span class="nav-text">SourceTask</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#SinkTask"><span class="nav-number">7.2.2.6.2.</span> <span class="nav-text">SinkTask</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Storage"><span class="nav-number">7.2.2.7.</span> <span class="nav-text">Storage</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#OffsetBackingStore"><span class="nav-number">7.2.2.7.1.</span> <span class="nav-text">OffsetBackingStore</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#StatusBackingStore"><span class="nav-number">7.2.2.7.2.</span> <span class="nav-text">StatusBackingStore</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#ConfigBackingStore"><span class="nav-number">7.2.2.7.3.</span> <span class="nav-text">ConfigBackingStore</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RestServer"><span class="nav-number">7.2.2.8.</span> <span class="nav-text">RestServer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Plugin"><span class="nav-number">7.2.2.9.</span> <span class="nav-text">Plugin</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#PluginDesc"><span class="nav-number">7.2.2.9.1.</span> <span class="nav-text">PluginDesc</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#DelegatingClassLoader"><span class="nav-number">7.2.2.9.2.</span> <span class="nav-text">DelegatingClassLoader</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Confluent-%E5%BC%80%E6%BA%90%E7%89%88%E6%9C%AC"><span class="nav-number">7.2.3.</span> <span class="nav-text">Confluent 开源版本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A8%A1%E5%9D%97%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="nav-number">7.2.3.1.</span> <span class="nav-text">模块架构图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka-connect-common"><span class="nav-number">7.2.3.2.</span> <span class="nav-text">kafka-connect-common</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka-connect-storage-common"><span class="nav-number">7.2.3.3.</span> <span class="nav-text">kafka-connect-storage-common</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Common-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.1.</span> <span class="nav-text">Common 模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Core-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.2.</span> <span class="nav-text">Core 模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Format-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.3.</span> <span class="nav-text">Format 模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Partitioner-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.4.</span> <span class="nav-text">Partitioner 模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#WAL-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.5.</span> <span class="nav-text">WAL 模块</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Hive-%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.3.3.6.</span> <span class="nav-text">Hive 模块</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kafka-connect-hdfs"><span class="nav-number">7.2.3.4.</span> <span class="nav-text">kafka-connect-hdfs</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%90%BD%E8%84%9A%E7%82%B9"><span class="nav-number">7.2.4.</span> <span class="nav-text">落脚点</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Kafka-Connect-%E7%BB%84%E4%BB%B6%E7%9A%84-ClassLoader-%E9%9A%94%E7%A6%BB"><span class="nav-number">7.2.4.1.</span> <span class="nav-text">Kafka Connect 组件的 ClassLoader 隔离</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E5%9C%A8%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%8C%E6%9B%B4%E6%96%B0%E4%BA%86-ClassLoader%EF%BC%8C%E5%B9%BF%E6%92%AD%E5%88%B0%E6%95%B4%E4%B8%AA%E9%9B%86%E7%BE%A4"><span class="nav-number">7.2.4.2.</span> <span class="nav-text">分布式情况下，如何做到在一个节点上，更新了 ClassLoader，广播到整个集群</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A4%BE%E5%8C%BA%E8%B7%9F%E8%BF%9B"><span class="nav-number">8.</span> <span class="nav-text">社区跟进</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%96%99"><span class="nav-number">9.</span> <span class="nav-text">资料</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Docker"><span class="nav-number">9.1.</span> <span class="nav-text">Docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Book"><span class="nav-number">9.2.</span> <span class="nav-text">Book</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A2%E8%BF%8E%E5%8A%A0%E5%85%A5%E6%88%91%E4%BB%AC%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BE%A4%EF%BC%8C%E4%B8%80%E8%B5%B7%E4%BA%A4%E6%B5%81%E5%AD%A6%E4%B9%A0"><span class="nav-number">10.</span> <span class="nav-text">欢迎加入我们的技术群，一起交流学习</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Benedict Jin" src="/yuzhouwan_logo_128x128.ico">
  <p class="site-author-name" itemprop="name">Benedict Jin</p>
  <div class="site-description" itemprop="description">Benedict Jin's Blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">147</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/asdf2014" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;asdf2014" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:asdf2014@apache.org" title="E-Mail → mailto:asdf2014@apache.org" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-nd.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="external nofollow noopener noreferrer" target="_blank">苏 ICP 备 17032505号 </a>
  </div>

<div class="copyright">
  
  &copy; 2014 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yuzhouwan.com</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20:06</span>
</div>

        
<meta name="referrer" content="always">
<div class="busuanzi-count">
  <script async src="/lib/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: '32px', // default: '32px'
  left: 'unset', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: false // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<style>
button.darkmode-toggle {
  z-index: 9999;
}
img, .darkmode-ignore {
  isolation: isolate;
  display: block;
}
</style>

  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

  

<script>
  var disqus_config = function() {
    this.page.url = "https://yuzhouwan.com/posts/26002/";
    this.page.identifier = "posts/26002/";
    this.page.title = "Apache Kafka 分布式消息队列框架";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://yuzhouwan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e5da61069b94deb8ef3a',
      clientSecret: 'c60ad4e430be258b705027a036eeee3d71cb934b',
      repo        : 'gitment',
      owner       : 'asdf2014',
      admin       : ['asdf2014'],
      id          : 'a33d7ce4657d5f70a63de8fe30437d51',
        language: '',
      distractionFreeMode: false
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'BU4bnWsdAliFfk3fz7iMcFUU-gzGzoHsz',
      appKey     : '1xVLNFSy1qGCda3wt4GiGzHG',
      placeholder: "上述信息都不是必填的，可以直接提交评论",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
