<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/yuzhouwan_logo_with_copyright.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/yuzhouwan_logo_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/yuzhouwan_logo_16x16.ico">
  <link rel="mask-icon" href="/yuzhouwan_logo_with_copyright.jpg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yuzhouwan.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":5,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":{"valine":{"text":"Valine：匿名","order":-1},"disqus":{"text":"Disqus：海外","order":-2},"gitalk":{"text":"Gitalk：可用 Github 账号登录","order":-3}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文主要介绍如何使用 Apache Spark 实现实时的机器学习。">
<meta property="og:type" content="article">
<meta property="og:title" content="Real-time ML with Spark">
<meta property="og:url" content="https://yuzhouwan.com/posts/4735/">
<meta property="og:site_name" content="宇宙湾">
<meta property="og:description" content="本文主要介绍如何使用 Apache Spark 实现实时的机器学习。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuzhouwan.com/picture/spark/spark_dag_process.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/spark/spark_narrow_dependencies_and_wide_dependencies.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/spark/spark_ecosystem.png">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg">
<meta property="article:published_time" content="2015-08-13T11:50:21.000Z">
<meta property="article:modified_time" content="2021-01-31T04:20:33.773Z">
<meta property="article:author" content="Benedict Jin">
<meta property="article:tag" content="Scala">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Apache Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuzhouwan.com/picture/spark/spark_dag_process.png">

<link rel="canonical" href="https://yuzhouwan.com/posts/4735/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Real-time ML with Spark | 宇宙湾</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97020848-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-97020848-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.null { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .null > span { position: relative; z-index: 10; }  .null img, .null .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .null img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .null-fallback { color: inherit; } .null-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="宇宙湾" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">宇宙湾</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">厚积薄发</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">156</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-书单">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>书单</a>

  </li>
        <li class="menu-item menu-item-影视">

    <a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>影视</a>

  </li>
        <li class="menu-item menu-item-友链">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuzhouwan.com/posts/4735/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/yuzhouwan_logo_128x128.ico">
      <meta itemprop="name" content="Benedict Jin">
      <meta itemprop="description" content="Benedict Jin's Blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="宇宙湾">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Real-time ML with Spark
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2015-08-13 19:50:21" itemprop="dateCreated datePublished" datetime="2015-08-13T19:50:21+08:00">2015-08-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-31 12:20:33" itemprop="dateModified" datetime="2021-01-31T12:20:33+08:00">2021-01-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>
            <div class="post-description">本文主要介绍如何使用 Apache Spark 实现实时的机器学习。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="什么是-Spark"><a href="#什么是-Spark" class="headerlink" title="什么是 Spark?"></a>什么是 Spark?</h2><p>　<strong>Apache Spark</strong>™ is a unified analytics engine for large-scale data processing.</p>
<h2 id="为什么要有-Spark"><a href="#为什么要有-Spark" class="headerlink" title="为什么要有 Spark?"></a>为什么要有 Spark?</h2><h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h3><p>　具备经济、快速、可靠、易扩充、数据共享、设备共享、通讯方便、灵活等分布式所具备的特性</p>
<h3 id="高层次抽象"><a href="#高层次抽象" class="headerlink" title="高层次抽象"></a>高层次抽象</h3><p>　<strong>RDD</strong>（<strong>R</strong>esilient <strong>D</strong>istributed <strong>D</strong>atasets）提供 一个可以被并行计算的 不变、分区的数据集 抽象</p>
<h3 id="快速计算能力"><a href="#快速计算能力" class="headerlink" title="快速计算能力"></a>快速计算能力</h3><p>　内存计算 基于内存的迭代计算框架，能够避免计算结果落地，磁盘 <code>I/O</code> 所带来的瓶颈<br>　Machine Learning、Data Mining 等都需要递归地计算，因此非常适合实现这些算法</p>
<h3 id="高效性能"><a href="#高效性能" class="headerlink" title="高效性能"></a>高效性能</h3><p>　<strong>DAG</strong>（<strong>D</strong>irected <strong>A</strong>cyclic <strong>G</strong>rap）利用有向无环图，构建优化任务中 父 RDD 和 子 RDD 的依赖关系</p>
<p><img data-src="/picture/spark/spark_dag_process.png" alt="Spark DAG"></p>
<p>　其中，依赖分为两种，一个为<strong>窄依赖</strong>（Narrow Dependencies），如 <code>map</code> / <code>filter</code> / <code>union</code> 等；另一种为<strong>宽依赖</strong>（Wide Dependencies），如 <code>groupByKey</code> 等</p>
<p>　在划分依赖时，<code>join</code> 需要额外考虑 <code>co-partitione</code>：</p>
<ul>
<li>如果 RDD 和 <code>cogroup</code> 有相同的 数据结构，将会确定一个 <code>OneToOneDependency</code></li>
<li><p>反之，则说明 <code>join</code> 的时候，需要 <code>shuffle</code>（<code>ShuffleDependency</code>）</p>
<p>因为，宽依赖只有等到所有 父 partiton 计算完，并传递结束，才能继续进行下一步运算，所以应该尽量减少宽依赖，避免失败后 recompute 的成本</p>
</li>
</ul>
<p><img data-src="/picture/spark/spark_narrow_dependencies_and_wide_dependencies.png" alt="Narrow Dependencies and Wide Dependencies"></p>
<center>（图片来源：<a href="https://spark.apache.org/" target="_blank" rel="external nofollow noopener noreferrer">Spark</a>™ 官网）</center>


<h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><p>　Lineage 血统，能在计算失败的时候，将会找寻 最小重新计算损耗的 结点，避免全部重新计算</p>
<span id="more"></span>
<h2 id="Spark-核心组件"><a href="#Spark-核心组件" class="headerlink" title="Spark 核心组件"></a>Spark 核心组件</h2><h3 id="主要概念"><a href="#主要概念" class="headerlink" title="主要概念"></a>主要概念</h3><h4 id="物理层面"><a href="#物理层面" class="headerlink" title="物理层面"></a>物理层面</h4><h5 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h5><p>　Master 负责分配资源</p>
<h5 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h5><p>　Worker 负责监控自己节点的内存和 CPU 等状况</p>
<h5 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h5><p>　在集群启动时，Driver 向 Master 申请资源</p>
<p>　运行时 Driver 能获得 Executor 的具体运行资源，Driver 与 Executor 之间直接进行通信，Driver 把 Job 划为 Task 传送给 Executor，Task 就是 Spark 程序的业务逻辑代码</p>
<h5 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h5><p>　Executor 接收任务，进行反序列化，得到数据的输入和输出，在分布式集群的相同数据分片上，数据的业务逻辑一样，只是数据不一样罢了。然后由 Executor 进程中的线程池负责执行，执行的结果汇报再返回汇报给 Driver 进程</p>
<h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>　Task 就是 Spark 程序的业务逻辑代码</p>
<h4 id="逻辑层面"><a href="#逻辑层面" class="headerlink" title="逻辑层面"></a>逻辑层面</h4><h5 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h5><p>　<strong>Row</strong> 表示关系运算中一行输出，本质上来说就是一个数组</p>
<h5 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h5><p>　<strong>DataSet</strong> 和 RDD、DataFrame 一样，都是分布式数据结构的概念。区别在于 DataSet 可以面向特定类型，也就是其无需将输入数据类型限制为 Row（还可以使用 Seq、Array、Product、Int 等类型）</p>
<h5 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h5><p>　<strong>DataFrame</strong> 相当于是 Dataset[Row] 的别名</p>
<h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>　<strong>Encoder</strong> 是 Dataset 中的关键组件，用来将外部类型转化为 Dataset 的内部类型 InternalRow</p>
<h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h3><p>　同时支持 <code>HiveQL</code> / <code>UDFs</code> / <code>SerDes</code> 等多样性的数据源，并采用 <code>JDBC</code> / <code>ODBC</code> 等标准化连接驱动，保证其通用性（整个流程的入口是 <code>org.apache.spark.sql.SparkSession#sql</code> 方法）</p>
<h3 id="Spark-GraphX"><a href="#Spark-GraphX" class="headerlink" title="Spark GraphX"></a>Spark GraphX</h3><p>　支持在 <code>graph</code> 或 <code>collection</code> 中查看数据，并提供丰富的 图形处理 API</p>
<h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><p>　将数据流 按 时间间隔 Duration 划分为一组连续的 RDD，再将这些 RDD 抽象为 DStream<br>　随后，通过对 DStream 这个 high-level 抽象的操作，实现对底层 标记了<code>时间间隙</code>的 RDD 组的操控</p>
<h3 id="Spark-MLbase"><a href="#Spark-MLbase" class="headerlink" title="Spark MLbase"></a>Spark MLbase</h3><p>　提供了对 Machine Learning 的易用、高效的实现<br>　总体的结构，基于 Spark，自底向上分别是，<code>MLlib</code> / <code>MLI</code> / <code>ML Optimizer</code></p>
<ul>
<li>MLlib 这一层，设计了 本地 / 分布式 的矩阵，对稀疏数据的支持，多模型的训练，提供了 计算模型 和 逻辑的 API</li>
<li>MLI 主要任务则是 提供 表针采集器 和 逻辑规则，并进一步对 高层次 ML 编程抽象成接口</li>
<li>ML Optimizer 则是通过自动构建 ML 的 pipe 管道路径实现 ML 优化器的作用。同时，该优化器还解决了一个在 <code>MLI</code> 和 <code>MLlib</code> 中 表征采集器 和 ML 逻辑规则的搜索问题</li>
</ul>
<h2 id="Spark-实时机器学习"><a href="#Spark-实时机器学习" class="headerlink" title="Spark 实时机器学习"></a>Spark 实时机器学习</h2><h3 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Machine_learning">机器学习</a>？</h3><p>　Wikipedia 给出的定义是，一个计算机科学的子领域，由 模式识别 和 人工智能 中的计算机学习理论 演变而来<br>　探索 结构化的、可学习的规则引擎，如何用来对数据 进行训练 和 预测</p>
<h3 id="什么又是-Real-time-机器学习呢？"><a href="#什么又是-Real-time-机器学习呢？" class="headerlink" title="什么又是 Real-time 机器学习呢？"></a>什么又是 Real-time 机器学习呢？</h3><p>　一般性的 机器学习 的对象 是一堆 offline 的训练集，通过对这些数据的学习，来确立模型<br>　如果数据是快速变化的，这时就需要将 新数据 分配好权重，加入到目标训练集中；之后，将预测出来的结果，再次反馈到 数据模型中去</p>
<h3 id="Real-time-和-No-Real-time-的本质区别在哪儿？"><a href="#Real-time-和-No-Real-time-的本质区别在哪儿？" class="headerlink" title="Real-time 和 No Real-time 的本质区别在哪儿？"></a>Real-time 和 No Real-time 的本质区别在哪儿？</h3><p>　因为 实时模型 是动态更新的，实现算法上，比 非实时的 ML 需要考虑，如何避免依赖 将 新数据 和 旧数据 整合在一块再计算所带来的性能问题<br>　更多时候，长期积累的数据，是很难再做到全量计算（比如，多项式贝叶斯 Multinomial naive bayes，用于处理 dataset 过大，而内存不足的情况）</p>
<h2 id="利用-Spark-实现-Real-time-ML"><a href="#利用-Spark-实现-Real-time-ML" class="headerlink" title="利用 Spark 实现 Real-time ML"></a>利用 Spark 实现 Real-time ML</h2><h3 id="源数据流"><a href="#源数据流" class="headerlink" title="源数据流"></a>源数据流</h3><ul>
<li>利用 <code>java.util.Random</code> 产生满足高斯分布的随机数据，再通过 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.scalanlp.org/">breeze</a> 放入到 <code>vector</code> 中，作为<strong>特征值</strong></li>
<li>在 <code>generateNoisyData</code> 中，将这个 <code>vector</code> 做 <code>inner product</code>, 并加入一点噪声数据，作为 <strong>label 标签</strong></li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">MaxEvents</span> = <span class="number">100</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">NumFeatures</span> = <span class="number">100</span></span><br><span class="line"><span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRandomArray</span></span>(n: <span class="type">Int</span>) = <span class="type">Array</span>.tabulate(n)(_ =&gt; random.nextGaussian())</span><br><span class="line"><span class="keyword">val</span> w = <span class="keyword">new</span> <span class="type">DenseVector</span>(generateRandomArray(<span class="type">NumFeatures</span>))</span><br><span class="line"><span class="keyword">val</span> intercept = random.nextGaussian() * <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateNoisyData</span></span>(n: <span class="type">Int</span>) = {</span><br><span class="line">  (<span class="number">1</span> to n).map { i =&gt;</span><br><span class="line">    <span class="keyword">val</span> x = <span class="keyword">new</span> <span class="type">DenseVector</span>(generateRandomArray(<span class="type">NumFeatures</span>))</span><br><span class="line">    <span class="comment">// inner product</span></span><br><span class="line">    <span class="keyword">val</span> y: <span class="type">Double</span> = w.dot(x)</span><br><span class="line">    <span class="keyword">val</span> noisy = y + intercept</span><br><span class="line">    (noisy, x)</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>通过 <code>socket</code> 将数据 发送到指定端口</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length != <span class="number">2</span>) {</span><br><span class="line">  <span class="type">System</span>.err.println(<span class="string">"Usage: &lt;port&gt; &lt;millisecond&gt;"</span>)</span><br><span class="line">  <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">val</span> listener = <span class="keyword">new</span> <span class="type">ServerSocket</span>(args(<span class="number">0</span>).toInt)</span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">  <span class="keyword">val</span> socket = listener.accept()</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>() {</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span> </span>= {</span><br><span class="line">      println(<span class="string">"Got client connected from: "</span> + socket.getInetAddress)</span><br><span class="line">      <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">PrintWriter</span>(socket.getOutputStream(), <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">        <span class="type">Thread</span>.sleep(args(<span class="number">1</span>).toLong)</span><br><span class="line">        <span class="keyword">val</span> num = random.nextInt(<span class="type">MaxEvents</span>)</span><br><span class="line">        <span class="keyword">val</span> data = generateNoisyData(num)</span><br><span class="line">        data.foreach { <span class="keyword">case</span> (y, x) =&gt;</span><br><span class="line">          <span class="keyword">val</span> xStr = x.data.mkString(<span class="string">","</span>)</span><br><span class="line">          <span class="keyword">val</span> content = <span class="string">s"<span class="subst">$y</span>\t<span class="subst">$xStr</span>"</span></span><br><span class="line">          println(content)</span><br><span class="line">          out.write(content)</span><br><span class="line">          out.write(<span class="string">"\n"</span>)</span><br><span class="line">        }</span><br><span class="line">      }</span><br><span class="line">      socket.close()</span><br><span class="line">    }</span><br><span class="line">  }.start()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="实时-Machine-Learning-模型"><a href="#实时-Machine-Learning-模型" class="headerlink" title="实时 Machine Learning 模型"></a>实时 Machine Learning 模型</h3><ul>
<li>指定 <code>spark-master</code> / <code>interval</code> 等参数，创建 StreamingContext（此处可以利用 local[n] 快速开发）</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (args.length &lt; <span class="number">4</span>) {</span><br><span class="line">  <span class="type">System</span>.err.println(<span class="string">"Usage: WindowCounter &lt;master&gt; &lt;hostname&gt; &lt;port&gt; &lt;interval&gt; \n"</span> +</span><br><span class="line">    <span class="string">"In local mode, &lt;master&gt; should be 'local[n]' with n &gt; 1"</span>)</span><br><span class="line">  <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(args(<span class="number">0</span>), <span class="string">"ML Analysis"</span>, <span class="type">Seconds</span>(args(<span class="number">3</span>).toInt))</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>获取到发送过来的 源数据</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream = ssc.socketTextStream(args(<span class="number">1</span>), args(<span class="number">2</span>).toInt, <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>)</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li><p>利用 <code>DenseVector.zeros[Double]</code> 创建全零的初始矩阵</p>
</li>
<li><p>使用 <code>StreamingLinearRegressionWithSGD</code> 创建 流式随机递归下降的线性回归 模型</p>
<p>目前 MLlib 只支持 Streaming（<code>KMeans</code> / <code>LinearRegression</code> / <code>LinearRegressionWithSGD</code>）in <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/apache/spark/">Spark 1.4.1</a><br>Streaming MLlib 和普通的 MLlib 没有本质上的区别，只是输入的训练集是 DStream，需要使用 <code>foreachRDD</code> / <code>map</code> 进行 <strong>训练</strong> / <strong>预测</strong></p>
</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">NumFeatures</span> = <span class="number">100</span></span><br><span class="line"><span class="keyword">val</span> zeroVector = <span class="type">DenseVector</span>.zeros[<span class="type">Double</span>](<span class="type">NumFeatures</span>)</span><br><span class="line"><span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">StreamingLinearRegressionWithSGD</span>()</span><br><span class="line">  .setInitialWeights(<span class="type">Vectors</span>.dense(zeroVector.data))</span><br><span class="line">  .setNumIterations(<span class="number">1</span>)</span><br><span class="line">  .setStepSize(<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">val</span> labeledStream = stream.map { event =&gt;</span><br><span class="line">  <span class="keyword">val</span> split = event.split(<span class="string">"\t"</span>)</span><br><span class="line">  <span class="keyword">val</span> y = split(<span class="number">0</span>).toDouble</span><br><span class="line">  <span class="keyword">val</span> features = split(<span class="number">1</span>).split(<span class="string">","</span>).map(_.toDouble)</span><br><span class="line">  <span class="type">LabeledPoint</span>(label = y, features = <span class="type">Vectors</span>.dense(features))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>利用 模型 进行 <code>train</code> / <code>predict</code></li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model.trainOn(labeledStream)</span><br><span class="line"><span class="keyword">val</span> predictAndTrue = labeledStream.transform { rdd =&gt;</span><br><span class="line">  <span class="keyword">val</span> latest = model.latestModel()</span><br><span class="line">  rdd.map { point =&gt;</span><br><span class="line">    <span class="keyword">val</span> predict = latest.predict(point.features)</span><br><span class="line">    (predict - point.label)</span><br><span class="line">  }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>通过 <strong>MSE</strong>（<strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror）均方差 和 <strong>RMSE</strong>（<strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quared <strong>E</strong>rror）均方根误差 对模型的性能进行评估（这里也可以使用 RegressionMetrics 来实现）</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">predictAndTrue.foreachRDD { (rdd, time) =&gt;</span><br><span class="line">  <span class="keyword">val</span> mse = rdd.map { <span class="keyword">case</span> (err) =&gt; err * err }.mean()</span><br><span class="line">  <span class="keyword">val</span> rmse = math.sqrt(mse)</span><br><span class="line">  println( <span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">              |-------------------------------------------</span></span><br><span class="line"><span class="string">              |Time: $time</span></span><br><span class="line"><span class="string">              |-------------------------------------------</span></span><br><span class="line"><span class="string">                  "</span><span class="string">""</span>.stripMargin)</span><br><span class="line">  println(<span class="string">s"MSE current batch: Model : <span class="subst">$mse</span>"</span>)</span><br><span class="line">  println(<span class="string">s"RMSE current batch: Model : <span class="subst">$rmse</span>"</span>)</span><br><span class="line">  println(<span class="string">"...\n"</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>启动 Spark 上下文</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="一劳永逸了？Not-at-all"><a href="#一劳永逸了？Not-at-all" class="headerlink" title="一劳永逸了？Not at all!"></a>一劳永逸了？Not at all!</h2><p>　一个优秀的 Machine Learning 模型，是要结合具体业务，从对数据流入的清洗，特征值维度的考量，模型类型的选择，到最终的性能的评估、监控、持续优化，都需要仔细地考究，最终才能打造出高效、稳定、精准的数据模型</p>
<h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>　对目标数据集进行处理之前，首先就是对数据的类型进行归类，是数值型、类别型、文本型，还是其他一些多媒体、地理信息等<br>　针对不同的数据，分别采取不同的处理手段，对于类别型常用 <code>1-of-k encoding</code> 对每个类别进行编码<br>　对于文本型，则会采用 分词、移除 <code>stop words</code>（的、这、地；<code>the</code>/<code>and</code>/<code>but</code> …）、向量化、标准化（避免度量单位的影响）</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">x = np.random.randn(<span class="number">10</span>)</span><br><span class="line">norm_x_2 = np.linalg.norm(x)</span><br><span class="line">normalized_x = x / norm_x_2</span><br><span class="line"><span class="keyword">print</span> <span class="string">"x:\n%s"</span> % x</span><br><span class="line"><span class="keyword">print</span> <span class="string">"2-Norm of x: %2.4f"</span> % norm_x_2</span><br><span class="line"><span class="keyword">print</span> <span class="string">"Normalized x:\n%s"</span> % normalized_x</span><br><span class="line"><span class="keyword">print</span> <span class="string">"2-Norm of normalized_x: %2.4f"</span> % np.linalg.norm(normalized_x)</span><br></pre></td></tr></tbody></table></figure>
<p>　还有还多常用的数据处理方式，如 平均值、中位数、总和、方差、差值、最大值、最小值<br>　针对时间的处理，还可以加上 “时间戳” 字段</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">assign_tod</span>(<span class="params">hr</span>):</span></span><br><span class="line">    times_of_day = {</span><br><span class="line">        <span class="string">'morning'</span> : range(<span class="number">7</span>, <span class="number">12</span>),</span><br><span class="line">        <span class="string">'lunch'</span> : range(<span class="number">12</span>, <span class="number">14</span>),</span><br><span class="line">        <span class="string">'afternoon'</span> : range(<span class="number">14</span>, <span class="number">18</span>),</span><br><span class="line">        <span class="string">'evening'</span> : range(<span class="number">18</span>, <span class="number">23</span>),</span><br><span class="line">        <span class="string">'night'</span> : range(<span class="number">23</span>, <span class="number">7</span>)</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> times_of_day.iteritems():</span><br><span class="line">    <span class="keyword">if</span> hr <span class="keyword">in</span> v:</span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line">time_of_day = hour_of_day.map(<span class="keyword">lambda</span> hr: assign_tod(hr))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="特征维度"><a href="#特征维度" class="headerlink" title="特征维度"></a>特征维度</h3><p>　常见的一个影响模型的因素，便是没有对特征 进行标准化</p>
<figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(element-wise - the preceding mean vector from the feature vector) / the vector of feature standard deviations</span><br></pre></td></tr></tbody></table></figure>
<p>　利用 StandarScaler 完成标准化工作</p>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.feature.<span class="type">StandardScaler</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> scaler = <span class="keyword">new</span> <span class="type">StandardScaler</span>(withMean = <span class="literal">true</span>, withStd = <span class="literal">true</span>).fit(vectors)</span><br><span class="line"><span class="keyword">val</span> scaledData = data.map(lp =&gt; <span class="type">LabeledPoint</span>(lp.label, scaler.transform(lp.features)))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="调整模型"><a href="#调整模型" class="headerlink" title="调整模型"></a>调整模型</h3><p>　首先需要在众多的模型 和 对应的算法 中找到最为适用的选择<br>　模型的类别主要有，<strong>推荐引擎</strong>、<strong>分类模型</strong>、<strong>回归模型</strong>、<strong>聚类模型</strong> 等<br>　相应的实现算法，又有（线性 / 逻辑 / 多元）<strong>回归</strong>、（随机森林）<strong>决策树</strong>、（朴素 / 高斯 / 多项式 / 伯努利 / 信念网络）<strong>贝叶斯</strong> 等<br>　在选择的时候，更多会考虑 特征值是否<strong>多维</strong>（可以尝试降维），目标类别是 <strong>multiclass</strong>，<strong>binary</strong>，还是 <strong>probability</strong>（连续值）</p>
<ul>
<li>根据 数据集的 稀疏程度 对正则化（Regularizer）进行调整</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zero: 没有任何正规化操作</span><br><span class="line"><span class="type">L1</span>:   <span class="type">SGD</span>（<span class="type">Stochastic</span> gradient descent，随机梯度下降）</span><br><span class="line"><span class="type">L2</span>:   <span class="type">LBFGS</span>（<span class="type">Limited</span>-memory <span class="type">BFGS</span>，受限的 <span class="type">BFGS</span>）</span><br><span class="line"></span><br><span class="line"><span class="type">L2</span> 相比 <span class="type">L1</span> 更为平滑（同样，<span class="type">L1</span> 可以让 稀疏的数据集 得到更 直观的模型）</span><br><span class="line">还有其它 求最优解 的方法，如 求全局最优解的 <span class="type">BGD</span>（<span class="type">Batch</span> gradient descent，批量梯度下降）</span><br><span class="line">但是，由于每次迭代都需要依据训练集中所有的数据，所以速度很慢；</span><br><span class="line">以及 <span class="type">CG</span>（<span class="type">Conjugate</span> gradient，共轭梯度法），但还没有被 <span class="type">Spark</span> <span class="type">MLlib</span> 所支持，可以在 <span class="type">Breeze</span> 中找到它</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>可以通过 <code>setUpdater</code> 将模型的 规则化算法 设置为 <code>L1</code>（默认为 <code>L2</code>）</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.optimization.<span class="type">L1Updater</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> svmAlg = <span class="keyword">new</span> <span class="type">SVMWithSGD</span>()</span><br><span class="line">svmAlg.optimizer</span><br><span class="line">  .setNumIterations(<span class="number">200</span>)</span><br><span class="line">  .setRegParam(<span class="number">0.1</span>)</span><br><span class="line">  .setUpdater(<span class="keyword">new</span> <span class="type">L1Updater</span>)</span><br><span class="line"><span class="keyword">val</span> modelL1 = svmAlg.run(training)</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>当然，还有举不胜举的优化方式，详见文章最后的 <a href="https://yuzhouwan.com/posts/4735/#整体知识树">Spark 思维导图</a> :-)</li>
</ul>
<h3 id="性能评估指标"><a href="#性能评估指标" class="headerlink" title="性能评估指标"></a>性能评估指标</h3><ul>
<li>针对不同的业务，对性能评测的手段，也需要相应取舍，毕竟有些 “宁可错杀一千” 的变态 防护系统，就需要对 <code>recall</code> 有较高的要求，而 <code>precision</code> 则相对宽松些<ul>
<li>这时便可采用 <strong>ROC</strong>（<strong>R</strong>eceiver <strong>O</strong>perating <strong>C</strong>haracteristic）curve 评测引擎：</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// binary classification</span></span><br><span class="line"><span class="keyword">val</span> metrics = <span class="type">Seq</span>(lrModel, svmModel).map { model =&gt;</span><br><span class="line">  <span class="keyword">val</span> scoreAndLabels = data.map { point =&gt;</span><br><span class="line">    (model.predict(point.features), point.label)</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">val</span> metrics = <span class="keyword">new</span> <span class="type">BinaryClassificationMetrics</span>(scoreAndLabels)</span><br><span class="line">  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>如果是 <strong>贝叶斯</strong> / <strong>决策树</strong> 的数据模型，则可以用 <code>0.5</code> 对其进行划分，转换为 <code>binary</code> 分类问题</li>
</ul>
<figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// naive bayes</span></span><br><span class="line"><span class="keyword">val</span> nbMetrics = <span class="type">Seq</span>(nbModel).map { model =&gt;</span><br><span class="line">  <span class="keyword">val</span> scoreAndLabels = nbData.map { point =&gt;</span><br><span class="line">    <span class="keyword">val</span> score = model.predict(point.features)</span><br><span class="line">    (<span class="keyword">if</span> (score &gt; <span class="number">0.5</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>, point.label)</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">val</span> metrics = <span class="keyword">new</span> <span class="type">BinaryClassificationMetrics</span>(scoreAndLabels)</span><br><span class="line">  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)</span><br><span class="line">}</span><br><span class="line"><span class="comment">// decision tree</span></span><br><span class="line"><span class="keyword">val</span> dtMetrics = <span class="type">Seq</span>(dtModel).map { model =&gt;</span><br><span class="line">  <span class="keyword">val</span> scoreAndLabels = data.map { point =&gt;</span><br><span class="line">    <span class="keyword">val</span> score = model.predict(point.features)</span><br><span class="line">    (<span class="keyword">if</span> (score &gt; <span class="number">0.5</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>, point.label)</span><br><span class="line">  }</span><br><span class="line">  <span class="keyword">val</span> metrics = <span class="keyword">new</span> <span class="type">BinaryClassificationMetrics</span>(scoreAndLabels)</span><br><span class="line">  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">val</span> allMetrics = metrics ++ nbMetrics ++ dtMetrics</span><br><span class="line">allMetrics.foreach { <span class="keyword">case</span> (m, pr, roc) =&gt;</span><br><span class="line">  println(<span class="string">f"<span class="subst">$m</span>, Area under PR: <span class="subst">${pr * 100.0}</span>%2.4f%%, Area under ROC: <span class="subst">${roc * 100.0}</span>%2.4f%%"</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>然而，如果是一些推荐系统，更多的希望能够了解到 大体的预测精度，则可以采用 <strong>MAP</strong>（<strong>M</strong>ean <strong>A</strong>verage <strong>P</strong>recision）平均精度均值 进行评估</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAP 同时还解决了 precision，recall，F-measure 中存在的单点值局限性问题</span><br></pre></td></tr></tbody></table></figure>
<h2 id="踩过的坑"><a href="#踩过的坑" class="headerlink" title="踩过的坑"></a>踩过的坑</h2><h3 id="spark-任务使用-UGI-之后，实际-task-在生成中间文件的时候，没有感知到外层设置的-UGI-信息"><a href="#spark-任务使用-UGI-之后，实际-task-在生成中间文件的时候，没有感知到外层设置的-UGI-信息" class="headerlink" title="spark 任务使用 UGI 之后，实际 task 在生成中间文件的时候，没有感知到外层设置的 UGI 信息"></a>spark 任务使用 UGI 之后，实际 task 在生成中间文件的时候，没有感知到外层设置的 UGI 信息</h3><h4 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /user | grep yuzhouwan</span><br><span class="line">  drwxrwxrwx   - yuzhouwan yuzhouwan          0 2018-04-24 14:27 /user/yuzhouwan</span><br><span class="line">$ hdfs dfs -ls /user/yuzhouwan</span><br><span class="line">  drwxrwxrwx   - user_A user_A                0 2018-04-24 14:27 /user/yuzhouwan/user_A</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代码中，需要将数据，写入到 /user/yuzhouwan/user_A 中，但是报错权限不足</span></span><br><span class="line">UGICache.doAs(<span class="string">"user_A"</span>, () =&gt; {</span><br><span class="line">  sparkContext.parallelize(Seq(1 to 10), 1).saveAsTextFile(<span class="string">"/user/yuzhouwan/user_A/seq"</span>)</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=user_A, access=WRITE, inode=<span class="string">"/user/yuzhouwan/user_A/seq"</span>:yuzhouwan:user_A:drwxr-xr-x</span><br></pre></td></tr></tbody></table></figure>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HADOOP_HOME</span>/logs</span><br><span class="line">$ grep <span class="string">"/user/yuzhouwan/user_A/"</span> hdfs-audit.log | grep -v <span class="string">"getfileinfo"</span></span><br><span class="line">  2018-04-23 19:04:34,764 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">true</span>	ugi=user_A (auth:SIMPLE)	ip=/10.10.10.1	cmd=mkdirs	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0	dst=null	perm=user_A:yuzhouwan:rwxr-xr-x	proto=rpc</span><br><span class="line">  2018-04-23 19:04:35,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">true</span>	ugi=yuzhouwan (auth:SIMPLE)	ip=/10.10.10.1	cmd=create	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0/_temporary/attempt_20180423190434_0723_m_000000_737/part-00000.deflate	dst=null	perm=yuzhouwan:yuzhouwan:rw-r--r--	proto=rpc</span><br><span class="line">  2018-04-23 19:04:36,017 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">true</span>	ugi=yuzhouwan (auth:SIMPLE)	ip=/10.10.10.1	cmd=rename	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0/_temporary/attempt_20180423190434_0723_m_000000_737	dst=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0/task_20180423190434_0723_m_000000	perm=yuzhouwan:yuzhouwan:rwxr-xr-x	proto=rpc</span><br><span class="line">  2018-04-23 19:04:36,040 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">true</span>	ugi=user_A (auth:SIMPLE)	ip=/10.10.10.1	cmd=listStatus	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0	dst=null	perm=null	proto=rpc</span><br><span class="line">  2018-04-23 19:04:36,060 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">true</span>	ugi=user_A (auth:SIMPLE)	ip=/10.10.10.1	cmd=listStatus	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0/task_20180423190434_0723_m_000000	dst=null	perm=null	proto=rpc</span><br><span class="line">  2018-04-23 19:04:36,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: allowed=<span class="literal">false</span>	ugi=user_A (auth:SIMPLE)	ip=/10.10.10.1	cmd=rename	src=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/_temporary/0/task_20180423190434_0723_m_000000/part-00000.deflate	dst=/user/yuzhouwan/user_A/2018-04-23/940_2018-04-23_user_A_68673976_0/part-00000.deflate	perm=null	proto=rpc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过查看 HDFS 的 audit 审计日志，可以看到创建 deflate 中间文件的时候，使用的 UGI 是 yuzhouwan，而不是 user_A，导致到最后一步，使用 user_A 用户 rename 的时候，报错权限不足</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>　首先，需要保证用户 user_A 不需要写入的数据，只能由它自己才能查看。一方面，如果权限控制不在 spark 任务中，而是交给上层应用来控制，则完全可以使用 yuzhouwan 用户权限写入到 HDFS 中；另一方面，也可以先使用 yuzhouwan 用户权限写入，然后再通过 chown 赋权到 user_A 用户下</p>
<h3 id="saveAsTextFile-不传入压缩类型，也对输出文件进行了压缩"><a href="#saveAsTextFile-不传入压缩类型，也对输出文件进行了压缩" class="headerlink" title="saveAsTextFile 不传入压缩类型，也对输出文件进行了压缩"></a>saveAsTextFile 不传入压缩类型，也对输出文件进行了压缩</h3><h4 id="描述-1"><a href="#描述-1" class="headerlink" title="描述"></a>描述</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ spark-shell --master <span class="built_in">local</span></span><br><span class="line">  scala&gt; sc.parallelize(Seq(1 to 10), 1).collect.foreach(<span class="built_in">print</span>(_))</span><br><span class="line">    Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br><span class="line"></span><br><span class="line">  scala&gt; sc.parallelize(Seq(1 to 10), 1).saveAsTextFile(<span class="string">"/user/yuzhouwan"</span>)</span><br><span class="line">    18/04/24 11:22:01 WARN DefaultCodec: DefaultCodec.createOutputStream() may leak memory. Create a compressor first.</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -ls /user/yuzhouwan</span><br><span class="line">  Found 2 items</span><br><span class="line">  -rw-r--r--   3 bigdata yuzhouwan          0 2018-04-24 11:22 /user/yuzhouwan/_SUCCESS</span><br><span class="line">  -rw-r--r--   3 bigdata yuzhouwan         45 2018-04-24 11:22 /user/yuzhouwan/part-00000.deflate</span><br></pre></td></tr></tbody></table></figure>
<h4 id="解决-1"><a href="#解决-1" class="headerlink" title="解决"></a>解决</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现依赖的 Hadoop 配置文件中，已经开启了压缩</span></span><br><span class="line">$ vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/mapred-site.xml</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.compress.map.output&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要设置 spark.hadoop.mapreduce.output.fileoutputformat.compress=false</span></span><br><span class="line"><span class="comment"># 或者 spark.hadoop.mapred.output.compress=false</span></span><br><span class="line"><span class="comment"># 前者，是负责往 hadoop 和 hive 里面写的时候，会设置压缩；后者，是负责往 json / txt / csv 里面写的时候，设置压缩</span></span><br><span class="line"><span class="comment"># 一个是面向 目标系统类型，另一个是面向 目标文件类型</span></span><br><span class="line">$ spark-shell --master <span class="built_in">local</span> --conf spark.hadoop.mapreduce.output.fileoutputformat.compress=<span class="literal">false</span></span><br><span class="line">  scala&gt; sc.parallelize(Seq(1 to 10), 1).saveAsTextFile(<span class="string">"/user/yuzhouwan0"</span>)</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -ls /user/yuzhouwan0</span><br><span class="line">  Found 2 items</span><br><span class="line">  -rw-r--r--   3 bigdata yuzhouwan          0 2018-04-24 11:41 /user/yuzhouwan0/_SUCCESS</span><br><span class="line">  -rw-r--r--   3 bigdata yuzhouwan         37 2018-04-24 11:41 /user/yuzhouwan0/part-00000</span><br><span class="line"></span><br><span class="line">$ hdfs dfs -cat /user/yuzhouwan0/part-00000</span><br><span class="line">  Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// saveAsTextFile 方法的第二个参数可以指定压缩类型，可以覆盖全局配置中，已经存在的压缩配置</span></span><br><span class="line"><span class="comment">// 需要注意的是，压缩算法只支持 CompressionCodec 接口的实现子类，包括 DefaultCodec、BZip2Codec、GzipCodec、HadoopSnappyCodec、Lz4Codec、SnappyCodec etc.</span></span><br><span class="line">sc.parallelize(<span class="type">Seq</span>(<span class="number">1</span> to <span class="number">10</span>), <span class="number">1</span>).coalesce(<span class="number">1</span>, shuffle = <span class="literal">true</span>).saveAsTextFile(<span class="string">"/user/yuzhouwan1"</span>, classOf[org.apache.hadoop.io.compress.<span class="type">GzipCodec</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="无法判断-spark-session-中临时函数是否存在"><a href="#无法判断-spark-session-中临时函数是否存在" class="headerlink" title="无法判断 spark session 中临时函数是否存在"></a>无法判断 spark session 中临时函数是否存在</h3><h4 id="描述-2"><a href="#描述-2" class="headerlink" title="描述"></a>描述</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取 session state 中判断临时函数 FunctionIdentifier 是否存在，但是仍然不行</span></span><br><span class="line"><span class="keyword">if</span> (!sparkSession.sessionState.catalog.functionExists(yuzhouwanIdentifier)) {</span><br><span class="line">  sparkSession.sql(<span class="string">"CREATE TEMPORARY FUNCTION yuzhouwan as 'org.apache.spark.sql.hive.udf.YuZhouWan'"</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="解决-2"><a href="#解决-2" class="headerlink" title="解决"></a>解决</h4><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!sparkSession.catalog.functionExists(<span class="string">"yuzhouwan"</span>)) {</span><br><span class="line">  sparkSession.sql(<span class="string">"CREATE TEMPORARY FUNCTION yuzhouwan as 'org.apache.spark.sql.hive.udf.YuZhouWan'"</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h4 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h4><p>　目前，Spark SQL 中 <code>IF NOT EXISTS</code> 的语法只支持创建 DATABASE、TABLE（包括 临时表 和 外部表）、VIEW（不包括 临时视图）、SCHEMA 的时候使用，尚<a target="_blank" rel="external nofollow noopener noreferrer" href="https://issues.apache.org/jira/browse/SPARK-24077">不支持</a>临时函数</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ spark-shell --master <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line">scala&gt; org.apache.spark.sql.SparkSession.builder().enableHiveSupport.getOrCreate.sql(<span class="string">"CREATE TEMPORARY FUNCTION IF NOT EXISTS yuzhouwan as 'org.apache.spark.sql.hive.udf.YuZhouWan'"</span>)</span><br><span class="line"></span><br><span class="line">  org.apache.spark.sql.catalyst.parser.ParseException: mismatched input <span class="string">'NOT'</span> expecting {<span class="string">'.'</span>, <span class="string">'AS'</span>}(line 1, pos 29)</span><br><span class="line"></span><br><span class="line">  == SQL ==</span><br><span class="line">  CREATE TEMPORARY FUNCTION IF NOT EXISTS yuzhouwan as <span class="string">'org.apache.spark.sql.hive.udf.YuZhouWan'</span></span><br><span class="line">  -----------------------------^^^</span><br><span class="line">    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:197)</span><br><span class="line">    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:99)</span><br><span class="line">    at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)</span><br><span class="line">    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)</span><br><span class="line">    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)</span><br><span class="line">    ... 48 elided</span><br></pre></td></tr></tbody></table></figure>
<h3 id="控制-spark-结果文件数量，并返回唯一的文件路径"><a href="#控制-spark-结果文件数量，并返回唯一的文件路径" class="headerlink" title="控制 spark 结果文件数量，并返回唯一的文件路径"></a>控制 spark 结果文件数量，并返回唯一的文件路径</h3><h4 id="解决-3"><a href="#解决-3" class="headerlink" title="解决"></a>解决</h4><figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.<span class="type">URI</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.<span class="type">Configuration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.{<span class="type">FileSystem</span>, <span class="type">Path</span>}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将数据写入到某一个文件下，会产生两个文件 _SUCCESS 和 part-00000-126ddb4d-22a8-4c3d-88cb-52a59da4c66a.json</span></span><br><span class="line"><span class="keyword">val</span> exportDir = <span class="string">"/user/yuzhouwan/export"</span></span><br><span class="line">sparkSession.sql(subTaskSql).limit(<span class="number">1000</span>).coalesce(<span class="number">1</span>).write.json(exportDir)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 排除掉 _SUCCESS 文件，拿到 json 文件的全路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSingleExportFile</span></span>(exportDir: <span class="type">String</span>): <span class="type">String</span> = {</span><br><span class="line">  <span class="keyword">val</span> sourceFS = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> <span class="type">URI</span>(exportDir), <span class="keyword">new</span> <span class="type">Configuration</span>())</span><br><span class="line">  <span class="keyword">if</span> (sourceFS == <span class="literal">null</span> || !sourceFS.exists(<span class="keyword">new</span> <span class="type">Path</span>(exportDir))) <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">  <span class="keyword">val</span> filesStatus = sourceFS.listStatus(<span class="keyword">new</span> <span class="type">Path</span>(exportDir))</span><br><span class="line">  <span class="keyword">if</span> (filesStatus == <span class="literal">null</span> || filesStatus.length &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">  <span class="keyword">for</span> (fileStatus &lt;- filesStatus) {</span><br><span class="line">    <span class="keyword">if</span> (fileStatus.isFile) {</span><br><span class="line">      <span class="keyword">val</span> path = fileStatus.getPath.getName</span><br><span class="line">      <span class="keyword">if</span> (!path.contains(<span class="string">"SUCCESS"</span>)) {</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">if</span> (exportDir.endsWith(<span class="string">"/"</span>)) exportDir.concat(path) <span class="keyword">else</span> exportDir.concat(<span class="string">"/"</span>).concat(path)</span><br><span class="line">      }</span><br><span class="line">    }</span><br><span class="line">  }</span><br><span class="line">  <span class="string">""</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> singleFilePath = getSingleExportFile(exportDir)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="KafkaUtils-createDirectStream-报错-OffsetOutOfRangeException"><a href="#KafkaUtils-createDirectStream-报错-OffsetOutOfRangeException" class="headerlink" title="KafkaUtils.createDirectStream 报错 OffsetOutOfRangeException"></a>KafkaUtils.createDirectStream 报错 OffsetOutOfRangeException</h3><h4 id="描述-3"><a href="#描述-3" class="headerlink" title="描述"></a>描述</h4><figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;scala.short.version&gt;<span class="number">2.11</span>&lt;/scala.short.version&gt;</span><br><span class="line">&lt;scala.version&gt;${scala.short.version}<span class="number">.8</span>&lt;/scala.version&gt;</span><br><span class="line">&lt;spark.version&gt;<span class="number">2.1</span><span class="number">.0</span><span class="number">.5</span>&lt;/spark.version&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-core_${scala.short.version}&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;${spark.version}&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"><span class="keyword">val</span> messageHandler = (mam: <span class="type">MessageAndMetadata</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]]) =&gt; (mam.key, mam.message)</span><br><span class="line"><span class="type">KafkaUtils</span>.createDirectStream[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">DefaultDecoder</span>, <span class="type">DefaultDecoder</span>, (<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])](ssc, kafkaParams, fromOffsets, messageHandler)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Caused by: kafka.common.OffsetOutOfRangeException</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</span><br><span class="line">    at java.lang.Class.newInstance(Class.java:374)</span><br><span class="line">    at kafka.common.ErrorMapping$.exceptionFor(ErrorMapping.scala:86)</span><br><span class="line">    at org.apache.spark.streaming.kafka.KafkaRDD<span class="variable">$KafkaRDDIterator</span>.handleFetchErr(KafkaRDD.scala:184)</span><br><span class="line">    at org.apache.spark.streaming.kafka.KafkaRDD<span class="variable">$KafkaRDDIterator</span>.fetchBatch(KafkaRDD.scala:193)</span><br><span class="line">    at org.apache.spark.streaming.kafka.KafkaRDD<span class="variable">$KafkaRDDIterator</span>.getNext(KafkaRDD.scala:208)</span><br><span class="line">    at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)</span><br><span class="line">    at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)</span><br><span class="line">    at org.apache.spark.storage.BlockManager$<span class="variable">$anonfun</span><span class="variable">$doPutIterator</span><span class="variable">$1</span>.apply(BlockManager.scala:957)</span><br><span class="line">    at org.apache.spark.storage.BlockManager$<span class="variable">$anonfun</span><span class="variable">$doPutIterator</span><span class="variable">$1</span>.apply(BlockManager.scala:948)</span><br><span class="line">    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)</span><br><span class="line">    at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)</span><br><span class="line">    at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)</span><br><span class="line">    at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)</span><br><span class="line">    at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)</span><br><span class="line">    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br><span class="line">    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)</span><br><span class="line">    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)</span><br><span class="line">    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h4><p>　该问题是因为 Kafka 的 topic 中，数据因为长时间未消费，超出了 <code>log.retention.[hours|minutes|ms]</code> 时间（默认 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/documentation/#brokerconfigs">168</a> 小时，也就是一周）。此时，broker 会将这部分数据清除掉，并更新 offset 信息（offset 变小了）。但是，程序中仍然用的是之前的 offset 信息，所以就会报错超出了现有 offset 的范围</p>
<h4 id="解决-4"><a href="#解决-4" class="headerlink" title="解决"></a>解决</h4><figure class="highlight scala"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用不指定 offset 的 createDirectStream 重载方法，并重启 spark streaming 程序</span></span><br><span class="line"><span class="type">KafkaUtils</span>.createDirectStream[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">DefaultDecoder</span>, <span class="type">DefaultDecoder</span>](ssc, kafkaParams, topicSet)</span><br></pre></td></tr></tbody></table></figure>
<p><br></p>
<h2 id="整体知识树"><a href="#整体知识树" class="headerlink" title="整体知识树"></a>整体知识树</h2><p><strong>至此，相信你已经对 Spark 这个生态圈有了大致了解了，下面就是一步一步地 在 实践 和 深入学习中，体验大数据的乐趣啦 O(∩_∩)O~~</strong></p>
<p><img data-src="/picture/spark/spark_ecosystem.png" alt="Spark EcoSystem"></p>
<center>（利用 <a href="https://www.xmind.net/" target="_blank" rel="external nofollow noopener noreferrer">XMind</a>™ 绘制而成）</center>



<h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><h3 id="Blog"><a href="#Blog" class="headerlink" title="Blog"></a>Blog</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/summerDG/spark-code-analysis">Spark 源码分析</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://blog.csdn.net/oopsoom/article/details/38257749">Spark SQL 源码分析系列文章</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html">Deep Dive into Spark SQL’s Catalyst Optimizer</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://hbasefly.com/2017/04/10/bigdata-join-2/">SparkSQL – 从 0 到 1 认识 Catalyst</a></li>
</ul>
<h3 id="Book"><a href="#Book" class="headerlink" title="Book"></a>Book</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/PySpark-Recipes-Problem-Solution-Approach-PySpark2/dp/1484231406">PySpark Recipes</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491972955">Advanced Analytics with Spark</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203">High Performance Spark</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Fast-Data-Processing-Spark-Third/dp/1785889273">Fast Data Processing with Spark 2, 3rd Edition</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Apache-Spark-Beginners-Rajanarayanan-Thottuvaikkatumana/dp/1785885006">Apache Spark 2 for Beginners</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Spark-Data-Science-Srinivas-Duvvuri/dp/1785885650">Spark for Data Science</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Spark-GraphX-Action-Michael-Malak/dp/1617292524">Spark GraphX in Action</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Pro-Spark-Streaming-Real-Time-Analytics/dp/1484214803">Pro Spark Streaming</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Machine-Learning-Spark-Rajdeep-Dua/dp/1785889931">Machine Learning with Spark, 2nd Edition</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Apache-Spark-2-x-Cookbook-Cloud-ready/dp/1787127265">Apache Spark 2.x Cookbook</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624">Learning Spark</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Big-Data-Analytics-Spark-Practitioners/dp/1484209656">Big Data Analytics with Spark</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Spark-Python-Developers-Amit-Nandi/dp/1784399698">Spark for Python Developers</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Mastering-Apache-Spark-2-x-DeepLearning4j/dp/1786462745">Mastering Apache Spark 2.x, 2nd Edition</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Apache-Spark-Processing-Rindra-Ramamonjison/dp/1784391808">Apache Spark Graph Processing</a></li>
</ul>
<h3 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/zhuyiche/awesome-anomaly-detection">Awesome Anomaly Detection</a></li>
</ul>
<h2 id="欢迎加入我们的技术群，一起交流学习"><a href="#欢迎加入我们的技术群，一起交流学习" class="headerlink" title="欢迎加入我们的技术群，一起交流学习"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/asdf2014/yuzhouwan#technical-discussion-group">欢迎加入我们的技术群，一起交流学习</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">群名称</th>
<th style="text-align:center">群号</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">人工智能（高级）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=71c6bd3fb0ff01d93abca654140387d99d3be752f92a53c1fbfd27f2dd4b4247"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">人工智能（进阶）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=deb268f65589a1a0a1dbaf7b72c849ed45298697805bef81e0c613dea40cd05e"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">BigData</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=f86b3c8de20da1658a3bb42df17a2fc4eee0d75c4a130a63585fdd257e3565ed"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">算法</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=bfbcf1453371a0810fd6be235ace47147f6fb9d262fb768b497c861f50af0af4"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg" alt></a></td>
</tr>
</tbody>
</table>
</div>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/18651/" rel="bookmark">Scala 实战</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/190816/" rel="bookmark">Gradle 实战</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/27328/" rel="bookmark">如何运用 JVM 知识提高编程水平</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/2254/" rel="bookmark">Maven 高级玩法</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/17444/" rel="bookmark">Qcon 2015 见闻之一：猿题库</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Benedict Jin
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuzhouwan.com/posts/4735/" title="Real-time ML with Spark">https://yuzhouwan.com/posts/4735/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-ND</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Scala/" rel="tag"># Scala</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/Apache-Spark/" rel="tag"># Apache Spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/25015/" rel="prev" title="Apache Storm 与 Kafka 的整合应用">
      <i class="fa fa-chevron-left"></i> Apache Storm 与 Kafka 的整合应用
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/27328/" rel="next" title="如何运用 JVM 知识提高编程水平">
      如何运用 JVM 知识提高编程水平 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-gitalk">Gitalk：可用 Github 账号登录</a></li>
            <li class="tab"><a href="#comment-disqus">Disqus：海外</a></li>
            <li class="tab"><a href="#comment-valine">Valine：匿名</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments" id="gitalk-container"></div>
            </div>
            <div class="tab-pane disqus" id="comment-disqus">
              
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
            </div>
            <div class="tab-pane valine" id="comment-valine">
              <div class="comments" id="valine-comments"></div>
            </div>
        </div>
      </div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-Spark"><span class="nav-number">1.</span> <span class="nav-text">什么是 Spark?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89-Spark"><span class="nav-number">2.</span> <span class="nav-text">为什么要有 Spark?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F"><span class="nav-number">2.1.</span> <span class="nav-text">分布式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E5%B1%82%E6%AC%A1%E6%8A%BD%E8%B1%A1"><span class="nav-number">2.2.</span> <span class="nav-text">高层次抽象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="nav-number">2.3.</span> <span class="nav-text">快速计算能力</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%95%88%E6%80%A7%E8%83%BD"><span class="nav-number">2.4.</span> <span class="nav-text">高效性能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E9%94%99%E6%80%A7"><span class="nav-number">2.5.</span> <span class="nav-text">容错性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">3.</span> <span class="nav-text">Spark 核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5"><span class="nav-number">3.1.</span> <span class="nav-text">主要概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%89%A9%E7%90%86%E5%B1%82%E9%9D%A2"><span class="nav-number">3.1.1.</span> <span class="nav-text">物理层面</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Master"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">Master</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Worker"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">Worker</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Driver"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">Driver</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Executor"><span class="nav-number">3.1.1.4.</span> <span class="nav-text">Executor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Task"><span class="nav-number">3.1.1.5.</span> <span class="nav-text">Task</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%B1%82%E9%9D%A2"><span class="nav-number">3.1.2.</span> <span class="nav-text">逻辑层面</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Row"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">Row</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dataset"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">Dataset</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrame"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Encoder"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">Encoder</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL"><span class="nav-number">3.2.</span> <span class="nav-text">Spark SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-GraphX"><span class="nav-number">3.3.</span> <span class="nav-text">Spark GraphX</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Streaming"><span class="nav-number">3.4.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-MLbase"><span class="nav-number">3.5.</span> <span class="nav-text">Spark MLbase</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E5%AE%9E%E6%97%B6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">Spark 实时机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="nav-number">4.1.</span> <span class="nav-text">什么是机器学习？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E5%8F%88%E6%98%AF-Real-time-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A2%EF%BC%9F"><span class="nav-number">4.2.</span> <span class="nav-text">什么又是 Real-time 机器学习呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Real-time-%E5%92%8C-No-Real-time-%E7%9A%84%E6%9C%AC%E8%B4%A8%E5%8C%BA%E5%88%AB%E5%9C%A8%E5%93%AA%E5%84%BF%EF%BC%9F"><span class="nav-number">4.3.</span> <span class="nav-text">Real-time 和 No Real-time 的本质区别在哪儿？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%A9%E7%94%A8-Spark-%E5%AE%9E%E7%8E%B0-Real-time-ML"><span class="nav-number">5.</span> <span class="nav-text">利用 Spark 实现 Real-time ML</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BA%90%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="nav-number">5.1.</span> <span class="nav-text">源数据流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E6%97%B6-Machine-Learning-%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.2.</span> <span class="nav-text">实时 Machine Learning 模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E5%8A%B3%E6%B0%B8%E9%80%B8%E4%BA%86%EF%BC%9FNot-at-all"><span class="nav-number">6.</span> <span class="nav-text">一劳永逸了？Not at all!</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE"><span class="nav-number">6.1.</span> <span class="nav-text">数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%BB%B4%E5%BA%A6"><span class="nav-number">6.2.</span> <span class="nav-text">特征维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E6%95%B4%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.3.</span> <span class="nav-text">调整模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-number">6.4.</span> <span class="nav-text">性能评估指标</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91"><span class="nav-number">7.</span> <span class="nav-text">踩过的坑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-%E4%BB%BB%E5%8A%A1%E4%BD%BF%E7%94%A8-UGI-%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%AE%9E%E9%99%85-task-%E5%9C%A8%E7%94%9F%E6%88%90%E4%B8%AD%E9%97%B4%E6%96%87%E4%BB%B6%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E6%B2%A1%E6%9C%89%E6%84%9F%E7%9F%A5%E5%88%B0%E5%A4%96%E5%B1%82%E8%AE%BE%E7%BD%AE%E7%9A%84-UGI-%E4%BF%A1%E6%81%AF"><span class="nav-number">7.1.</span> <span class="nav-text">spark 任务使用 UGI 之后，实际 task 在生成中间文件的时候，没有感知到外层设置的 UGI 信息</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0"><span class="nav-number">7.1.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">7.1.2.</span> <span class="nav-text">分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3"><span class="nav-number">7.1.3.</span> <span class="nav-text">解决</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#saveAsTextFile-%E4%B8%8D%E4%BC%A0%E5%85%A5%E5%8E%8B%E7%BC%A9%E7%B1%BB%E5%9E%8B%EF%BC%8C%E4%B9%9F%E5%AF%B9%E8%BE%93%E5%87%BA%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%8E%8B%E7%BC%A9"><span class="nav-number">7.2.</span> <span class="nav-text">saveAsTextFile 不传入压缩类型，也对输出文件进行了压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-1"><span class="nav-number">7.2.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-1"><span class="nav-number">7.2.2.</span> <span class="nav-text">解决</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85"><span class="nav-number">7.2.3.</span> <span class="nav-text">补充</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E6%B3%95%E5%88%A4%E6%96%AD-spark-session-%E4%B8%AD%E4%B8%B4%E6%97%B6%E5%87%BD%E6%95%B0%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8"><span class="nav-number">7.3.</span> <span class="nav-text">无法判断 spark session 中临时函数是否存在</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-2"><span class="nav-number">7.3.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-2"><span class="nav-number">7.3.2.</span> <span class="nav-text">解决</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85-1"><span class="nav-number">7.3.3.</span> <span class="nav-text">补充</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6-spark-%E7%BB%93%E6%9E%9C%E6%96%87%E4%BB%B6%E6%95%B0%E9%87%8F%EF%BC%8C%E5%B9%B6%E8%BF%94%E5%9B%9E%E5%94%AF%E4%B8%80%E7%9A%84%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84"><span class="nav-number">7.4.</span> <span class="nav-text">控制 spark 结果文件数量，并返回唯一的文件路径</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-3"><span class="nav-number">7.4.1.</span> <span class="nav-text">解决</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KafkaUtils-createDirectStream-%E6%8A%A5%E9%94%99-OffsetOutOfRangeException"><span class="nav-number">7.5.</span> <span class="nav-text">KafkaUtils.createDirectStream 报错 OffsetOutOfRangeException</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0-3"><span class="nav-number">7.5.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E6%9E%90-1"><span class="nav-number">7.5.2.</span> <span class="nav-text">分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3-4"><span class="nav-number">7.5.3.</span> <span class="nav-text">解决</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E7%9F%A5%E8%AF%86%E6%A0%91"><span class="nav-number">8.</span> <span class="nav-text">整体知识树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%96%99"><span class="nav-number">9.</span> <span class="nav-text">资料</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Blog"><span class="nav-number">9.1.</span> <span class="nav-text">Blog</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Book"><span class="nav-number">9.2.</span> <span class="nav-text">Book</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Github"><span class="nav-number">9.3.</span> <span class="nav-text">Github</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A2%E8%BF%8E%E5%8A%A0%E5%85%A5%E6%88%91%E4%BB%AC%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BE%A4%EF%BC%8C%E4%B8%80%E8%B5%B7%E4%BA%A4%E6%B5%81%E5%AD%A6%E4%B9%A0"><span class="nav-number">10.</span> <span class="nav-text">欢迎加入我们的技术群，一起交流学习</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Benedict Jin" src="/yuzhouwan_logo_128x128.ico">
  <p class="site-author-name" itemprop="name">Benedict Jin</p>
  <div class="site-description" itemprop="description">Benedict Jin's Blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">156</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/asdf2014" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;asdf2014" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:asdf2014@apache.org" title="E-Mail → mailto:asdf2014@apache.org" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-nd.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="external nofollow noopener noreferrer" target="_blank">苏 ICP 备 17032505号 </a>
  </div>

<div class="copyright">
  
  &copy; 2014 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yuzhouwan.com</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20:14</span>
</div>

        
<meta name="referrer" content="always">
<div class="busuanzi-count">
  <script async src="/lib/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: '32px', // default: '32px'
  left: 'unset', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: false // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<style>
button.darkmode-toggle {
  z-index: 9999;
}
img, .darkmode-ignore {
  isolation: isolate;
  display: block;
}
</style>

  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

  

<script>
  var disqus_config = function() {
    this.page.url = "https://yuzhouwan.com/posts/4735/";
    this.page.identifier = "posts/4735/";
    this.page.title = "Real-time ML with Spark";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://yuzhouwan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e5da61069b94deb8ef3a',
      clientSecret: 'c60ad4e430be258b705027a036eeee3d71cb934b',
      repo        : 'gitment',
      owner       : 'asdf2014',
      admin       : ['asdf2014'],
      id          : 'e04ed372e504e37b0c73f1a1895d7d58',
        language: '',
      distractionFreeMode: false
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'BU4bnWsdAliFfk3fz7iMcFUU-gzGzoHsz',
      appKey     : '1xVLNFSy1qGCda3wt4GiGzHG',
      placeholder: "上述信息都不是必填的，可以直接提交评论",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
