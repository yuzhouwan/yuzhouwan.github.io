<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/yuzhouwan_logo_with_copyright.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/yuzhouwan_logo_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/yuzhouwan_logo_16x16.ico">
  <link rel="mask-icon" href="/yuzhouwan_logo_with_copyright.jpg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yuzhouwan.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":5,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":{"valine":{"text":"Valine：匿名","order":-1},"disqus":{"text":"Disqus：海外","order":-2},"gitalk":{"text":"Gitalk：可用 Github 账号登录","order":-3}}},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="什么是人工智能　人工智能（Artificial Intelligence, AI）亦称机器智能，是指由人工制造出来的系统所表现出来的智能。 — wikipedia.org 　从 深蓝到 AlphaZero，人工智能的智力水平、普适性、学习能力 正在以爆炸式地速度快速发展；　从 棋类到 医学，人工智能开始在各类应用领域，都在大展身手；　从 CPU &#x2F; GPU 到 TPU，人工智能的计算能力正向着无法">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能">
<meta property="og:url" content="https://yuzhouwan.com/posts/42737/">
<meta property="og:site_name" content="宇宙湾">
<meta property="og:description" content="什么是人工智能　人工智能（Artificial Intelligence, AI）亦称机器智能，是指由人工制造出来的系统所表现出来的智能。 — wikipedia.org 　从 深蓝到 AlphaZero，人工智能的智力水平、普适性、学习能力 正在以爆炸式地速度快速发展；　从 棋类到 医学，人工智能开始在各类应用领域，都在大展身手；　从 CPU &#x2F; GPU 到 TPU，人工智能的计算能力正向着无法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_cause_rule_effect.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_boxplot_vs_pdf.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_standard_deviation_diagram.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_1_of_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_sqrt_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_x_2.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_sin_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_cos_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_math_tan_x.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_sigmoid.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_tanh.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_relu.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_relu_prelu_rrelu.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_noisy_moons.gif">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_beale_function.gif">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_long_valley.gif">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_saddle_point.gif">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_max_pooling.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_linear_regression.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_tensorboard_scalars.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_tensorboard_graph.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_tensorboard_embeddings.png">
<meta property="og:image" content="https://yuzhouwan.com/picture/ai/ai_four.png">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg">
<meta property="og:image" content="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg">
<meta property="article:published_time" content="2017-05-16T10:58:02.000Z">
<meta property="article:modified_time" content="2021-07-18T05:44:29.572Z">
<meta property="article:author" content="Benedict Jin">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="TensorFlow">
<meta property="article:tag" content="微积分">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="MP Model">
<meta property="article:tag" content="Perceptron">
<meta property="article:tag" content="MLP">
<meta property="article:tag" content="DNN">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="RNN">
<meta property="article:tag" content="LSTM">
<meta property="article:tag" content="统计学">
<meta property="article:tag" content="线性代数">
<meta property="article:tag" content="概率论">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Caffe">
<meta property="article:tag" content="xgboost">
<meta property="article:tag" content="Apache PredictionIO">
<meta property="article:tag" content="NuPIC">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yuzhouwan.com/picture/ai/ai_cause_rule_effect.png">

<link rel="canonical" href="https://yuzhouwan.com/posts/42737/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>人工智能 | 宇宙湾</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-97020848-1"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-97020848-1');
      }
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.null { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .null > span { position: relative; z-index: 10; }  .null img, .null .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .null img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .null-fallback { color: inherit; } .null-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="宇宙湾" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">宇宙湾</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">厚积薄发</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">163</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">55</span></a>

  </li>
        <li class="menu-item menu-item-书单">

    <a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>书单</a>

  </li>
        <li class="menu-item menu-item-影视">

    <a href="/movies/" rel="section"><i class="fa fa-film fa-fw"></i>影视</a>

  </li>
        <li class="menu-item menu-item-友链">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yuzhouwan.com/posts/42737/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/yuzhouwan_logo_128x128.ico">
      <meta itemprop="name" content="Benedict Jin">
      <meta itemprop="description" content="Benedict Jin's Blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="宇宙湾">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          人工智能
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-05-16 18:58:02" itemprop="dateCreated datePublished" datetime="2017-05-16T18:58:02+08:00">2017-05-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-18 13:44:29" itemprop="dateModified" datetime="2021-07-18T13:44:29+08:00">2021-07-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>42k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>39 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="什么是人工智能"><a href="#什么是人工智能" class="headerlink" title="什么是人工智能"></a>什么是人工智能</h2><p>　<strong>人工智能</strong>（<strong>A</strong>rtificial <strong>I</strong>ntelligence, <strong>AI</strong>）亦称<strong>机器智能</strong>，是指由人工制造出来的系统所表现出来的智能。 — <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">wikipedia.org</a></p>
<p>　从 深蓝到 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/pdf/1712.01815.pdf">AlphaZero</a>，人工智能的智力水平、普适性、学习能力 正在以爆炸式地速度快速发展；<br>　从 棋类到 医学，人工智能开始在各类应用领域，都在大展身手；<br>　从 CPU / GPU 到 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu">TPU</a>，人工智能的计算能力正向着无法穷举的极限不断逼近 …</p>
<p>　但是，我们并不浮躁，踏踏实实地点亮 AI 知识树的每个枝叶，才是我们每位富有科学精神的人所应该做的</p>
<h2 id="关于本文"><a href="#关于本文" class="headerlink" title="关于本文"></a>关于本文</h2><p>　我们将分为三块对 AI 进行诠释</p>
<p>　首先，将介绍人工智能的<strong>主流思想</strong>和<strong>实用技巧</strong>，通过一些耳熟能详的<a href="https://yuzhouwan.com/posts/4534/">有趣定理</a>，我们可以对人工智能有些直观、初步的认识；随后，言归正传，我们将开始接触 AI 领域的几大<strong>理论支柱</strong>，由浅入深地学习 <a href="https://yuzhouwan.com/posts/42737/#统计学">统计学</a>、<a href="https://yuzhouwan.com/posts/42737/#微积分">微积分</a>、<a href="https://yuzhouwan.com/posts/42737/#线性代数">线性代数</a>、<a href="https://yuzhouwan.com/posts/42737/#概率论">概率论</a> 等知识体系；最后，落地到实践，我们需要紧跟人工智能的<strong>技术发展前沿</strong>，对重大的突破性项目进行了解、学习，以及运用。如此，对人工智能领域进行横向分层，可以很方便地找到我们学习的突破点</p>
<p>　不过，出于文章编排的考虑，可能部分编码就要放在其他博文中了，如有不便，还望见谅（<a href="https://yuzhouwan.com/posts/43687/#Python-第三方库">Python</a>、Prolog、R、<a href="https://yuzhouwan.com/posts/27328/">Java</a>）。本文持续更新中，若有不妥之处，还请不吝赐教哈 (^o^)/</p>
<h2 id="主流思想"><a href="#主流思想" class="headerlink" title="主流思想"></a>主流思想</h2><h3 id="演绎法-amp-溯因法-amp-归纳法"><a href="#演绎法-amp-溯因法-amp-归纳法" class="headerlink" title="演绎法 &amp; 溯因法 &amp; 归纳法"></a>演绎法 &amp; 溯因法 &amp; 归纳法</h3><p><img data-src="/picture/ai/ai_cause_rule_effect.png" alt></p>
<center>（利用 <a href="https://www.axure.com.cn/" target="_blank" rel="external nofollow noopener noreferrer">Axure</a>™ 绘制而成）</center>



<h2 id="实用技巧"><a href="#实用技巧" class="headerlink" title="实用技巧"></a>实用技巧</h2><h3 id="Occam-剃刀原理"><a href="#Occam-剃刀原理" class="headerlink" title="Occam 剃刀原理"></a>Occam 剃刀原理</h3><p>　<strong>奥卡姆剃刀</strong>（Occam´s Razor），意为<strong>简约之法</strong>，是由 14 世纪<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%80%BB%E8%BE%91%E5%AD%A6">逻辑学</a>家、<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%81%96%E6%96%B9%E6%BF%9F%E5%90%84%E6%9C%83">圣方济各会</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E4%BF%AE%E5%A3%AB">修士</a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E7%9A%84%E5%A8%81%E5%BB%89">奥卡姆的威廉</a>提出的一个解决问题的法则，即<code>"切勿浪费较多资源，去做'用较少的资源，同样可以做好'的事情"</code>，相同思想见于郑板桥的<strong>删繁就简三秋树</strong></p>
<span id="more"></span>
<p>　在机器学习中的解释，就是 <code>在所有可能选择的模型中，能够很好地解释已知数据且十分简单的才是最好的模型</code>。并由此引入了<strong><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/20924039">正则化</a></strong>的理念。<a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.cnblogs.com/jianxinzhou/p/4083921.html">正则化</a>方法的思想是，处理最优化函数问题时，在目标函数中加入对参数的<strong>约束惩罚项</strong>，从而达到<strong>简化模型</strong>的目的。其中，<a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/zouxy09/article/details/24971995">L0，L1 和 L2</a> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0">范数</a> 指的就是三种不同惩罚函数的形式，用数学公式来表达就是 $L_p$ = $\mid\mid\beta\mid\mid_p$ = $(\mid \beta_1 \mid ^p + \mid \beta_2 \mid^p$ $+ \ldots +$ $\mid\,\beta_n \mid^p)^{\frac1p}$</p>
<ul>
<li><strong>L0 范数</strong> 是指向量中非 0 元素的个数（如果我们用 L0 范数来规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素都是 0，即让参数矩阵 W 是稀疏的）</li>
<li><strong>L1 范数</strong> 是指向量中各系数绝对值之和（又称为 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/zouxy09/article/details/24971995">LASSO</a>）</li>
<li><strong>L2 范数</strong> 是指向量中各系数平方和的平方根（在计量经济学里面，它又被称为 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.ics.uci.edu/~welling/teaching/KernelsICS273B/Kernel-Ridge.pdf">岭回归</a>）</li>
</ul>
<h3 id="大数定律"><a href="#大数定律" class="headerlink" title="大数定律"></a>大数定律</h3><p>　<strong>大数定律</strong>又称大数法则、大数律，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其平均就越趋近<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%9C%9F%E6%9C%9B%E5%80%BC">期望值</a></p>
<p>　在概率论中，细分为<strong>弱大数定律</strong>和<strong>强大数定律</strong></p>
<h4 id="前要定义"><a href="#前要定义" class="headerlink" title="前要定义"></a>前要定义</h4><p>　独立同分布的随机变量序列：$X_1, X_2, \dots, X_n$</p>
<p>　样本均值：$M_n = \frac{ \sum_{i=1}^nX_i }n$</p>
<p>　当样本量很大的时候，从 $X$ 抽取的样本平均值：$E[X]$（可得，$E[M_n] = \mu$）</p>
<p>　设在某一试验中，$A$ 是一个事件，满足条件 $P(A) &gt; 0$，又设 $X$ 和 $Y$ 是在同一个试验中的两个随机变量。若 $X$ 和 $Y$ 相互独立，则 $var(X + Y) =$ $var(X) + var(Y)$（可得，$var(M_n) = \frac{ \sigma^2 }n$）</p>
<h4 id="弱大数定律"><a href="#弱大数定律" class="headerlink" title="弱大数定律"></a>弱大数定律</h4><p>　设 $X_1, X_2, \dots, X_n$ 独立同分布，其公共分布的均值为 $\mu$，则对任意的 $\epsilon \gt 0$，当 $n \to +\infty$ 时，$P(\mid M_n - \mu \mid \ge \epsilon) = $ $P(\mid \frac{ \sum_{i=1}^n{X_i} }n - \mu\,\mid$ $ \ge \epsilon) \to 0$</p>
<p>　<strong>弱大数定律</strong>出给的结论是，$M_n$ 落在 $[\mu - \epsilon, \mu + \epsilon]$ 区间（即 $\mu$ 的 $\epsilon$ 邻域）内的概率会非常大（也可以表述为 “$M_n$ 收敛于 $\mu$”）。同时可以看出，随着 $n$ 值的增大，$M_n$ 落在 $\mu$ 邻域内的概率也会变大</p>
<h4 id="强大数定律"><a href="#强大数定律" class="headerlink" title="强大数定律"></a>强大数定律</h4><p>　设 $X_1, X_2, \dots, X_n$ 是均值为 $\mu$ 的独立同分布随机变量序列，则样本均值 $M_n$ 以概率 $1$ 收敛于 $\mu$，即 $P(\lim_{n \to +\infty}M_n = \mu) = 1$</p>
<p>　在无穷序列中，弱大数定律无法给出到底存在多少元素显著性偏离了 $\mu$。而利用<strong>强大数定律</strong>则可得出，$M_n$ 以概率 $1$（即几乎处处）收敛于 $\mu$ 的结论，意味着对于任何 $\epsilon \gt 0$，偏离 $\mid M_n - \mu\mid$ 超过 $\epsilon$ 的元素只会存在有限多个</p>
<h2 id="统计学"><a href="#统计学" class="headerlink" title="统计学"></a>统计学</h2><h3 id="什么是统计学"><a href="#什么是统计学" class="headerlink" title="什么是统计学"></a>什么是统计学</h3><p>　统计学（Statistics）是一套用以<strong>收集数据</strong>、<strong>分析数据</strong>和<strong>由数据得出结论</strong>的概念、原则和方法。  — <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.swarthmore.edu/SocSci/cwareha1/iversen.html">Gudmund R.Iversen</a></p>
<h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><h4 id="随机性-amp-规律性"><a href="#随机性-amp-规律性" class="headerlink" title="随机性 &amp; 规律性"></a>随机性 &amp; 规律性</h4><p>　随机性，指不能预测某一特定事件的结果<br>　规律性，指从大量的收集数据中发现的模式</p>
<p>　可以说，统计就是在<strong>随机性</strong>中寻找<strong>规律性</strong></p>
<h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><h4 id="数据分布中心"><a href="#数据分布中心" class="headerlink" title="数据分布中心"></a>数据分布中心</h4><h5 id="均值（Mean）"><a href="#均值（Mean）" class="headerlink" title="均值（Mean）"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zybuluo.com/spiritnotes/note/297176">均值</a>（Mean）</h5><p>　算术平均值：$\mu = \frac{ \Sigma X }N$ 或 $\bar x = \frac{ \sum_{i=1}^n X_i }n$</p>
<h5 id="中位数（Median）"><a href="#中位数（Median）" class="headerlink" title="中位数（Median）"></a>中位数（Median）</h5><p>　高偏斜分布（存在异常值）的情况下，中位数则能更好地反映数据的集中趋势</p>
<h5 id="众数（Mode）"><a href="#众数（Mode）" class="headerlink" title="众数（Mode）"></a>众数（Mode）</h5><p>　平均分布的情况下，则无法得出众数</p>
<p>　另外，在正太分布的数据集中，均值、中位数、众数均相等</p>
<h4 id="稳健统计"><a href="#稳健统计" class="headerlink" title="稳健统计"></a>稳健统计</h4><h5 id="值域（Range）"><a href="#值域（Range）" class="headerlink" title="值域（Range）"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/zh-hans/%E5%80%BC%E5%9F%9F">值域</a>（Range）</h5><p>　函数的<strong>值域</strong>（Range）是由定义域中一切<strong>元素</strong>所能产生的所有函数值的<strong>集合</strong>（也被称为<strong>函数的像</strong>）</p>
<h5 id="四分位数（Quartile）"><a href="#四分位数（Quartile）" class="headerlink" title="四分位数（Quartile）"></a>四分位数（Quartile）</h5><p>　<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6">统计学</a>中<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%88%86%E4%BD%8D%E6%95%B0">分位数</a>的一种，即把所有数值由小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数</p>
<p>　数学表达式：$L_p = n \frac{ p }{100}$，其中，$p$ 为四分位的百分比值，$n$ 为样本总量</p>
<h5 id="四分位距（IQR-Interquartile-Range）"><a href="#四分位距（IQR-Interquartile-Range）" class="headerlink" title="四分位距（IQR, Interquartile Range）"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%9B%9B%E5%88%86%E4%BD%8D%E8%B7%9D">四分位距</a>（IQR, <strong>I</strong>nter<strong>q</strong>uartile <strong>R</strong>ange）</h5><p>　第三<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%9B%9B%E5%88%86%E4%BD%8D%E6%95%B0">四分位数</a>和第一四分位数的差距，即 $Q_3 - Q_1$</p>
<p>　可以使用 $Q_1-1.5 \cdot IQR$ 和 $Q_3+1.5 \cdot IQR$ 区间，来判断离群值（Outlier）</p>
<p><img data-src="/picture/ai/ai_boxplot_vs_pdf.png" alt></p>
<center>（图片来源：<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>™，已确认版权为 CC BY 2.5 协议）</center>




<h5 id="四分位差（QD-Quartile-Deviation）"><a href="#四分位差（QD-Quartile-Deviation）" class="headerlink" title="四分位差（QD, Quartile Deviation）"></a>四分位差（QD, <strong>Q</strong>uartile <strong>D</strong>eviation）</h5><p>　四分位差是指 $Q_1$ 和 $Q_3$ 的差距，即 $QD = Q_3 - Q_1$（这里单独提出来，是因为旧版教材中的公式为 $QD = \frac{ Q_3 - Q_1 }2$）</p>
<h4 id="偏差"><a href="#偏差" class="headerlink" title="偏差"></a>偏差</h4><h5 id="离均差（Deviation-from-Average）"><a href="#离均差（Deviation-from-Average）" class="headerlink" title="离均差（Deviation from Average）"></a>离均差（Deviation from Average）</h5><p>　$x_i - \bar x$</p>
<h5 id="平均偏差（Average-Deviation）"><a href="#平均偏差（Average-Deviation）" class="headerlink" title="平均偏差（Average Deviation）"></a>平均偏差（Average Deviation）</h5><p>　$\frac{ \sum_{i = 1}^n (x_i - \bar x) }n$</p>
<h5 id="标准偏差（Standard-Deviation）"><a href="#标准偏差（Standard-Deviation）" class="headerlink" title="标准偏差（Standard Deviation）"></a>标准偏差（Standard Deviation）</h5><p>　标准差的概念，由<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8D%A1%E5%B0%94%C2%B7%E7%9A%AE%E5%B0%94%E9%80%8A">卡尔·皮尔逊</a>引入到概率统计中，是测量<strong>离散程度</strong>最为常用的方法。公式是 $\sigma = \sqrt{\frac{ \sum_{i=1}^n(x_i - \bar x)^2 }n}$ = $\sqrt{\frac{ (x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \ldots + (x_i - \bar x)^2 }n}$。该方法避免了负值的出现，并且能够保证得到的结果，具备和被测量数据一样的度量单位，方便进行比对</p>
<p>　标准差还可以用于判断数据集是否属于<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%B8%B8%E6%85%8B%E5%88%86%E4%BD%88">正态分布</a>。若其符合正态概率分布，则如下图所示，约<strong>68.3%</strong> 的数值会分布在距离 平均值$\bar x$ 的 1个 标准差$\sigma$ 范围之内，约<strong>95.4%</strong> 的数值分布在距离 平均值$\bar x$ 的 2个 标准差$\sigma$ 范围之内，以及 约<strong>99.73%</strong> 的数值分布在距离 平均值$\bar x$ 的 3个 标准差$\sigma$ 范围之内。这一现象我们称之为 “<strong>68-95-99.7法则</strong>” 或 “<strong>经验法则</strong>”</p>
<p><img data-src="/picture/ai/ai_standard_deviation_diagram.png" alt></p>
<center>（图片来源：<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>™，已确认无版权）</center>

<h5 id="贝塞尔校正"><a href="#贝塞尔校正" class="headerlink" title="贝塞尔校正"></a>贝塞尔校正</h5><p>　又因为样本抽选的数据，很容易落入平均值附近，导致低估了整体数据集的偏差。因此引入了贝塞尔校正，使得标准差的值变大，来更好地表达<strong>样本标准差</strong>（<strong>S</strong>ample <strong>S</strong>tandard <strong>D</strong>eviation）</p>
<p>　$s = \sqrt {\frac{ \sum_{i=1}^n(x_i - \bar x)^2 }{n - 1}}$</p>
<h3 id="回归分析"><a href="#回归分析" class="headerlink" title="回归分析"></a>回归分析</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>　<strong><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/zh-hans/迴歸分析">回归分析</a></strong>（<strong>R</strong>egression <strong>A</strong>nalysis）是一种确定两种或多个变量间相互依赖的定量关系的统计分析方法</p>
<h4 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h4><p>　包括未知参数 $\beta$（表示一个标量或者向量）、自变量 $X$ 和 因变量 $Y$<br>　回归模型则是将三者关联起来，表示为 $Y \approx f(X, \beta)$</p>
<h4 id="种类"><a href="#种类" class="headerlink" title="种类"></a>种类</h4><h5 id="分类方式"><a href="#分类方式" class="headerlink" title="分类方式"></a>分类方式</h5><p>　按照涉及的变量的多少，分为<strong>一元</strong>回归和<strong>多元</strong>回归分析<br>　按照因变量的多少，分为<strong>简单</strong>回归分析和<strong>多重</strong>回归分析<br>　按照自变量和因变量之间的关系类型，分为<strong>线性</strong>回归分析和<strong>非线性</strong>回归分析</p>
<h5 id="常见类型"><a href="#常见类型" class="headerlink" title="常见类型"></a>常见类型</h5><h6 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h6><p>　如果在回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为<strong>一元线性回归分析</strong></p>
<h6 id="多重线性回归"><a href="#多重线性回归" class="headerlink" title="多重线性回归"></a>多重线性回归</h6><p>　如果回归分析中包括两个或两个以上的自变量，且自变量之间存在线性相关，则称为<strong>多重线性回归分析</strong></p>
<h6 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h6><p>　<strong>多元线性回归</strong>可表示为 $Y = a + b_1 X_1 + b_2 X_2 + e$，其中 $a$ 表示截距，$b$ 表示直线的斜率，$e$ 是误差项。多元线性回归可以根据给定的预测变量 $s$ 来预测目标变量的值</p>
<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="增量"><a href="#增量" class="headerlink" title="增量"></a>增量</h4><p>　点 $(x_1, y_1)$ 移动到 点 $(x_2, y_2)$，其坐标的<strong>增量</strong>为 $\Delta x = x_2 - x_1$ 和 $\Delta y = y_2 - y_1$</p>
<h4 id="平行线-amp-垂直线"><a href="#平行线-amp-垂直线" class="headerlink" title="平行线 &amp; 垂直线"></a>平行线 &amp; 垂直线</h4><p>　斜率 $m$ = $\frac{ \Delta y }{ \Delta x }$，直线 $L_1$ 和 $L_2$ 的斜率分别记作 $m_1$ 和 $m_2$<br>　如果 $m_1 = m_2$，则 $L_1$ 与 $L_2$ <strong>平行</strong>或重合，记作 $L_1 \mid \mid L_2$；如果 $m_1m_2 = -1$，则两者互相<strong>垂直</strong>，记作 $L_1$ ⊥ $L_2$</p>
<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>　<strong>函数</strong>是将一个对象转化为另一个对象的规则 $f$。函数必须要给每个有效输入指定唯一的输出。这里我们输入的集合 $x$ 称之为<strong>定义域</strong>、输出的集合 $f(x)$ 称之为<strong>值域</strong></p>
<h5 id="常用集合"><a href="#常用集合" class="headerlink" title="常用集合"></a>常用集合</h5><p>　$Z$ 整数集，$N$ 非负数集，$Q$ 有理数集</p>
<h5 id="常用函数的定义域和值域"><a href="#常用函数的定义域和值域" class="headerlink" title="常用函数的定义域和值域"></a>常用函数的定义域和值域</h5><div class="table-container">
<table>
<thead>
<tr>
<th>函数</th>
<th>定义域</th>
<th>值域</th>
<th>图形</th>
</tr>
</thead>
<tbody>
<tr>
<td>$y = \frac{ 1 }{ x }$</td>
<td>$(-\infty, +\infty)$</td>
<td>$(-\infty, 0) \,\bigcup\, (0, +\infty)$</td>
<td><img data-src="/picture/ai/ai_math_1_of_x.png" alt></td>
</tr>
<tr>
<td>$y = \sqrt{x}$</td>
<td>$[0, +\infty)$</td>
<td>$[0, +\infty)$</td>
<td><img data-src="/picture/ai/ai_math_sqrt_x.png" alt></td>
</tr>
<tr>
<td>$y = x$</td>
<td>$(-\infty, +\infty)$</td>
<td>$(-\infty, +\infty)$</td>
<td><img data-src="/picture/ai/ai_math_x.png" alt></td>
</tr>
<tr>
<td>$y = x^2$</td>
<td>$(-\infty, +\infty)$</td>
<td>$[0, +\infty)$</td>
<td><img data-src="/picture/ai/ai_math_x_2.png" alt></td>
</tr>
<tr>
<td>$y = \sin(x)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$[-1, 1]$</td>
<td><img data-src="/picture/ai/ai_math_sin_x.png" alt></td>
</tr>
<tr>
<td>$y = \cos(x)$</td>
<td>$(-\infty, +\infty)$</td>
<td>$[-1, 1]$</td>
<td><img data-src="/picture/ai/ai_math_cos_x.png" alt></td>
</tr>
<tr>
<td>$y = \tan(x)$</td>
<td>$(-\frac{ \pi }{ 2 } + 2k\pi,$ $\frac{ \pi }{ 2 } + 2k\pi)$, $k \in Z$</td>
<td>$(-\infty, +\infty)$</td>
<td><img data-src="/picture/ai/ai_math_tan_x.png" alt></td>
</tr>
</tbody>
</table>
</div>
<center>（使用 <a href="https://www.intmath.com/functions-and-graphs/graphs-using-svg.php" target="_blank" rel="external nofollow noopener noreferrer">intmath.com</a> 绘制而成）</center>




<h4 id="反函数"><a href="#反函数" class="headerlink" title="反函数"></a>反函数</h4><p>　对于 $y = f(x)$，如果将其逆转变换后，仍然满足 $f(x) = y$，则称新函数为 $f$ 的<strong>反函数</strong>，并记作 $f^{-1}$</p>
<h4 id="函数复合"><a href="#函数复合" class="headerlink" title="函数复合"></a>函数复合</h4><p>　$f(x) = h(g(x)) = h \circ g$</p>
<h4 id="奇偶性"><a href="#奇偶性" class="headerlink" title="奇偶性"></a>奇偶性</h4><h5 id="奇函数"><a href="#奇函数" class="headerlink" title="奇函数"></a>奇函数</h5><p>　$f(x) = -f(-x)$</p>
<h5 id="偶函数"><a href="#偶函数" class="headerlink" title="偶函数"></a>偶函数</h5><p>　$f(x) = f(-x)$</p>
<h4 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h4><p>　$f(x) = mx + b$</p>
<h5 id="点斜式"><a href="#点斜式" class="headerlink" title="点斜式"></a>点斜式</h5><p>　直线通过一个点 $(x_0, y_0)$，斜率为 $m$，则方程可表示为 $y - y_0 = m (x - x_0)$<br>　直线通过两个点 $(x_1, y_1)$ 和 $(x_2, y_2)$，则斜率为 $m$ = $\frac{ y_2 - y_1 }{ x_2 - x_1 }$ = $\frac{ \Delta y }{ \Delta x }$，则方程式表示为 $y - y_1$= $\frac{ \Delta y }{ \Delta x }$ $(x - x_1)$<br>　已知斜率 $m$ 和截距 $b$，可以直接求得直线方程式 $y = m(x - 0) + b = mx + b$</p>
<h4 id="多项式"><a href="#多项式" class="headerlink" title="多项式"></a>多项式</h4><p>　$p(x)$ = $a_nx^n + \ldots + a_2x^2 + a_1x + a_0$</p>
<h4 id="二次函数"><a href="#二次函数" class="headerlink" title="二次函数"></a>二次函数</h4><p>　<strong>二次函数</strong>是一个最高次为 $2$ 的多项式，表示为 $p(x) = ax^2 + bx + c$</p>
<p>　依据 $\Delta = b^2 - 4ac$ 判别式，可知：</p>
<ul>
<li>当 $\Delta \gt 0$ 时，$p(x)$ 有<strong>两个不同解</strong>，且解为 $\frac{ -b \pm \sqrt{b^2 - 4ac} }{ 2a }$；</li>
<li>当 $\Delta = 0$ 时，$p(x)$ 有两个相同的解，即<strong>只有一个解</strong>；</li>
<li>当 $\Delta \lt 0$ 时，$p(x)$ <strong>无解</strong></li>
</ul>
<h5 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h5><h6 id="求解-3x-2-5x-7-0"><a href="#求解-3x-2-5x-7-0" class="headerlink" title="求解 $3x^2 -5x + 7 = 0$"></a>求解 $3x^2 -5x + 7 = 0$</h6><p>　因为 $\Delta = (-5)^2 - 4 \cdot 3 \cdot 7$ $= -59 \lt 0$，所有该方程无解</p>
<p>　可以将方程拆解组合，进行证明：</p>
<script type="math/tex; mode=display">
3(x^2 - \frac{ 5 }3x + \frac{ 7 }3) = 0</script><script type="math/tex; mode=display">
x^2 - \frac{ 5 }3x + \frac{ 7 }3 = 0</script><script type="math/tex; mode=display">
x^2 - \frac{ 5 }3x + \frac{ 25 }{ 36 } + \frac{ 7 }3 - \frac{ 25 }{ 36 } = 0</script><script type="math/tex; mode=display">
(x - \frac{ 5 }6)^2 + \frac{ 59 }{ 36 } = 0</script><script type="math/tex; mode=display">
(x - \frac{ 5 }6)^2 = - \frac{ 59 }{ 36 }</script><p>　可见等式左边是恒为非负数的，因此，是不可能存在 $x$ 使等式成立的，即方程无解</p>
<h4 id="有理函数"><a href="#有理函数" class="headerlink" title="有理函数"></a>有理函数</h4><p>　若 $p$ 和 $q$ 均为多项式，则形如 $\frac{ p(x) }{ q(x) }$ 的函数，我们称之为 <strong>有理函数</strong></p>
<h4 id="指数函数"><a href="#指数函数" class="headerlink" title="指数函数"></a>指数函数</h4><p>　$y = a^x$</p>
<h4 id="对数函数"><a href="#对数函数" class="headerlink" title="对数函数"></a>对数函数</h4><p>　将 指数函数 以 $y = x$ 做镜像，则得到 $y = \log_a(x)$ 指数函数</p>
<h4 id="带绝对值的函数"><a href="#带绝对值的函数" class="headerlink" title="带绝对值的函数"></a>带绝对值的函数</h4><script type="math/tex; mode=display">
f(|x|) = 
\begin{cases}
f(x), x \geq 0 \\
f(-x), x \lt 0
\end{cases}</script><h3 id="求导"><a href="#求导" class="headerlink" title="求导"></a>求导</h3><h4 id="分数求导"><a href="#分数求导" class="headerlink" title="分数求导"></a>分数求导</h4><p>　$f(x) = \frac{ g(x) }{ h(x) }$ 的导数为 $f^\prime(x) = \frac{ g^\prime(x)h(x)-h^\prime(x)g(x) }{ h(x)^2 }$</p>
<h4 id="复合函数求导"><a href="#复合函数求导" class="headerlink" title="复合函数求导"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99">复合函数</a>求导</h4><p>　$(f \circ g)(x)$ 的导数为 $(f \circ g)^\prime(x) = f^\prime(g(x))g^\prime(x)$</p>
<h3 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h3><h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><h5 id="邻域"><a href="#邻域" class="headerlink" title="邻域"></a>邻域</h5><p>　以点 $x_0$ 为中心的任何开区间，称之为点 $x_0$ 的邻域，记做 $U(x_0)$</p>
<ul>
<li><p>$\delta$ 邻域</p>
<p>设 $\delta$ 为正数，则称开区间 $(x_0 - \delta, \, x_0 + \delta)$ 为点 $x_0$ 的 $\delta$ 邻域，记做 $U(x_0, \, \delta)$ = $\{x \mid x_0 - \delta \lt x \lt x_0 + \delta\}$。其中，点 $x_0$ 称为邻域的中心，$\delta$ 为邻域的半径</p>
</li>
<li><p>$\delta$ 去心邻域</p>
<p>$\mathring{U}(x_0, \delta)$ = $\{ x \mid 0 \lt \mid x - x_0 \mid \lt \delta \}$</p>
</li>
</ul>
<ul>
<li><p>二维 $\delta$ 邻域</p>
<p>设 $P_0(x_0, \, y_0)$ 为 $xOy$ 平面上的一个点，$\delta$ 为某个正数，与点 $P_0(x_0, \, y_0)$ 距离小于 $\delta$ 的点 $P(x, \, y)$ 的全体，称之为点 $P_0$ 的 $\delta$ 邻域，记做 $U(P_0, \delta)$ = $\{ x \mid \sqrt{(x - x_0)^2 + (y - y_0)^2} \lt \delta \}$</p>
</li>
</ul>
<h5 id="函数极限"><a href="#函数极限" class="headerlink" title="函数极限"></a>函数极限</h5><p>　设函数 $f(x)$ 在点 $x_0$ 的某一去心邻域内有定义，若存在常数 $A$，对于任意给定的 $\epsilon \gt 0$（无论 $\epsilon$ 有多小），总存在正数 $\delta$，使得当 $0 \lt \mid x - x_0 \mid \lt \delta$ 时，对应函数的值 $f(x)$ 都满足不等式 $\mid f(x) - A \mid \lt \epsilon$，则 $A$ 就叫做函数 $f(x)$ 当 $x \to x_0$ 的极限，记做 $\lim_{x \to x_0}f(x)=A$，或 $f(x) \to A$（当 $x \to x_0$）</p>
<p>　换一种说法是，$\lim_{x \to x_0}f(x) = A \Leftrightarrow \forall \epsilon \gt 0$，$\exists \delta \gt 0$，当 $0 \lt \mid x - x_0 \mid \lt \delta$ 时，$\mid f(x) - A \mid \lt \epsilon$</p>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E5%BC%8F">行列式</a></h4><p>　<strong>行列式</strong>（<strong>D</strong>eterminant）是数学中的一个将 $n \cdot n$ 的矩阵 $A$ 映射到一个<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E7%B4%94%E9%87%8F">标量</a>的函数，记作 $det(A)$ 或 $\mid A \mid$。行列式可以看做是有向面积或体积的概念，在欧几里得空间中的推广。或者说，在 $n$ 维欧几里得空间中，行列式描述的是一个<strong>线性变换</strong>对体积所造成的影响</p>
<h4 id="单位矩阵"><a href="#单位矩阵" class="headerlink" title="单位矩阵"></a>单位矩阵</h4><p>　<strong>单位矩阵</strong> 是主对角线为 $1$，其余元素为 $0$ 的方形矩阵，记作 $I$ 或 $E$</p>
<p>$I_n = \begin{bmatrix} 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 \\ \end{bmatrix}$</p>
<h4 id="初等矩阵"><a href="#初等矩阵" class="headerlink" title="初等矩阵"></a>初等矩阵</h4><p>　<strong>初等矩阵</strong>（又称为<strong>基本矩阵</strong>）是由一个 $n$ 阶单位矩阵 $E$，经过一次<strong>初等行列变换</strong>所得的矩阵。我们称之为 $n$ 阶初等矩阵</p>
<h4 id="增广矩阵"><a href="#增广矩阵" class="headerlink" title="增广矩阵"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/zh-hans/%E5%A2%9E%E5%B9%BF%E7%9F%A9%E9%98%B5">增广矩阵</a></h4><p>　<strong>增广矩阵</strong>是由系数矩阵的右边，添上线性方程组等号右边的常数列 得到的矩阵<br>　方程 $AX=b$ 系数矩阵为 $A$，常数列为 $b$，则它的增广矩阵表示为 $(A \mid b)$，形式如下</p>
<script type="math/tex; mode=display">
(A \mid b) = 
\left[
\begin{array}{cc|c}
1 & 0 & 1 \\
0 & 1 & 1
\end{array}
\right]</script><p>　通过对增广矩阵进行初等行变换，可判断线性方程组是否有解，以及化简原方程组，方便求解</p>
<h3 id="实用技巧-1"><a href="#实用技巧-1" class="headerlink" title="实用技巧"></a>实用技巧</h3><h4 id="消元法"><a href="#消元法" class="headerlink" title="消元法"></a>消元法</h4><h4 id="高斯-约当（Gauss-Jordan）消元法"><a href="#高斯-约当（Gauss-Jordan）消元法" class="headerlink" title="高斯-约当（Gauss-Jordan）消元法"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/zh-hans/%E9%AB%98%E6%96%AF-%E8%8B%A5%E7%88%BE%E7%95%B6%E6%B6%88%E5%85%83%E6%B3%95">高斯-约当</a>（Gauss-Jordan）消元法</h4><p>　直接上一个例子，会比较直观一些</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{ccc|c}
3 & 7 & 1 & 1 \\
1 & 1 & -6 & 0 \\
2 & 6 & 7 & 1
\end{array}
\right]</script><script type="math/tex; mode=display">
\require{AMScd}\begin{CD}
@>{R_1 - R_2 - R_3, \, R_1 \longleftrightarrow R_3}>>
\end{CD}</script><script type="math/tex; mode=display">
\left[
\begin{array}{ccc|c}
2 & 6 & 7 & 1 \\
1 & 1 & -6 & 0 \\
0 & 0 & 0 & 0
\end{array}
\right]</script><script type="math/tex; mode=display">
\require{AMScd}\begin{CD}
@>{R_1 \longleftrightarrow R_2, \, R_2 - 2R_1, \, 1/4R_2}>>
\end{CD}</script><script type="math/tex; mode=display">
\left[
\begin{array}{ccc|c}
1 & 1 & -6 & 0 \\
0 & 1 & 19/4 & 1/4 \\
0 & 0 & 0 & 0
\end{array}
\right]</script><script type="math/tex; mode=display">
\require{AMScd}\begin{CD}
@>{R_1 - R_2}>>
\end{CD}</script><script type="math/tex; mode=display">
\left[
\begin{array}{ccc|c}
1 & 0 & -43/4 & -1/4 \\
0 & 1 & 19/4 & 1/4 \\
0 & 0 & 0 & 0
\end{array}
\right]</script><p>　化简到上三角形式方程组后，就可以一眼看出，当自由列取值为 $0$ 时，能得到一个特解：</p>
<script type="math/tex; mode=display">
\left[
\begin{matrix}
-1/4 \\
1/4 \\
0
\end{matrix}
\right]</script><h4 id="克拉默法则（Gramer´s-Rule）"><a href="#克拉默法则（Gramer´s-Rule）" class="headerlink" title="克拉默法则（Gramer´s Rule）"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%85%8B%E8%90%8A%E5%A7%86%E6%B3%95%E5%89%87">克拉默法则</a>（Gramer´s Rule）</h4><script type="math/tex; mode=display">
给定 \, f(x) = 
\begin{cases}
a_{11}x_1 + a_{12}x_2 = b_1 \\
a_{21}x_1 + a_{22}x_2 = b_2
\end{cases} 方程组，</script><p>　可以通过消元代入法，得到 $x_1 = \frac{ b_1a_{22} - b_2a_{12} }{a_{11}a_{22} - a_{12}a_{21}}$，$x_2 = \frac{b_2a_{11} - b_1a_{21}}{a_{11}a_{22} - a_{12}a_{21}}$，换成行列式表达式形式，则可表示为</p>
<script type="math/tex; mode=display">
x_1 = 
\frac{
  \begin{vmatrix}
  b_1 & a_{12} \\
  b_2 & a_{22}
\end{vmatrix}}
{
  \begin{vmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
  \end{vmatrix}
}，
x_2 = 
\frac{
  \begin{vmatrix}
  a_{11} & b_1 \\
  b_{21} & b_2
\end{vmatrix}}
{
  \begin{vmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
  \end{vmatrix}
}</script><h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>　将一些研究对象放在一起，形成<strong>集合</strong>，而这些对象就称为集合的元素</p>
<h3 id="概率模型"><a href="#概率模型" class="headerlink" title="概率模型"></a>概率模型</h3><p>　<strong>概率模型</strong>是对不确定现象的一种数学描述</p>
<h4 id="主要构成"><a href="#主要构成" class="headerlink" title="主要构成"></a>主要构成</h4><h5 id="样本空间"><a href="#样本空间" class="headerlink" title="样本空间"></a>样本空间</h5><p>　用 $\Omega$ 表示一个试验中所有可能结果的集合</p>
<h5 id="概率律"><a href="#概率律" class="headerlink" title="概率律"></a>概率律</h5><p>　用一个非负数 $P(A)$ 表示某一个事件 $A$ 在试验结果中出现的概率</p>
<h6 id="公理"><a href="#公理" class="headerlink" title="公理"></a>公理</h6><ul>
<li><strong>非负性</strong><br>对于一切事件 $A$，满足 $P(A) \geq 0$</li>
<li><strong>可加性</strong><br>对于 $A_1, A_2, \ldots, A_n$ 互不相交的集合，满足 $P(A_1 \bigcup A_2 \bigcup \ldots \bigcup A_n)$ = $P(A_1) + P(A_2) + \ldots + P(A_n)$</li>
<li><strong>归一化</strong><br>$P(\Omega) = 1$<br>综合推导，可得 $P(\emptyset)$ = $1- P(\Omega) + P(\emptyset)$ = $1 - P(\Omega \bigcup \emptyset)$ = $1 - P(\Omega) = 0$</li>
</ul>
<h6 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h6><ul>
<li>若 $A \subset B$，则 $P(A) \leq P(B)$</li>
<li>$P(A \bigcup B)$ = $P(A) + P(B) - P(A \bigcap B)$</li>
<li>$P(A \bigcup B)$ $\leq$ $P(A) + P(B)$</li>
<li>由上式推广，可得 $P(A_1 \bigcup A_2 \bigcup \ldots \bigcup A_n)$ $\leq \sum_{i=1}^nP(A_i)$</li>
<li>$P(A \bigcup B \bigcup C)$ = $P(A) + P(A^c \bigcap B)$ $+ P(A^c \bigcap B^c \bigcap C)$</li>
</ul>
<h2 id="技术发展史"><a href="#技术发展史" class="headerlink" title="技术发展史"></a>技术发展史</h2><h3 id="机器学习发展总图"><a href="#机器学习发展总图" class="headerlink" title="机器学习发展总图"></a>机器学习发展总图</h3><div class="note info">To be continued...</div>



<h3 id="深度学习发展总图"><a href="#深度学习发展总图" class="headerlink" title="深度学习发展总图"></a>深度学习发展总图</h3><pre class="mermaid">graph TD
start[Start] --&gt; nerve_cell(生物神经元)
nerve_cell --&gt; mp_model(MP 模型)
mp_model --&gt; weight{自主学习权重}
weight --&gt; |No| non_perceptron[参数权重需要人为设定]
weight --&gt; |Yes| perceptron(感知器)
perceptron --&gt; bp{BP 反向传播}
bp --&gt; |No| non_mlp[无法解决最简单的线性不可分问题]
bp --&gt; |Yes| mlp(多层感知器)
mlp --&gt; pre_train{预训练 + 激活函数}
pre_train --&gt; |No| no_dnn[局部最优解 + 指数梯度衰减]
pre_train --&gt; |Yes| dnn(深度神经网络)
dnn --&gt; kernal{卷积核}
kernal --&gt; |No| non_cnn[过拟合 + 运算时间成本高]
kernal --&gt; |Yes| cnn(卷积神经网络)
cnn --&gt; directed{神经元间链成有向图}
directed --&gt; |No| non_rnn[无法对事件序列建模]
directed --&gt; |Yes| rnn(递归神经网络)
rnn --&gt; cell{细胞状态}
cell --&gt; |No| non_lstm[无法捕获间隔太长的事件间关系]
cell --&gt; |Yes| lstm(长短期记忆网络)
lstm --&gt; e[End?]</pre>



<h4 id="麦卡洛克-皮茨神经元模型（McCulloch-Pitts-Neuron-Model）"><a href="#麦卡洛克-皮茨神经元模型（McCulloch-Pitts-Neuron-Model）" class="headerlink" title="麦卡洛克-皮茨神经元模型（McCulloch - Pitts Neuron Model）"></a>麦卡洛克-皮茨神经元模型（McCulloch - Pitts Neuron Model）</h4><p>　<strong>麦卡洛克-皮茨神经元模型</strong>（<strong>M</strong>cCulloch - <strong>P</strong>itts Neuron <strong>Model</strong>）是模仿生物学神经元功能的简单线性模型，由心理学家 Warren McCulloch 和 数学家 Walter Pitts 在 1943 年提出。该模型针对输入的 $x_1, x_2, \dots, x_n$，分别赋予不同的权重 $w_1, w_2, \dots, w_n$，形成 $f(x, w) = \sum_{i = 1}^nx_iw_i$ 检验函数用以模仿生物神经元的<strong>膜电位</strong>，再通过 $o_j(t + 1)$ = $f\{[\sum_{i=1}^nw_{ij}x_i(t)] - T_j\}$ 模仿在每个 $t$ 时刻<strong>神经元细胞的输出</strong>（信号空间的求和与神经元阀值 $T_j$ 差值的正负），完成分类。同时 MP 模型也存在不足之处，即权重需要人为给定，一旦权重分配不合理，将无法得出期望的分类结果</p>
<h4 id="感知器（Perceptron）"><a href="#感知器（Perceptron）" class="headerlink" title="感知器（Perceptron）"></a>感知器（<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">Perceptron</a>）</h4><p>　<strong>感知器</strong>（<strong>P</strong>erceptron）是 Frank Rosenblatt 在 1957 年就职于 Cornell 航空实验室（Cornell Aeronautical Laboratory）时所发明的一种<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">人工神经网络</a>。它可以被视为一种最简单形式的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>，是一种二元<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8">线性分类器</a>。感知器，解决了 MP 模型的无法自主学习的问题，成为第一个能根据每个类别的输入样本来学习权重的模型。此外，Frank Rosenblatt 给出了相应的感知器学习算法，常用的有 感知器学习、最小二乘法和梯度下降法。譬如，感知器利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知器模型。感知器也被称为 单层人工神经网络，以区别于较复杂的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA">多层感知器</a>（<strong>M</strong>ulti<strong>l</strong>ayer <strong>P</strong>erceptron）。尽管结构简单，感知器却能学习并解决相当复杂的问题。不过，感知器存在本质上的缺陷，就是无法处理<a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/puqutogether/article/details/41309745">线性不可分</a>问题</p>
<h4 id="多层感知器（MLP-Multilayer-Perceptron）"><a href="#多层感知器（MLP-Multilayer-Perceptron）" class="headerlink" title="多层感知器（MLP, Multilayer Perceptron）"></a>多层感知器（MLP, Multilayer Perceptron）</h4><p>　<strong>多层感知器</strong>（<strong>MLP</strong>, <strong>M</strong>ulti<strong>l</strong>ayer <strong>P</strong>erceptron）是一种<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>（Feedforward Neural Network），映射一组输入向量到一组输出向量。MLP 可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a>的神经元（或称处理单元）。另外，MLP 是（单层）<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8">感知器</a>的推广，摆脱了早期离散传输函数的束缚，使用 激活函数（Sigmoid / Tanh / ReLU / …）模拟神经元对激励的响应，在训练算法上则使用<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">反向传播算法</a>，解决了感知器不能对<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/27210162">线性不可分</a>数据进行处理的问题</p>
<h4 id="深度神经网络（DNN-Deep-Neural-Networks）"><a href="#深度神经网络（DNN-Deep-Neural-Networks）" class="headerlink" title="深度神经网络（DNN, Deep Neural Networks）"></a>深度神经网络（<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0#.E6.B7.B1.E5.BA.A6.E7.A5.9E.E7.BB.8F.E7.BD.91.E7.BB.9C">DNN</a>, Deep Neural Networks）</h4><p>　<strong>深度神经网络</strong>（<strong>DNN</strong>, <strong>D</strong>eep <strong>N</strong>eural <strong>N</strong>etworks）是一种<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B">判别模型</a>，可以使用<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">反向传播算法</a>进行训练。DNN 通过 “预训练” 和 “ReLU、Maxout 等激活函数” 解决了 多层感知器 中的 “局部最优解” 和 “<a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.cnblogs.com/maybe2030/p/6336896.html#_label2">梯度衰减</a>” 问题。不过，与其他神经网络模型类似，如果仅仅是简单地训练，深度神经网络可能会存在很多问题。常见的两类问题是<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%BF%87%E6%8B%9F%E5%90%88">过拟合</a>（Overfiting）和过长的运算时间</p>
<p>　因为增加了隐藏层，会使得模型对训练数据中较为罕见的依赖关系进行建模，所以深度神经网络很容易出现<strong>过拟合</strong>现象。对此，可以利用 <strong>稀疏</strong>（$L_1$ 正则化）或者 <strong>权重递减</strong>（$L_2$ 正则化） 等方法在训练过程中减少过拟合现象。另外，还有一种叫做丢弃法（Dropout）的正则化方法，即在训练中随机<strong>丢弃一部分隐层单元</strong>，来避免对较为罕见的依赖进行建模</p>
<p>　<strong>反向传播算法</strong>和<strong>梯度下降法</strong>由于其实现简单，并且相比其他方法，能够收敛到更好的局部最优值，因而成为神经网络训练的通行方法。但是，这些方法的计算代价很高，尤其是在训练 DNN 时，因为其规模（即层数和每层的节点数）、学习率、初始权重等众多参数都需要考虑。考虑到<strong>时间代价</strong>，想要扫描所有的参数是不可行的，因而考虑将多个训练样本组合进行 <strong>小批量训练</strong>（mini-batching），而非每次只使用一个样本进行训练，从而加速模型训练。而最显著地速度提升来自 GPU，因为矩阵和向量计算非常适合使用 GPU 实现。但使用大规模集群进行 DNN 训练仍然存在瓶颈，因而在并行化方面仍有很大的提升空间</p>
<h4 id="卷积神经网络（CNN-Convolutional-Neural-Network）"><a href="#卷积神经网络（CNN-Convolutional-Neural-Network）" class="headerlink" title="卷积神经网络（CNN, Convolutional Neural Network）"></a>卷积神经网络（CNN, Convolutional Neural Network）</h4><p>　<strong>卷积神经网络</strong>（<strong>CNN</strong>, <strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork）是一种<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>，由哈弗医学院生理学家 Hubel 和 Wiesel 通过对猫视觉皮层细胞的研究，在 1962 年提出了<strong>感受野</strong>（Receptive Field）的概念，随后在 1984 年日本学者 Fukushima 基于 RF 的概念，设计出了神经感知机（Neocognitron）。其人工神经元可以响应一部分覆盖范围内的周围单元，对于图像处理和语音识别有着出色的表现</p>
<p>　卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（Pooling Layer）。这一结构使得 CNN 能够利用输入数据的二维结构，也可以使用<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">反向传播算法</a>进行训练。相比较其他的前馈神经网络，CNN 通过<strong>卷积核</strong>控制只在同一个核内的神经元进行全连接，使得需要估计的参数很少，从而在根本上解决了 DNN 的<strong>参数膨胀</strong>的问题。但是，CNN 仍然存在无法对时间序列进行建模的缺陷</p>
<h4 id="递归神经网络（RNN-Recurrent-Neural-Networks）"><a href="#递归神经网络（RNN-Recurrent-Neural-Networks）" class="headerlink" title="递归神经网络（RNN, Recurrent Neural Networks）"></a>递归神经网络（<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/zh-hans/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">RNN</a>, Recurrent Neural Networks）</h4><p>　<strong>递归神经网络</strong>（<strong>RNN</strong>, <strong>R</strong>ecurrent <strong>N</strong>eural <strong>N</strong>etworks）是两种<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">人工神经网络</a>的总称。一种是<strong>时间递归神经网络</strong>（Recurrent Neural Network），另一种是<strong>结构递归神经网络</strong>（Recursive Neural Network）。前者的神经元间连接构成<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%9C%89%E5%90%91%E5%9B%BE">有向图</a>，而后者利用相似的 神经网络结构 递归构造更为复杂的深度网络。RNN 一般指代 时间递归神经网络。单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing Gradient Problem），所以难以捕捉长期时间的关联；而结合不同变种的 <strong>LSTM</strong> 网络 可以很好解决这个问题</p>
<h4 id="长短期记忆网络（LSTM-Long-Short-Term-Memory-Networks）"><a href="#长短期记忆网络（LSTM-Long-Short-Term-Memory-Networks）" class="headerlink" title="长短期记忆网络（LSTM, Long Short Term Memory Networks）"></a>长短期记忆网络（<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6">LSTM</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Long Short Term Memory Networks</a>）</h4><p>　<strong>长短期记忆</strong>（<strong>LSTM</strong>, <strong>L</strong>ong <strong>S</strong>hort <strong>T</strong>erm <strong>M</strong>emory <strong>N</strong>etworks）是 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">时间递归神经网络</a>（RNN） 的一种，论文首次发表于 1997 年。由于巧妙的设计结构，LSTM 适合于处理和预测<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97">时间序列</a>中间隔和延迟非常长的重要事件。因为 时间递归神经网络 和 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">前馈神经网络</a>（Feedforward Neural Network）接受较特定结构的输入不同，RNN 将状态（Cell State） 在自身网络中循环传递，使得 LSTM 可以接受更广泛的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97">时间序列</a>结构的输入，进而更好地描述动态时间行为</p>
<p>　一般的，LSTM 的表现会比 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">时间递归神经网络</a>（RNN）及<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B">隐马尔科夫模型</a>（HMM）更好，比如用在不分段连续<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB">手写识别</a>上。在 2009 年，用 LSTM 构建的人工神经网络模型就赢过了 ICDAR 手写识别比赛冠军。同时，LSTM 还普遍应用于自主<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB">语音识别</a>，2013 年运用 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/TIMIT">TIMIT</a> 自然演讲数据库达成 17.7% 错误率的纪录。而作为<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%BA%BF%E6%80%A7">非线性</a>模型，LSTM 又可当作复杂的非线性单元用于构造更庞大的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度神经网络</a></p>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>　其实将机器学习和深度学习分为两个章节来讲，会容易让人产生错觉，误以为机器学习和深度学习是两个不相干的领域。实际上，后者只是前者的一个子集。针对两者关系，更详细的描述可以参见后面 “<a href="https://yuzhouwan.com/posts/42737/#AI-到底是什么">AI 到底是什么</a>” 部分</p>
<h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><h4 id="分类与回归"><a href="#分类与回归" class="headerlink" title="分类与回归"></a>分类与回归</h4><p>　机器学习任务中，预测值为<strong>离散</strong>类型的，则称该任务为<strong>分类</strong>任务（Classification）；反之，预测值为<strong>连续</strong>类型的，则称该任务为<strong>回归</strong>任务（Regression）</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86">贝叶斯定理</a></h3><h4 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h4><script type="math/tex; mode=display">P(B \mid A) = \frac{P(B)\,P(A \mid B)}{P(A)}</script><p>　其中，$P(A \mid B)$ 是在 $B$ 发生的情况下 $A$ 发生的可能性，称为 $A$ 的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87">后验概率</a>。相应的，$P(B \mid A)$ 则称为 $B$ 后验概率；$P(A)$ 是不考虑任何 $B$ 方面的因素下 $A$ 发生的可能性，称为 $A$ 的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87">先验概率</a>（或<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87">边缘概率</a>）。相应的，$P(B)$ 则称为 $B$ 的先验概率</p>
<h4 id="种类-1"><a href="#种类-1" class="headerlink" title="种类"></a>种类</h4><h5 id="特征独立性"><a href="#特征独立性" class="headerlink" title="特征独立性"></a>特征独立性</h5><p>　按照特征之间<strong>独立性</strong>的强弱，可以分为 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zhuanlan.zhihu.com/p/25097242">朴素贝叶斯</a>、半朴素贝叶斯、（一般的）贝叶斯 等</p>
<h5 id="分布情况"><a href="#分布情况" class="headerlink" title="分布情况"></a>分布情况</h5><p>　按照属性和特征的<strong>分布</strong>情况，又可以分为 高斯贝叶斯、多项式贝叶斯、伯努利贝叶斯 等</p>
<h5 id="离散程度"><a href="#离散程度" class="headerlink" title="离散程度"></a>离散程度</h5><p>　按照训练集的<strong>离散</strong>程度，还可以分为 离散型贝叶斯、连续型贝叶斯、混合型贝叶斯 等</p>
<h4 id="编码实战"><a href="#编码实战" class="headerlink" title="编码实战"></a>编码实战</h4><p>　The set <code>A</code> contains 30 <code>a</code> and 10 <code>b</code>, and the set <code>B</code> contains 20 <code>a</code> and 20 <code>b</code>, then what is the value of <code>P(A|a)</code>?</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">situations = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">priori_probability</span>(<span class="params">situation, probability</span>):</span></span><br><span class="line">    situations[situation] = probability  <span class="comment"># P(A), P(B)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">posterior_probability</span>(<span class="params">situation, probability</span>):</span></span><br><span class="line">    old_prob = situations[situation]</span><br><span class="line">    situations[situation] = old_prob * probability  <span class="comment"># P(A)P(a|A), P(B)P(a|B)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>():</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> situation <span class="keyword">in</span> situations.values():</span><br><span class="line">        count += situation  <span class="comment"># P(A)P(a|A) + P(B)P(a|B)</span></span><br><span class="line">    <span class="keyword">for</span> situation, probability <span class="keyword">in</span> situations.items():</span><br><span class="line">        <span class="comment"># P(A)P(a|A)/(P(A)P(a|A) + P(B)P(a|B)), P(B)P(a|B)/(P(A)P(a|A) + P(B)P(a|B))</span></span><br><span class="line">        situations[situation] = situations[situation] / count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prob</span>(<span class="params">hypothis</span>):</span></span><br><span class="line">    <span class="keyword">return</span> situations[hypothis]  <span class="comment"># P(A|a), P(B|a)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">priori_probability(<span class="string">'A'</span>, <span class="number">0.5</span>)  <span class="comment"># P(A)=1/2</span></span><br><span class="line">priori_probability(<span class="string">'B'</span>, <span class="number">0.5</span>)  <span class="comment"># P(B)=1/2</span></span><br><span class="line"></span><br><span class="line">posterior_probability(<span class="string">'A'</span>, <span class="number">0.75</span>)  <span class="comment"># P(a|A)=3/4</span></span><br><span class="line">posterior_probability(<span class="string">'B'</span>, <span class="number">0.5</span>)   <span class="comment"># P(a|B)=1/2</span></span><br><span class="line"></span><br><span class="line">normalize()</span><br><span class="line">prob = prob(<span class="string">'A'</span>)  <span class="comment"># P(A|a)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'The probability of getting `a` that belongs to set `A`: %s'</span> % prob)</span><br><span class="line"><span class="comment"># P(a|A): 从 A 中获取 a</span></span><br><span class="line"><span class="comment"># P(A|a): 获取 a，并且 a 恰巧是属于 A 的</span></span><br><span class="line"><span class="comment"># 这两个描述的场景完全不同，对应的概率也因而不同</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>　当存在多个特征变量时，表达式可扩展为</p>
<script type="math/tex; mode=display">P(C \mid F_1,\dots,F_n) = \frac{P(C) \, P(F_1,\dots,F_n \mid C)}{P(F_1,\dots,F_n)} = \frac{1}{Z} P(C)\prod_{i=1}^n P(F_i \mid C)</script><p>　其中，$\frac{1}{Z}$ 是一个只与 $F_i$ 相关的缩放因子，且当特征变量的值固定时，$\frac{1}{Z}$ 为常量</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h3 id="深度网络概要"><a href="#深度网络概要" class="headerlink" title="深度网络概要"></a>深度网络概要</h3><h4 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Activation_function">激活函数</a>（Activation Function）</h4><h5 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h5><p>　<strong>激活函数</strong> 就是在神经网络的神经元上运行的函数，负责将神经元的输入映射到输出端。主要解决线性不可分问题，如 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/22334626">XOR 异或</a> 等</p>
<h5 id="种类-2"><a href="#种类-2" class="headerlink" title="种类"></a>种类</h5><h6 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h6><p>　<strong>Sigmoid</strong> 激活函数的表达式为 $f(x)$ = $\frac{e^x}{e^x + 1}$ = $\frac{1}{1 + e^{-x}}$</p>
<p><img data-src="/picture/ai/ai_sigmoid.png" alt></p>
<center>（图片来源：<a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>，已确认版权为 CC BY-SA 3.0 协议）</center>



<h6 id="TanHyperbolic（Tanh）"><a href="#TanHyperbolic（Tanh）" class="headerlink" title="TanHyperbolic（Tanh）"></a>TanHyperbolic（Tanh）</h6><p>　<strong>Tanh</strong> 激活函数的表达式为 $f(x)$ = $\frac{e^x - e^{-x}}{e^x + e^{-x}}$，属于<a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E5%87%BD%E6%95%B0">双曲</a>正切函数，如下图表示</p>
<p><img data-src="/picture/ai/ai_tanh.png" alt></p>
<center>（图片来源：<a href="https://en.wikipedia.org/wiki/Hyperbolic_function" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>，已确认无版权）</center>



<h6 id="ReLU-amp-Softplus"><a href="#ReLU-amp-Softplus" class="headerlink" title="ReLU &amp; Softplus"></a>ReLU &amp; Softplus</h6><p>　<strong>ReLU</strong> 激活函数表达式为 $f(x)$ = $max(0, x)$</p>
<p>　<strong>Softplus</strong> 激活函数表达式为 $f(x)$ = $\log(1+e^x)$</p>
<p><img data-src="/picture/ai/ai_relu.png" alt></p>
<center>（图片来源：<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>，已确认版权为 CC0 1.0 协议）</center>



<h6 id="LReLU-amp-PReLU-amp-RReLU"><a href="#LReLU-amp-PReLU-amp-RReLU" class="headerlink" title="LReLU &amp; PReLU &amp; RReLU"></a>LReLU &amp; PReLU &amp; RReLU</h6><p><img data-src="/picture/ai/ai_relu_prelu_rrelu.png" alt></p>
<center>（图片来源：<a href="https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons" target="_blank" rel="external nofollow noopener noreferrer">stackexchange.com</a>，已确认版权为 CC BY-SA 3.0 协议）</center>



<h6 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/pdf/1302.4389.pdf">Maxout</a></h6><p>　<strong>Maxout</strong> 激活函数表达式为 $f(x)$ = $max_{j\in[1, k]}z_{ij}$，其中，$x \in R$，$z_{ij}$ = $x^TW_{\dots ij} + b_{ij}$，$W \in R^{d \cdot m \cdot k}$，$b \in R^{m \cdot k}$</p>
<h6 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/pdf/1710.05941.pdf">Swish</a></h6><p>　<strong>Swish</strong> 激活函数表达式为 $f(x)$ = $x \cdot sigmoid(x)$</p>
<h4 id="代价函数（Cost-Function）"><a href="#代价函数（Cost-Function）" class="headerlink" title="代价函数（Cost Function）"></a>代价函数（Cost Function）</h4><h5 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h5><p>　<strong>代价函数</strong>（又称为 <strong>损失函数</strong> 或 <strong>成本函数</strong>）用来估量模型预测值 $f(x)$ 与 真实值 $Y$ 的偏差程度，通常表示为 $L(Y, f(x))$。代价函数的值越小，说明模型的鲁棒性越好。在特定的领域中，代价函数又会被称为 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Reward_function">回报函数</a>，<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Profit_function">利润函数</a>，<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Utility_function">效用函数</a>，<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Fitness_function">适应度函数</a> 等</p>
<h5 id="种类-3"><a href="#种类-3" class="headerlink" title="种类"></a>种类</h5><h6 id="二次代价函数（Quadratic-Cost）"><a href="#二次代价函数（Quadratic-Cost）" class="headerlink" title="二次代价函数（Quadratic Cost）"></a>二次代价函数（Quadratic Cost）</h6><p>　<strong>二次代价函数</strong>表达式为 $C$ = $\frac{1}{2n}\sum\mid\mid y(x)$ - $a^L(x)\mid\mid^2$，其中 $x$ 代表样本，$y$ 代表实际值，$a$ 代表输出值，$n$ 代表样本的总量</p>
<p>　当 $n = 1$ 时，样本集中只有一个样本，则二次代价函数可表示为 $C = \frac{1}{2}(((y - a)^1)^{\frac{1}1})^2$ = $\frac{(y - a)^2}{2}$，其中信号总量表示为 $z = \sum{W_jX_j} + b$, 激活函数表示为 $a = \sigma(z)$，则 $C = \frac{(y - \sigma(\sum{W_jX_j} + b))^2}{2}$。此时，我们对 $C$ 分别求 权重值 $w$ 和 偏置量 $b$ 的偏导，则得到 $\frac{\partial C}{\partial w}$ = $(a - y)\sigma^\prime(z) x$ 和 $\frac{\partial C}{\partial b}$ = $(a - y)\sigma^\prime(z)$。由此可见，$w$ 和 $b$ 的梯度和 $\sigma$ 的梯度成正比，$\sigma$ 的梯度越大，$w$ 和 $b$ 的调整速度越快，训练收敛便越快（推导过程相对比较简单，只用到了 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E5%BA%A6%E9%87%8F">度量空间求向量距离</a> 和 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99">复合函数求偏导</a>）</p>
<h6 id="交叉熵（Cross-Entropy）"><a href="#交叉熵（Cross-Entropy）" class="headerlink" title="交叉熵（Cross Entropy）"></a>交叉熵（Cross Entropy）</h6><p>　<strong>交叉熵代价函数</strong>表达式为 $C = -\frac{1}{n}\sum_{x=1}^{n}[y\ln a + (1 - y)\ln(1 - a)]$，其中 $x$ 代表样本，$y$ 代表实际值，$a$ 代表输出值，$n$ 代表样本的总量</p>
<p>　和二次代价函数一样，不改变其激活函数，信号总量表示为 $z = \sum{W_jX_j} + b$, 激活函数表示为 $a = \sigma(z)$，则 $\sigma^\prime(z) = \sigma(x)(1 - \sigma(z))$。再次对 $C$ 求 $w$ 和 $b$ 的偏导分别为 $\frac{\partial C}{\partial w_j} = \frac{1}n\sum_{x=1}^nx_j(\sigma(z)-y)$ 和 $\frac{\partial C}{\partial b} = \frac{1}n\sum_{x=1}^n(\sigma(z) - y)$。由此可见，$w$ 和 $b$ 的调整与 $\sigma^\prime(z)$ 无关，并且，当 $\sigma(z) - y$ 预测值与实际值误差越大，$C$ 的梯度（求导）也就越大，训练的收敛速度也就越大。因此，$sigmoid$ 此类 $S$ 形激活函数，则不适合使用上文中介绍的二次代价函数；不过如果激活函数是线性的，则可能二次代价函数的表现更佳</p>
<h6 id="对数似然代价函数（Log-likelihood-Cost）"><a href="#对数似然代价函数（Log-likelihood-Cost）" class="headerlink" title="对数似然代价函数（Log-likelihood Cost）"></a>对数似然代价函数（Log-likelihood Cost）</h6><p>　<strong>对数似然代价函数</strong>常用于 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/u014313009/article/details/51045303">Softmax</a> 归一化指数函数，可以有效地解决学习速度变慢的问题；但是，如果输出层神经元是 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap1/c1s2.html">Sigmoid</a>，则建议采用交叉熵代价函数</p>
<h4 id="优化器（Optimizer）"><a href="#优化器（Optimizer）" class="headerlink" title="优化器（Optimizer）"></a>优化器（Optimizer）</h4><p>　<a target="_blank" rel="external nofollow noopener noreferrer" href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf">Momentum</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://books.google.com.tr/books?id=2-ElBQAAQBAJ&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false">NAG</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adagrad</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="http://arxiv.org/abs/1212.5701">Adadelta</a>, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://class.coursera.org/neuralnets-2012-001/lecture/67">RMSprop</a></p>
<p><img data-src="/picture/ai/ai_noisy_moons.gif" alt></p>
<p><img data-src="/picture/ai/ai_beale_function.gif" alt></p>
<p><img data-src="/picture/ai/ai_long_valley.gif" alt></p>
<p><img data-src="/picture/ai/ai_saddle_point.gif" alt></p>
<center>（图片来源：<a href="http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html" target="_blank" rel="external nofollow noopener noreferrer">denizyuret.com</a>，已询问作者）</center>




<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h4><h4 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h4><h5 id="常见类型-1"><a href="#常见类型-1" class="headerlink" title="常见类型"></a>常见类型</h5><h6 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h6><p><img data-src="/picture/ai/ai_max_pooling.png" alt></p>
<center>（图片来源：<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling" target="_blank" rel="external nofollow noopener noreferrer">wikipedia.org</a>，已确认版权为 CC BY-SA 4.0 协议）</center>


<h6 id="Mean-Pooling"><a href="#Mean-Pooling" class="headerlink" title="Mean Pooling"></a>Mean Pooling</h6><h6 id="L2-norm-pooling"><a href="#L2-norm-pooling" class="headerlink" title="L2-norm pooling"></a>L2-norm pooling</h6><h6 id="Down-Pooling"><a href="#Down-Pooling" class="headerlink" title="Down Pooling"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.cnblogs.com/wangduo/p/6762914.html">Down Pooling</a></h6><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><h5 id="Same-Padding"><a href="#Same-Padding" class="headerlink" title="Same Padding"></a>Same Padding</h5><p>　给采样平面外部补 $0$，使得卷积窗口采样之后，可以得到一个与被采样平面，大小一样的结果平面</p>
<h5 id="Valid-Padding"><a href="#Valid-Padding" class="headerlink" title="Valid Padding"></a>Valid Padding</h5><p>　不会超出采样平面，卷积窗口采样之后，会到得到一个比原来平面小的结果平面</p>
<h2 id="开源项目"><a href="#开源项目" class="headerlink" title="开源项目"></a>开源项目</h2><h3 id="Tensorflow-Tensorflow"><a href="#Tensorflow-Tensorflow" class="headerlink" title="Tensorflow / Tensorflow"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow">Tensorflow</a> / <strong><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/tensorflow/tensorflow">Tensorflow</a></strong></h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>TensorFlow</strong>™ 是一个端到端开源机器学习平台。它拥有一个全面而灵活的生态系统，其中包含各种工具、库和社区资源，可助力研究人员推动先进机器学习技术的发展，并使开发者能够轻松地构建和部署由机器学习提供支持的应用。  — <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/">tensorflow.org</a></p>
<h5 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h5><ul>
<li>高度的灵活性</li>
<li>可移植性</li>
<li>极大地提高了科研产出率</li>
<li>自动求微分</li>
<li>多语言支持（<a href="https://yuzhouwan.com/posts/43687/">Python</a> / C++ / <a href="https://yuzhouwan.com/posts/27328/">Java</a> / Golang）</li>
<li>最大化硬件的性能</li>
</ul>
<h5 id="基本概念-2"><a href="#基本概念-2" class="headerlink" title="基本概念"></a>基本概念</h5><ul>
<li>使用 <strong>图</strong>（Graph）来表示计算任务</li>
<li>在被称之为 <strong>会话</strong>（Session）的<strong>上下文</strong>（Context）中执行图</li>
<li>使用 <strong>Tensor</strong> 表示数据</li>
<li>通过 <strong>变量</strong>（Variable）维护状态</li>
<li>使用 <code>feed</code> 和 <code>fetch</code> 可以为任意的<strong>操作</strong>（Arbitrary Operation）赋值或从中获取数据</li>
</ul>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><h5 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h5><p>　需要注意的是，如果是在 Windows 环境下，只能<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.python.org/downloads/">安装</a> <code>Python3.5+</code> 版本。详细安装步骤见《<a href="https://yuzhouwan.com/posts/43687/#环境部署">Python - 环境部署</a>》</p>
<h5 id="Anaconda3"><a href="#Anaconda3" class="headerlink" title="Anaconda3"></a>Anaconda3</h5><p>　下载地址：<a target="_blank" rel="external nofollow noopener noreferrer" href="https://repo.continuum.io/archive/">Download Page</a></p>
<p>　启动程序的快捷链接，都自动创建在了 <code>C:\ProgramData\Microsoft\Windows\Start Menu\Programs\Anaconda3 (64-bit)</code> 目录下</p>
<p>　在运行 <code>Jupyter</code> 之前，需要简单设置下，文件的存放路径（如果只想使用 Jupyter，相关安装步骤见《<a href="https://yuzhouwan.com/posts/43687/#科学分析工具">Python - 科学分析工具</a>》）</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ jupyter notebook --generate-config</span><br><span class="line">  Writing default config to: C:\Users\Benedict Jin\.jupyter\jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line">$ vim jupyter_notebook_config.py</span><br><span class="line">  <span class="comment">## The directory to use for notebooks and kernels.</span></span><br><span class="line">  c.NotebookApp.notebook_dir = <span class="string">'E:\Jupyter\TensorFlow'</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.tensorflow.org/install/">TensorFlow</a></h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 安装</span></span><br><span class="line"><span class="comment"># cpu</span></span><br><span class="line">$ pip3 install tensorflow</span><br><span class="line"><span class="comment"># gpu</span></span><br><span class="line">$ pip3 install tensorflow-gpu</span><br><span class="line"></span><br><span class="line"><span class="comment">## 升级</span></span><br><span class="line"><span class="comment"># cpu</span></span><br><span class="line">$ pip3 install --upgrade tensorflow</span><br><span class="line"><span class="comment"># gpu</span></span><br><span class="line">$ pip3 install --upgrade tensorflow-gpu</span><br><span class="line"></span><br><span class="line"><span class="comment">## 测试</span></span><br><span class="line">$ python</span><br><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</span><br><span class="line">&gt;&gt;&gt; sess = tf.Session()</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(sess.run(hello))</span><br><span class="line">  b<span class="string">'Hello, TensorFlow!'</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: 当然还有其他的方式，可以帮助我们更方便地使用这些科学分析库，包括 <a target="_blank" rel="external nofollow noopener noreferrer" href="http://winpython.sourceforge.net/">WinPython</a>、<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.enthought.com/product/canopy/">Enthought Canopy</a> etc.</p>
<h4 id="编程实战"><a href="#编程实战" class="headerlink" title="编程实战"></a>编程实战</h4><h5 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h5><h6 id="加减"><a href="#加减" class="headerlink" title="加减"></a>加减</h6><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">b = tf.Variable([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># sub/add 两个 operation</span></span><br><span class="line">sub = tf.subtract(a, b)</span><br><span class="line">add = tf.add(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 Session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="built_in">print</span>(sess.run(sub))</span><br><span class="line">    <span class="built_in">print</span>(sess.run(add))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#[1 -1]</span></span><br><span class="line">  <span class="comment">#[1 1]</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="累计"><a href="#累计" class="headerlink" title="累计"></a>累计</h6><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Counter</span></span><br><span class="line">counter = tf.Variable(<span class="number">0</span>, name = <span class="string">"counter"</span>)</span><br><span class="line">add_one = tf.add(counter, <span class="number">1</span>)</span><br><span class="line">assign = tf.assign(counter, add_one)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        sess.run(assign)</span><br><span class="line">        <span class="built_in">print</span>(sess.run(counter))</span><br><span class="line"></span><br><span class="line">  <span class="number">1</span></span><br><span class="line">  <span class="number">2</span></span><br><span class="line">  <span class="number">3</span></span><br><span class="line">  <span class="number">4</span></span><br><span class="line">  <span class="number">5</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/Variable.ipynb">here</a>.</p>
<h5 id="矩阵积"><a href="#矩阵积" class="headerlink" title="矩阵积"></a>矩阵积</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一行两列</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 两行一列</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">1</span>], [<span class="number">0</span>]])</span><br><span class="line"><span class="comment"># 矩阵相乘</span></span><br><span class="line">matmul = tf.matmul(matrix1, matrix2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果（Tensor 而不是结果 2）</span></span><br><span class="line"><span class="built_in">print</span>(matmul)</span><br><span class="line"></span><br><span class="line">  Tensor(<span class="string">"MatMul:0"</span>, shape = (<span class="number">1</span>, <span class="number">1</span>), dtype = int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义会话</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># 真正开始执行</span></span><br><span class="line"><span class="built_in">print</span>(sess.run(matmul))</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line">  [[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 改写成 `with ... as ...`，则可以保证 Session 会自动关闭</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(matmul))</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/Matrix.ipynb">here</a>.</p>
<h5 id="Fetch-amp-Feed"><a href="#Fetch-amp-Feed" class="headerlink" title="Fetch &amp; Feed"></a>Fetch &amp; Feed</h5><h6 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h6><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fetch</span></span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">multiply = tf.multiply(a, b)</span><br><span class="line">add = tf.add(matmul, c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fetch 可以运行多个 Op</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run([add, multiply]))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#[5.0, 2.0]</span></span><br></pre></td></tr></tbody></table></figure>
<h6 id="Feed"><a href="#Feed" class="headerlink" title="Feed"></a>Feed</h6><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Feed</span></span><br><span class="line">ph1 = tf.placeholder(tf.float32)</span><br><span class="line">ph2 = tf.placeholder(tf.float32)</span><br><span class="line">m = tf.multiply(ph1, ph2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在真正执行的时候，再将数据以字典的形式传入</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(m, feed_dict={ph1: [<span class="number">1.0</span>], ph2: [<span class="number">2.0</span>]}))</span><br><span class="line"></span><br><span class="line">  <span class="comment">#[ 2.]</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/Feed%20%26%20Fetch.ipynb">here</a>.</p>
<h5 id="二次代价函数-amp-梯度下降"><a href="#二次代价函数-amp-梯度下降" class="headerlink" title="二次代价函数 &amp; 梯度下降"></a>二次代价函数 &amp; 梯度下降</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line">y_data = x_data * <span class="number">0.1</span> + <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性模型的斜率 &amp; 偏置量</span></span><br><span class="line">k = tf.Variable(<span class="number">0.</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.</span>)</span><br><span class="line">y = k * x_data + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次代价函数</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_data - y))</span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 最小化代价函数</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性模型中，k、b 作为变量，会在梯度下降法的作用下不断变化，以使得 loss 函数越来越小</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">501</span>):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Step: "</span>, step, <span class="string">"[k, b]"</span>, sess.run([k, b]))</span><br><span class="line"></span><br><span class="line">  Step:  <span class="number">0</span> [k, b] [<span class="number">0.054201454</span>, <span class="number">0.10031855</span>]</span><br><span class="line">  Step:  <span class="number">100</span> [k, b] [<span class="number">0.10048652</span>, <span class="number">0.19973609</span>]</span><br><span class="line">  Step:  <span class="number">200</span> [k, b] [<span class="number">0.10003704</span>, <span class="number">0.1999799</span>]</span><br><span class="line">  Step:  <span class="number">300</span> [k, b] [<span class="number">0.10000283</span>, <span class="number">0.19999847</span>]</span><br><span class="line">  Step:  <span class="number">400</span> [k, b] [<span class="number">0.10000023</span>, <span class="number">0.19999987</span>]</span><br><span class="line">  Step:  <span class="number">500</span> [k, b] [<span class="number">0.10000023</span>, <span class="number">0.19999987</span>]</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/Loss%20Function%20%26%20GD.ipynb">here</a>.</p>
<h5 id="非线性回归"><a href="#非线性回归" class="headerlink" title="非线性回归"></a>非线性回归</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 [-0.5, 0.5] 区间内生成随机数</span></span><br><span class="line">x_data = np.linspace(-<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">100</span>)[:, np.newaxis]</span><br><span class="line"><span class="comment"># 在 100 个随机数基础上，增加噪音</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.02</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 神经网络 中间层</span></span><br><span class="line">weight_L1 = tf.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">10</span>]))</span><br><span class="line">biase_L1 = tf.Variable(tf.zeros([<span class="number">1</span>, <span class="number">10</span>]))</span><br><span class="line">w_plus_b_L1 = tf.matmul(x, weight_L1) + biase_L1</span><br><span class="line"><span class="comment"># 激活函数</span></span><br><span class="line">L1 = tf.nn.tanh(w_plus_b_L1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 神经网络 输出层</span></span><br><span class="line">wegith_L2 = tf.Variable(tf.random_normal([<span class="number">10</span>, <span class="number">1</span>]))</span><br><span class="line">biase_L2 = tf.Variable(tf.zeros([<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">w_plus_b_L2 = tf.matmul(L1, wegith_L2) + biase_L2</span><br><span class="line">prediction = tf.nn.tanh(w_plus_b_L2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss Function</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - prediction))</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1500</span>):</span><br><span class="line">        sess.run(train, feed_dict = {x: x_data, y: y_data})</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用训练好的模式进行预测</span></span><br><span class="line">    prediction_result = sess.run(prediction, feed_dict = {x: x_data})</span><br><span class="line"></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data, y_data)</span><br><span class="line">    plt.plot(x_data, prediction_result, <span class="string">'r-'</span>, lw = <span class="number">5</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img data-src="/picture/ai/ai_linear_regression.png" alt></p>
<center>（使用 <a href="https://yuzhouwan.com/posts/43687/#Matplotlib" target="_blank">Matplotlib</a> 绘制而成）</center>

<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/Non%20Linear%20Regression.ipynb">here</a>.</p>
<h5 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download MNIST datasource</span></span><br><span class="line"><span class="comment"># 6w 个 28*28 个像素的手写数字图片集</span></span><br><span class="line"><span class="comment"># 用 [60000, 784] 的张量表示 [图片索引，图片像素点索引]</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># `one-hot vectors`：向量中只有一个数据为 1，其余维度只能为 0</span></span><br><span class="line"><span class="comment"># 转化为 [60000, 10] 的张量表示 [图片索引，图片表示的数值]</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 28 * 28 = 784 的占位符</span></span><br><span class="line"><span class="comment"># None 表示可能是任何数值</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">z = tf.placeholder(tf.float32)                        <span class="comment"># 用于 drop_out 操作时的依据（0.8：80% 的神经元在工作）</span></span><br><span class="line">lr = tf.Variable(<span class="number">0.001</span>, dtype = tf.float32)           <span class="comment"># 用于不断递减的学习率，使得梯度下降到最低点时，能更好地命中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重值（截断的随机正太分布）和 偏置量（0.1）</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>, <span class="number">600</span>], stddev = <span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">600</span>]) + <span class="number">0.1</span>)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1, z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层</span></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([<span class="number">600</span>, <span class="number">400</span>], stddev = <span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">400</span>]) + <span class="number">0.1</span>)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2, z)</span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([<span class="number">400</span>, <span class="number">10</span>], stddev = <span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>]) + <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax 回归模型</span></span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop, W3) + b3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次 Loss Func</span></span><br><span class="line"><span class="comment"># loss = tf.reduce_mean(tf.square(y - prediction))</span></span><br><span class="line"><span class="comment"># 交叉熵 Loss Func</span></span><br><span class="line"><span class="comment"># loss = tf.reduce_mean(- tf.reduce_sum(y * tf.log(prediction), reduction_indices = [1]))</span></span><br><span class="line">loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="comment"># train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="comment"># 判断 一维张量 y、prediction 中最大值的位置是否相等</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(prediction, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 准确率</span></span><br><span class="line"><span class="comment"># 将 布尔型列表 corrent_prediction 转化为 float32 类型</span></span><br><span class="line"><span class="comment"># [true, false, false, ...]  =&gt; [1.0, 0., 0., ...]</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">        batch_size = <span class="number">100</span></span><br><span class="line">        batch = (<span class="built_in">int</span>) (<span class="number">60000</span> / batch_size)</span><br><span class="line">        <span class="comment"># batch = mnist.train.num_examples</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">            sess.run(tf.assign(lr, <span class="number">0.001</span> * (<span class="number">0.95</span> ** _)))</span><br><span class="line">            <span class="keyword">for</span> batch_step <span class="keyword">in</span> <span class="built_in">range</span>(batch):</span><br><span class="line">                batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">                sess.run(train_step, feed_dict = {x: batch_xs, y: batch_ys, z: <span class="number">0.9973</span>})</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (_ % <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">                test_accuracy = sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels, z: <span class="number">1.0</span>})</span><br><span class="line">                train_accuracy = sess.run(accuracy, feed_dict = {x: mnist.train.images, y: mnist.train.labels, z: <span class="number">1.0</span>})</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"Batch: "</span>, _, <span class="string">"Accuracy: ["</span>, test_accuracy, <span class="string">","</span>, train_accuracy, <span class="string">"]"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 二次 Loss Func</span></span><br><span class="line">Batch:  <span class="number">0</span> Accuracy:  <span class="number">0.8394</span></span><br><span class="line">Batch:  <span class="number">10</span> Accuracy:  <span class="number">0.9067</span></span><br><span class="line">Batch:  <span class="number">20</span> Accuracy:  <span class="number">0.9142</span></span><br><span class="line">Batch:  <span class="number">30</span> Accuracy:  <span class="number">0.9187</span></span><br><span class="line">Batch:  <span class="number">40</span> Accuracy:  <span class="number">0.9199</span></span><br><span class="line">Batch:  <span class="number">50</span> Accuracy:  <span class="number">0.9219</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵 Loss Func</span></span><br><span class="line">Batch:  <span class="number">0</span> Accuracy:  <span class="number">0.8262</span></span><br><span class="line">Batch:  <span class="number">10</span> Accuracy:  <span class="number">0.9183</span></span><br><span class="line">Batch:  <span class="number">20</span> Accuracy:  <span class="number">0.9224</span></span><br><span class="line">Batch:  <span class="number">30</span> Accuracy:  <span class="number">0.9232</span></span><br><span class="line">Batch:  <span class="number">40</span> Accuracy:  <span class="number">0.9273</span></span><br><span class="line">Batch:  <span class="number">50</span> Accuracy:  <span class="number">0.9274</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏层 + DropOut</span></span><br><span class="line">Batch:  <span class="number">0</span> Accuracy: [ <span class="number">0.9176</span> , <span class="number">0.915527</span> ]</span><br><span class="line">Batch:  <span class="number">10</span> Accuracy: [ <span class="number">0.9565</span> , <span class="number">0.963182</span> ]</span><br><span class="line">Batch:  <span class="number">20</span> Accuracy: [ <span class="number">0.9669</span> , <span class="number">0.975236</span> ]</span><br><span class="line">Batch:  <span class="number">30</span> Accuracy: [ <span class="number">0.9718</span> , <span class="number">0.982</span> ]</span><br><span class="line">Batch:  <span class="number">40</span> Accuracy: [ <span class="number">0.9737</span> , <span class="number">0.984836</span> ]</span><br><span class="line">Batch:  <span class="number">50</span> Accuracy: [ <span class="number">0.9768</span> , <span class="number">0.987036</span> ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdamOptimizer + GPU</span></span><br><span class="line">Batch:  <span class="number">0</span> Accuracy: [ <span class="number">0.9573</span> , <span class="number">0.962128</span> ]</span><br><span class="line">Batch:  <span class="number">10</span> Accuracy: [ <span class="number">0.9803</span> , <span class="number">0.994455</span> ]</span><br><span class="line">Batch:  <span class="number">20</span> Accuracy: [ <span class="number">0.9795</span> , <span class="number">0.997073</span> ]</span><br><span class="line">Batch:  <span class="number">30</span> Accuracy: [ <span class="number">0.9816</span> , <span class="number">0.997709</span> ]</span><br><span class="line">Batch:  <span class="number">40</span> Accuracy: [ <span class="number">0.9828</span> , <span class="number">0.997946</span> ]</span><br><span class="line">Batch:  <span class="number">50</span> Accuracy: [ <span class="number">0.9823</span> , <span class="number">0.998128</span> ]</span><br><span class="line">Batch:  <span class="number">60</span> Accuracy: [ <span class="number">0.9828</span> , <span class="number">0.998237</span> ]</span><br><span class="line">Batch:  <span class="number">70</span> Accuracy: [ <span class="number">0.9828</span> , <span class="number">0.998309</span> ]</span><br><span class="line">Batch:  <span class="number">80</span> Accuracy: [ <span class="number">0.9831</span> , <span class="number">0.998346</span> ]</span><br><span class="line">Batch:  <span class="number">90</span> Accuracy: [ <span class="number">0.9828</span> , <span class="number">0.9984</span> ]</span><br><span class="line">Batch:  <span class="number">100</span> Accuracy: [ <span class="number">0.9831</span> , <span class="number">0.9984</span> ]</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: <strong>Softmax</strong> 公式为：$softmax(x)_i$ = $\frac{exp(x_i)}{\sum_j{exp(x_j)}}$ = $\frac{e^{x_i}}{\sum_j{e^{x_j}}}$<br>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/MNIST.ipynb">here</a>.</p>
<h4 id="TensorBoard-可视化"><a href="#TensorBoard-可视化" class="headerlink" title="TensorBoard 可视化"></a>TensorBoard 可视化</h4><h5 id="Scalar-amp-NameScope-amp-Embedding"><a href="#Scalar-amp-NameScope-amp-Embedding" class="headerlink" title="Scalar &amp; NameScope &amp; Embedding"></a>Scalar &amp; NameScope &amp; Embedding</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># Download MNIST datasource</span></span><br><span class="line"><span class="comment"># 6w 个 28*28 个像素的手写数字图片集</span></span><br><span class="line"><span class="comment"># 用 [60000, 784] 的张量表示 [图片索引，图片像素点索引]</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.tensorboard.plugins <span class="keyword">import</span> projector</span><br><span class="line"></span><br><span class="line"><span class="comment"># `one-hot vectors`：向量中只有一个数据为 1，其余维度只能为 0</span></span><br><span class="line"><span class="comment"># 转化为 [60000, 10] 的张量表示 [图片索引，图片表示的数值]</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">image_num = <span class="number">10000</span></span><br><span class="line">embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable = <span class="literal">False</span>, name = <span class="string">'embedding'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义求统计指标的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">summaries</span>(<span class="params">var</span>):</span></span><br><span class="line">    <span class="comment"># 申明一个命名空间</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'summaries'</span>):</span><br><span class="line">        tf.summary.scalar(<span class="string">'max'</span>, tf.reduce_max(var))        <span class="comment"># 最大值</span></span><br><span class="line">        tf.summary.scalar(<span class="string">'min'</span>, tf.reduce_min(var))        <span class="comment"># 最小值</span></span><br><span class="line">        mean = tf.reduce_mean(var)</span><br><span class="line">        tf.summary.scalar(<span class="string">'mean'</span>, mean)                     <span class="comment"># 平均值</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'stddev'</span>):</span><br><span class="line">            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))</span><br><span class="line">        tf.summary.scalar(<span class="string">'stddev'</span>, stddev)                 <span class="comment"># 标准差</span></span><br><span class="line">        tf.summary.histogram(<span class="string">'histogram'</span>, var)              <span class="comment"># 直方图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'input'</span>):</span><br><span class="line">    <span class="comment"># 28 * 28 = 784 的占位符</span></span><br><span class="line">    <span class="comment"># None 表示可能是任何数值</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>], name = <span class="string">'x_input'</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>], name = <span class="string">'y_input'</span>)</span><br><span class="line">    <span class="comment"># 用于 drop_out 操作时的依据（0.8：80% 的神经元在工作）</span></span><br><span class="line">    z = tf.placeholder(tf.float32, name = <span class="string">'drop_output_input'</span>)</span><br><span class="line">    lr = tf.Variable(<span class="number">0.001</span>, dtype = tf.float32)           <span class="comment"># 用于不断递减的学习率，使得梯度下降到最低点时，能更好地命中</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'layer'</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer_1'</span>):</span><br><span class="line">        <span class="comment"># 权重值（截断的随机正太分布）和 偏置量（0.1）</span></span><br><span class="line">        W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>, <span class="number">600</span>], stddev = <span class="number">0.1</span>), name = <span class="string">'W1'</span>)</span><br><span class="line">        b1 = tf.Variable(tf.zeros([<span class="number">600</span>]) + <span class="number">0.1</span>, name = <span class="string">'b1'</span>)</span><br><span class="line">        <span class="comment"># 调用函数求权重、偏置值的统计指标</span></span><br><span class="line">        summaries(W1)</span><br><span class="line">        summaries(b1)</span><br><span class="line">        L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)</span><br><span class="line">        L1_drop = tf.nn.dropout(L1, z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer_2'</span>):</span><br><span class="line">        <span class="comment"># 隐藏层</span></span><br><span class="line">        W2 = tf.Variable(tf.truncated_normal([<span class="number">600</span>, <span class="number">400</span>], stddev = <span class="number">0.1</span>), name = <span class="string">'W2'</span>)</span><br><span class="line">        b2 = tf.Variable(tf.zeros([<span class="number">400</span>]) + <span class="number">0.1</span>, name = <span class="string">'b2'</span>)</span><br><span class="line">        summaries(W2)</span><br><span class="line">        summaries(b2)</span><br><span class="line">        L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)</span><br><span class="line">        L2_drop = tf.nn.dropout(L2, z)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer_output'</span>):</span><br><span class="line">        W3 = tf.Variable(tf.truncated_normal([<span class="number">400</span>, <span class="number">10</span>], stddev = <span class="number">0.1</span>), name = <span class="string">'W3'</span>)</span><br><span class="line">        b3 = tf.Variable(tf.zeros([<span class="number">10</span>]) + <span class="number">0.1</span>, name = <span class="string">'b3'</span>)</span><br><span class="line">        summaries(W3)</span><br><span class="line">        summaries(b3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'softmax'</span>):</span><br><span class="line">        <span class="comment"># softmax 回归模型</span></span><br><span class="line">        prediction = tf.nn.softmax(tf.matmul(L2_drop, W3) + b3)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'loss'</span>):</span><br><span class="line">    <span class="comment"># 二次 Loss Func</span></span><br><span class="line">    <span class="comment"># loss = tf.reduce_mean(tf.square(y - prediction))</span></span><br><span class="line">    <span class="comment"># 交叉熵 Loss Func</span></span><br><span class="line">    <span class="comment"># loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(prediction), reduction_indices=[1]))</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = prediction))</span><br><span class="line">    tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    <span class="comment"># train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'correct_prediction'</span>):</span><br><span class="line">        <span class="comment"># 评估模型</span></span><br><span class="line">        <span class="comment"># 判断 一维张量 y、prediction 中最大值的位置是否相等</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(prediction, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'accuracy'</span>):</span><br><span class="line">        <span class="comment"># 准确率</span></span><br><span class="line">        <span class="comment"># 将 布尔型列表 corrent_prediction 转化为 float32 类型</span></span><br><span class="line">        <span class="comment"># [true, false, false, ...]  =&gt; [1.0, 0., 0., ...]</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">        tf.summary.scalar(<span class="string">'accuracy'</span>, accuracy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得所有定义的 Summary</span></span><br><span class="line">summary_all = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置运行资源</span></span><br><span class="line">session_config = tf.ConfigProto(device_count={<span class="string">"CPU"</span>: <span class="number">8</span>}, inter_op_parallelism_threads = <span class="number">32</span>, intra_op_parallelism_threads = <span class="number">48</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = session_config) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 产生 MetaData 文件 [注意，这里只能使用绝对路径]</span></span><br><span class="line">    base_path = <span class="string">'E:/Jupyter/_drafts/ipython/TensorFlow/tensorboard/'</span></span><br><span class="line">    metadata_path = base_path + <span class="string">'metadata.tsv'</span></span><br><span class="line">    <span class="keyword">if</span> tf.gfile.Exists(metadata_path):</span><br><span class="line">        tf.gfile.DeleteRecursively(metadata_path)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(metadata_path, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        labels = sess.run(tf.argmax(mnist.test.labels[:], <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(image_num):</span><br><span class="line">            f.write(<span class="built_in">str</span>(labels[i]) + <span class="string">'\n'</span>)   </span><br><span class="line"></span><br><span class="line">    writer = tf.summary.FileWriter(base_path, sess.graph)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    config = projector.ProjectorConfig()</span><br><span class="line">    embed = config.embeddings.add()</span><br><span class="line">    embed.tensor_name = embedding.name</span><br><span class="line">    embed.metadata_path = metadata_path</span><br><span class="line">    embed.sprite.image_path = base_path + <span class="string">'data/mnist_10k_sprite.png'</span></span><br><span class="line">    embed.sprite.single_image_dim.extend([<span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    projector.visualize_embeddings(writer, config)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">100</span></span><br><span class="line">    batch = (<span class="built_in">int</span>) (<span class="number">60000</span> / batch_size)</span><br><span class="line">    <span class="comment"># batch = mnist.train.num_examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里主要是为了测试 TensorBoard，所以只训练 5 次</span></span><br><span class="line">    summary_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        sess.run(tf.assign(lr, <span class="number">0.001</span> * (<span class="number">0.95</span> ** _)))</span><br><span class="line">        <span class="keyword">for</span> batch_step <span class="keyword">in</span> <span class="built_in">range</span>(batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            <span class="comment"># 真正开始生成 metadata</span></span><br><span class="line">            run_options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE)</span><br><span class="line">            run_metadata = tf.RunMetadata()</span><br><span class="line">            summary_, result = sess.run([summary_all, train_step], feed_dict = {x: batch_xs, y: batch_ys, z: <span class="number">0.997</span>}, options = run_options, run_metadata = run_metadata)</span><br><span class="line">            summary_count = summary_count + <span class="number">1</span></span><br><span class="line">            writer.add_run_metadata(run_metadata, <span class="string">'step%03d'</span> % summary_count)</span><br><span class="line">            writer.add_summary(summary_, summary_count)</span><br><span class="line"></span><br><span class="line">        test_accuracy = sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels, z: <span class="number">1.0</span>})</span><br><span class="line">        train_accuracy = sess.run(accuracy, feed_dict = {x: mnist.train.images, y: mnist.train.labels, z: <span class="number">1.0</span>})</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Batch: "</span>, _, <span class="string">"Accuracy: ["</span>, test_accuracy, <span class="string">","</span>, train_accuracy, <span class="string">"]"</span>)</span><br><span class="line"></span><br><span class="line">    saver.save(sess, base_path + <span class="string">'minst_model.ckpt'</span>, global_step = summary_count)</span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/TensorBoard.ipynb">here</a>.</p>
<h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行完，会在程序中指定的目录下生成文件 events.out.tfevents.1502692143.BENEDICT_JIN</span></span><br><span class="line">$ tensorboard --logdir=E:\Jupyter\_drafts\ipython\TensorFlow\tensorboard</span><br><span class="line">  Starting TensorBoard b<span class="string">'54'</span> at http://Benedict_Jin:6006</span><br><span class="line">  (Press CTRL+C to quit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里 windows 用户需要注意，使用 `cmd` 而不要使用 `git bash` 等工具，同时，还需要切换盘符</span></span><br><span class="line"><span class="comment"># 第二次执行，需要删除生成的文件，使用 `Kernel` - `Restart &amp; Run all` 清理缓存</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="Scalars"><a href="#Scalars" class="headerlink" title="Scalars"></a>Scalars</h5><p><img data-src="/picture/ai/ai_tensorboard_scalars.png" alt></p>
<center>（对 <a href="https://www.tensorflow.org" target="_blank" rel="external nofollow noopener noreferrer">TensorBoard</a> 可视化界面的截图）</center>


<h5 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h5><p><img data-src="/picture/ai/ai_tensorboard_graph.png" alt></p>
<center>（对 <a href="https://www.tensorflow.org" target="_blank" rel="external nofollow noopener noreferrer">TensorBoard</a> 可视化界面的截图）</center>


<h5 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h5><p><img data-src="/picture/ai/ai_tensorboard_embeddings.png" alt></p>
<center>（对 <a href="https://www.tensorflow.org" target="_blank" rel="external nofollow noopener noreferrer">TensorBoard</a> 可视化界面的截图）</center>



<h4 id="GPU-加速"><a href="#GPU-加速" class="headerlink" title="GPU 加速"></a>GPU 加速</h4><h5 id="准备-NVIDIA-显卡"><a href="#准备-NVIDIA-显卡" class="headerlink" title="准备 NVIDIA 显卡"></a>准备 NVIDIA 显卡</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">显卡2详情</span><br><span class="line">显卡名称  NVIDIA GeForce GTX 960M</span><br><span class="line">显卡厂商  英伟达</span><br><span class="line">显存大小  4095 MB</span><br><span class="line">内核名称  GeForce GTX 960M</span><br><span class="line">内核频率  324 MHz</span><br><span class="line">显存频率  405 MHz</span><br><span class="line">驱动版本  21.21.13.7651</span><br></pre></td></tr></tbody></table></figure>
<h5 id="CUDA（Compute-Unified-Device-Architecture）"><a href="#CUDA（Compute-Unified-Device-Architecture）" class="headerlink" title="CUDA（Compute Unified Device Architecture）"></a>CUDA（Compute Unified Device Architecture）</h5><p>　在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://developer.nvidia.com/cuda-downloads">下载地址</a> 找到系统对应的版本进行下载安装（<code>Windows</code> - <code>x86_64</code> - <code>10</code> - <code>exe</code> - <code>cuda_8.0.61_win10.exe</code>）</p>
<p>　安装成功后，将 <code>bin</code> 和 <code>lib/x64</code> 添加到系统 <code>PATH</code> 环境变量中</p>
<h5 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h5><p>　在 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://developer.nvidia.com/rdp/cudnn-download">登陆页面</a> 注册好 Nvidia 的账户后，下载对应 CUDA 版本的 cuDNN 即可（<code>cudnn-8.0-windows10-x64-v5.1.zip</code>）</p>
<p>　将压缩包中 <code>bin</code> / <code>include</code> / <code>lib</code> 中的文件，拷贝到 <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0</code> 下对应目录中。最后，将 <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64\cupti64_80.dll</code> 文件复制到 <code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin</code> 下</p>
<h5 id="tensorflow-gpu"><a href="#tensorflow-gpu" class="headerlink" title="tensorflow-gpu"></a>tensorflow-gpu</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ pip uninstall tensorflow</span><br><span class="line">$ pip install tensorflow-gpu</span><br><span class="line"><span class="comment"># 如果只有一颗 gpu，程序是不需要修改的，默认会直接使用该 gpu 进行运算</span></span><br><span class="line"><span class="comment"># 如果有多个，可以使用 with tf.device('/gpu:1'): 进行指定</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="nvidia-smi-命令"><a href="#nvidia-smi-命令" class="headerlink" title="nvidia-smi 命令"></a>nvidia-smi 命令</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nvidia-smi 可以查看 GPU 的资源、监控 GPU 运行状态、设置 GPU 超频 等等</span></span><br><span class="line">$ <span class="string">"C:\Program Files\NVIDIA Corporation\NVSMI\nvidia-smi.exe"</span></span><br><span class="line">  Tue Sep 19 23:33:29 2017</span><br><span class="line">  +-----------------------------------------------------------------------------+</span><br><span class="line">  | NVIDIA-SMI 385.41                 Driver Version: 385.41                    |</span><br><span class="line">  |-------------------------------+----------------------+----------------------+</span><br><span class="line">  | GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">  | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">  |===============================+======================+======================|</span><br><span class="line">  |   0  GeForce GTX 960M   WDDM  | 00000000:02:00.0 Off |                  N/A |</span><br><span class="line">  | N/A   55C    P8    N/A /  N/A |     30MiB /  4096MiB |      0%      Default |</span><br><span class="line">  +-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">  +-----------------------------------------------------------------------------+</span><br><span class="line">  | Processes:                                                       GPU Memory |</span><br><span class="line">  |  GPU       PID   Type   Process name                             Usage      |</span><br><span class="line">  |=============================================================================|</span><br><span class="line">  |  No running processes found                                                 |</span><br><span class="line">  +-----------------------------------------------------------------------------+</span><br></pre></td></tr></tbody></table></figure>
<h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><h5 id="CNN-版手写数字识别"><a href="#CNN-版手写数字识别" class="headerlink" title="CNN 版手写数字识别"></a>CNN 版手写数字识别</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.truncated_normal(shape=shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial_value=initial)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 偏置值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial_value=initial)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">x, W</span>):</span></span><br><span class="line">    <span class="comment"># x:  `  [batch, in_height, in_width, in_channels]`</span></span><br><span class="line">    <span class="comment">#        [批次大小，输入图片的长和宽，通道数（黑白：2；彩色：3）]</span></span><br><span class="line">    <span class="comment"># W: `[filter_height, filter_width, in_channels, out_channels]`</span></span><br><span class="line">    <span class="comment">#     [滤波器长，宽，输入通道数，输出通道数]</span></span><br><span class="line">    <span class="comment"># strides: `[1, stride, stride, 1]`</span></span><br><span class="line">    <span class="comment">#           [固定为 1，x/y 方向的步长，固定为 1]</span></span><br><span class="line">    <span class="comment"># padding: 是否在外部补零</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 池化层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># x:  `  [batch, in_height, in_width, in_channels]`</span></span><br><span class="line">    <span class="comment">#        [批次大小，输入图片的长和宽，通道数（黑白：2；彩色：3）]</span></span><br><span class="line">    <span class="comment"># ksize: [固定为 1，窗口大小，固定为 1]</span></span><br><span class="line">    <span class="comment"># strides: `[1, stride, stride, 1]`</span></span><br><span class="line">    <span class="comment">#           [固定为 1，x/y 方向的步长，固定为 1]</span></span><br><span class="line">    <span class="comment"># padding: 是否在外部补零</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place Holder</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># Learn Rate 学习率</span></span><br><span class="line">lr = tf.Variable(<span class="number">0.001</span>, dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 x 的转化为 4D 向量</span></span><br><span class="line"><span class="comment"># [batch, in_height, in_width, in_channels]</span></span><br><span class="line">x_image = tf.reshape(x, [-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化第一个卷积层 权值和偏置值</span></span><br><span class="line"><span class="comment"># 5*5 的采样窗口，32 个卷积核（输出 channels 数）从 1 个平面（输入 channels 数）抽取特征，获得 32 个特征平面</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line"><span class="comment"># 32 个卷积核，每个卷积核对应一个偏置值</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行卷积采样操作，并加上偏置值</span></span><br><span class="line">conv2d_1 = conv2d(x_image, W_conv1) + b_conv1</span><br><span class="line"><span class="comment"># ReLU 激活函数，获得第一个卷积层，计算得到的结果</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d_1)</span><br><span class="line"><span class="comment"># 执行 pooling 池化操作</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个卷积层 + 激活函数 + 池化层</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">conv2d_2 = conv2d(h_pool1, W_conv2) + b_conv2</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d_2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一次卷积操作后，28 * 28 图片仍然是 28 * 28</span></span><br><span class="line"><span class="comment"># 第一次池化之后，因为 2 * 2 的窗口，所以变成了 14 * 14</span></span><br><span class="line"><span class="comment"># 第二次卷积之后，仍然保持 14 * 14 的平面大小</span></span><br><span class="line"><span class="comment"># 第二次池化之后，因为 2 * 2 的窗口，所以变成了 7 * 7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层一共有 1000 个神经元，连接上一层的 7 * 7* 64 = 3136 个神经元</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1000</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把上一层的池化层，转化为 1 维（-1 代表任意值）</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [-<span class="number">1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line"><span class="comment"># 矩阵相乘，并加上偏置值</span></span><br><span class="line">wx_plus_b1 = tf.matmul(h_pool2_flat, W_fc1) + b_fc1</span><br><span class="line"><span class="comment"># ReLU 激活函数</span></span><br><span class="line">h_fc1 = tf.nn.relu(wx_plus_b1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># dropout 正则化</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个全连接层</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1000</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算输出</span></span><br><span class="line">wx_plus_b2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2</span><br><span class="line">prediction = tf.nn.softmax(wx_plus_b2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵 Loss Function</span></span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))  </span><br><span class="line"><span class="comment"># Adam 优化器，配合一个不断下降的学习率</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># argmax 方法，会返回一维张量中最大值所在的位置</span></span><br><span class="line"><span class="comment"># 计算正确率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line"></span><br><span class="line">        batch_size = <span class="number">100</span></span><br><span class="line">        batch = (<span class="built_in">int</span>) (<span class="number">60000</span> / batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">            sess.run(tf.assign(lr, <span class="number">0.0001</span> * (<span class="number">0.95</span> ** _)))</span><br><span class="line">            <span class="keyword">for</span> batch_step  <span class="keyword">in</span> <span class="built_in">range</span>(batch):</span><br><span class="line">                batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">                sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: <span class="number">0.68</span>})</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> _ % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">                test_acc = sess.run(accuracy,feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: <span class="number">1.0</span>})</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"Iterator: "</span>, _, <span class="string">"Accuracy:"</span>, test_acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  Iterator:  <span class="number">0</span> Accuracy: <span class="number">0.954</span></span><br><span class="line">  Iterator:  <span class="number">20</span> Accuracy: <span class="number">0.9912</span></span><br><span class="line">  Iterator:  <span class="number">40</span> Accuracy: <span class="number">0.9916</span></span><br><span class="line">  Iterator:  <span class="number">60</span> Accuracy: <span class="number">0.9924</span></span><br><span class="line">  Iterator:  <span class="number">80</span> Accuracy: <span class="number">0.9924</span></span><br><span class="line">  Iterator:  <span class="number">100</span> Accuracy: <span class="number">0.9926</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/MNIST_CNN.ipynb">here</a>, and Kaggle competition is <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/c/digit-recognizer/leaderboard">here</a>.</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><h5 id="LSTM-版手写数字识别"><a href="#LSTM-版手写数字识别" class="headerlink" title="LSTM 版手写数字识别"></a>LSTM 版手写数字识别</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入的图片，每张 28*28 个像素</span></span><br><span class="line">n_inputs = <span class="number">28</span>      <span class="comment"># 输入的每行有 28 个数据，输入层 神经元的个数</span></span><br><span class="line">max_time = <span class="number">28</span>      <span class="comment"># 输入的次数为 28 次</span></span><br><span class="line">lstm_size = <span class="number">100</span>    <span class="comment"># 隐藏层 block 单元</span></span><br><span class="line">n_classes = <span class="number">10</span>     <span class="comment"># 分类个数</span></span><br><span class="line">batch_size = <span class="number">50</span>    <span class="comment"># 单批次的样本数量</span></span><br><span class="line">n_batch = mnist.train.num_examples / batch_size   <span class="comment"># 一共会分成多少批次</span></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">weights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev=<span class="number">0.1</span>))</span><br><span class="line">biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[n_classes]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM</span>(<span class="params">x, weights, biases</span>):</span></span><br><span class="line">    inputs = tf.reshape(x, [-<span class="number">1</span>, max_time, n_inputs])</span><br><span class="line">    <span class="comment"># 定义隐藏层 block 单元</span></span><br><span class="line">    lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)</span><br><span class="line">    <span class="comment"># final_state[0]: cell state</span></span><br><span class="line">    <span class="comment"># final_state[1]: hidden_state</span></span><br><span class="line">    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, inputs, dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.softmax(tf.matmul(final_state[<span class="number">1</span>], weights) + biases)</span><br><span class="line"></span><br><span class="line">prediction = LSTM(x, weights, biases)</span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(prediction, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">101</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(n_batch)):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Iterator:"</span>, <span class="built_in">str</span>(epoch), <span class="string">", Accuracy:"</span>, <span class="built_in">str</span>(acc))</span><br><span class="line"></span><br><span class="line">  Iterator: <span class="number">0</span> , Accuracy: <span class="number">0.7244</span></span><br><span class="line">  Iterator: <span class="number">10</span> , Accuracy: <span class="number">0.946</span></span><br><span class="line">  Iterator: <span class="number">20</span> , Accuracy: <span class="number">0.9646</span></span><br><span class="line">  Iterator: <span class="number">30</span> , Accuracy: <span class="number">0.9696</span></span><br><span class="line">  Iterator: <span class="number">40</span> , Accuracy: <span class="number">0.9717</span></span><br><span class="line">  Iterator: <span class="number">50</span> , Accuracy: <span class="number">0.9764</span></span><br><span class="line">  Iterator: <span class="number">60</span> , Accuracy: <span class="number">0.9776</span></span><br><span class="line">  Iterator: <span class="number">70</span> , Accuracy: <span class="number">0.9801</span></span><br><span class="line">  Iterator: <span class="number">80</span> , Accuracy: <span class="number">0.9814</span></span><br><span class="line">  Iterator: <span class="number">90</span> , Accuracy: <span class="number">0.9793</span></span><br><span class="line">  Iterator: <span class="number">100</span> , Accuracy: <span class="number">0.9814</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-tensorflow/src/main/resources/ipython/MNIST_LSTM.ipynb">here</a>.</p>
<h3 id="pyTorch-pyTorch"><a href="#pyTorch-pyTorch" class="headerlink" title="pyTorch / pyTorch"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/pytorch">pyTorch</a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/pytorch/pytorch"><strong>pyTorch</strong></a></h3><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>PyTorch</strong>™ is a Python package that provides two high-level features: Tensor computation (like NumPy) with strong GPU acceleration and Deep neural networks built on a tape-based autograd system</p>
<h4 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h4><p>　到 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/peterjc123/pytorch-scripts">pytorch-scripts</a> 项目中，下载当前最新的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/pytorch/pytorch/releases">release</a> 版本 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ci.appveyor.com/api/buildjobs/9gexsv1wq91hj2hj/artifacts/output%2Ftorch-0.3.0b0%2B591e73e-cp35-cp35m-win_amd64.whl">torch-0.3.0b0+591e73e-cp35-cp35m-win_amd64.whl</a>，并<a target="_blank" rel="external nofollow noopener noreferrer" href="http://pytorch.org/#pip-install-pytorch">安装</a></p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install <span class="string">"torch-0.3.0b0+591e73e-cp35-cp35m-win_amd64.whl"</span></span><br></pre></td></tr></tbody></table></figure>
<p>Tips: Full code is <a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/asdf2014/yuzhouwan/blob/master/yuzhouwan-ai/yuzhouwan-ai-pytorch/src/main/resources/ipython/hello_world.ipynb">here</a>.</p>
<h4 id="编程实战-1"><a href="#编程实战-1" class="headerlink" title="编程实战"></a>编程实战</h4><h5 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello-world"></a>Hello-world</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t = torch.Tensor(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(t)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1.00000e-29 *</span></span><br><span class="line">  <span class="comment">#   2.7413</span></span><br><span class="line">  <span class="comment"># [torch.FloatTensor of size 1]</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="BVLC-Caffe"><a href="#BVLC-Caffe" class="headerlink" title="BVLC / Caffe"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/BVLC">BVLC</a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/BVLC/caffe"><strong>Caffe</strong></a></h3><h4 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>Caffe</strong>™ (<strong>C</strong>onvolutional <strong>A</strong>rchitecture for <strong>F</strong>ast <strong>F</strong>eature <strong>E</strong>mbedding) is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (<a target="_blank" rel="external nofollow noopener noreferrer" href="http://bair.berkeley.edu/">BAIR</a>) and by community contributors. <a target="_blank" rel="external nofollow noopener noreferrer" href="http://daggerfs.com/">Yangqing Jia</a> created the project during his PhD at UC Berkeley. Caffe is released under the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/BVLC/caffe/blob/master/LICENSE">BSD 2-Clause license</a>.</p>
<h3 id="dmlc-xgboost"><a href="#dmlc-xgboost" class="headerlink" title="dmlc / xgboost"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/dmlc">dmlc</a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/dmlc/xgboost"><strong>xgboost</strong></a></h3><h4 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>XGBoost</strong>™ is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. <a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/sb19931201/article/details/52557382">XGBoost</a> provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.</p>
<h3 id="Apache-PredictionIO"><a href="#Apache-PredictionIO" class="headerlink" title="Apache / PredictionIO"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/apache">Apache</a> / <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/apache/predictionio">PredictionIO</a></h3><h4 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>Apache PredictionIO</strong>™ is an open source Machine Learning Server built on top of a state-of-the-art open source stack for developers and data scientists to create predictive engines for any machine learning task.</p>
<h3 id="Numenta-NuPIC"><a href="#Numenta-NuPIC" class="headerlink" title="Numenta / NuPIC"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/numenta">Numenta</a> / <strong><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/numenta/nupic">NuPIC</a></strong></h3><h4 id="介绍-5"><a href="#介绍-5" class="headerlink" title="介绍"></a>介绍</h4><p>　<strong>NuPIC</strong>™<code>(Numenta Platform for Intelligent Computing, Numenta 智能计算平台)</code> 是一个与众不同的开源人工智能平台，它基于一种<code>脑皮质学习算法</code>，即 “层级实时记忆”（<strong>H</strong>ierarchical <strong>T</strong>emporal <strong>M</strong>emory, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://numenta.com/assets/pdf/whitepapers/hierarchical-temporal-memory-cortical-learning-algorithm-0.2.1-en.pdf">HTM</a>）。该算法旨在模拟新大脑皮层的工作原理，将复杂的问题转化为模式匹配与预测，而传统的 AI算法大多是针对特定的任务目标而设计的<br>　NuPIC 聚焦于分析实时数据流，可以通过学习数据之间基于时间的状态变化，对未知数据进行预测，并揭示其中的非常规特性</p>
<h5 id="特性-1"><a href="#特性-1" class="headerlink" title="特性"></a>特性</h5><ul>
<li>持续的在线学习（根据快速变化的数据流进行<strong>实时调整</strong>）</li>
<li>时间和空间分析（可以同时模拟<strong>时间</strong>和<strong>空间的变化</strong>）</li>
<li>通过<strong>通用性</strong>的大脑皮层算法，进行预测和建模</li>
<li>强大的异常检测能力（实时检测数据流的扰动，不依靠<strong>僵化的阈值</strong>设置和过时的算法）</li>
<li>层级实时存储算法</li>
</ul>
<h4 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h4><h5 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ yum install glibc-devel.i686 gcc -y</span><br><span class="line">$ python -V</span><br><span class="line">  Python 2.7.12</span><br><span class="line">$ pip -V</span><br><span class="line">  pip 9.0.1 from /usr/<span class="built_in">local</span>/lib/python2.7/site-packages (python 2.7)</span><br><span class="line">$ pip install nupic</span><br></pre></td></tr></tbody></table></figure>
<h5 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h5><p>　可能部分版本不支持，需要结合 Nupic 的 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/numenta/nupic/releases">Release</a> 和 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://pypi.python.org/pypi/nupic.bindings">PyPi 平台</a>找到合适的版本</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ pip install https://s3-us-west-2.amazonaws.com/artifacts.numenta.org/numenta/nupic.core/releases/nupic.bindings/nupic.bindings-0.4.5-cp27-none-linux_x86_64.whl</span><br><span class="line">$ pip install nupic==0.4.5</span><br></pre></td></tr></tbody></table></figure>
<h2 id="AI-到底是什么？"><a href="#AI-到底是什么？" class="headerlink" title="AI 到底是什么？"></a>AI 到底是什么？</h2><h3 id="套娃"><a href="#套娃" class="headerlink" title="套娃"></a>套娃</h3><p>　从体系组成成分的角度来讲，人工智能，包含<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Machine_learning">机器学习</a>。而机器学习，又包含了<a target="_blank" rel="external nofollow noopener noreferrer" href="https://en.wikipedia.org/wiki/Deep_learning">深度学习</a></p>
<h3 id="四象限"><a href="#四象限" class="headerlink" title="四象限"></a>四象限</h3><p>　从 “是否类人类”、“是否产生动作” 两个方向，又可以将人工智能化分为四个象限</p>
<p>　在 $x$ 轴正方向上，我们希望 AI 系统只需给出某类问题的最优解，并不用考虑是否会和人类一样去思考这个问题，例如地图导航系统，只需要算出两点的最佳路径即可。而 $x$ 轴的负方向上，则希望 AI 系统能表现出人类的思想活动，比如能听着音乐点头、摇摆的机器人，能去享受旋律；另一方面，在 $y$ 轴的正方向上，AI 系统更侧重于思考，譬如 类似 Siri™ 助理系统，可以处理人们通过语音输入的指令，并能思考出最为合理的反应。而 $y$ 轴的负方向上，则会偏向于动作的产生，比方说流水线上完成材料加工、处理工作的工业机器人</p>
<p><img data-src="/picture/ai/ai_four.png" alt></p>
<center>（使用 <a href="https://www.apple.com/cn/ipad/" target="_blank" rel="external nofollow noopener noreferrer">iPad</a>™ 手绘而成）</center>






<h2 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h2><h3 id="OverSampling-和-Data-Augmentation-的区别"><a href="#OverSampling-和-Data-Augmentation-的区别" class="headerlink" title="OverSampling 和 Data Augmentation 的区别"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://stats.stackexchange.com/questions/249142/over-sampling-for-minority-classes">OverSampling</a> 和 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/pdf/1609.08764.pdf">Data Augmentation</a> 的区别</h3><h3 id="Transfer-Learning-和-Fine-tuning-的区别"><a href="#Transfer-Learning-和-Fine-tuning-的区别" class="headerlink" title="Transfer Learning 和 Fine-tuning 的区别"></a>Transfer Learning 和 Fine-tuning 的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/49534423">区别</a></h3><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><h3 id="Doc"><a href="#Doc" class="headerlink" title="Doc"></a>Doc</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://nupic.docs.numenta.org/stable/quick-start/index.html">NuPIC Quick Start</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/lxzheng/machine_learning/wiki/TFlearn---%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">TFlearn 快速入门</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://gym.openai.com/docs/">OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://pytorch.org/tutorials/">Welcome to PyTorch Tutorials</a></li>
</ul>
<h3 id="Blog"><a href="#Blog" class="headerlink" title="Blog"></a>Blog</h3><h4 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/zouxy09">zouxy09 的专栏</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/20224890">机器学习领域有哪些著名的期刊和会议?</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/v_july_v/article/details/7624837">支持向量机通俗导论（理解 SVM 的三层境界）</a></li>
</ul>
<h4 id="人工智能"><a href="#人工智能" class="headerlink" title="人工智能"></a>人工智能</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/54884091">有哪些 AI 开源框架可供开发者使用？</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/shinanhualiu/article/details/49864219">理解 LSTM 网络</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.csdn.net/diamonjoy_zone/article/details/70904212">Deep Learning-TensorFlow（14）CNN 卷积神经网络：深度残差网络 ResNet</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.primaryobjects.com/2013/01/27/using-artificial-intelligence-to-write-self-modifying-improving-programs/">Using Artificial Intelligence to Write Self-Modifying/Improving Programs</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.infoq.com/cn/articles/cnn-and-imagenet-champion-model-analysis">CNN 浅析和历年 ImageNet 冠军模型解析</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zhuanlan.zhihu.com/p/28749411">变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.echen.me/2017/05/30/exploring-lstms/">Exploring LSTMs</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://harishnarayanan.org/writing/artistic-style-transfer/">Convolutional neural networks for artistic style transfer</a></li>
</ul>
<h4 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zhuanlan.zhihu.com/p/21498750">深度强化学习导引</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zhuanlan.zhihu.com/p/22542101">深度强化学习 强化学习概述</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://blog.greenwicher.com/2016/12/18/drl-general_ai-intro/">走向通用人工智能之路</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://zhuanlan.zhihu.com/p/21609472?refer=intelligentunit">DQN 从入门到放弃系列</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zhihu.com/question/49230922">强化学习（reinforcement learning）有什么好的开源项目、网站、文章推荐一下？</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning: Pong from Pixels</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://devblogs.nvidia.com/parallelforall/train-reinforcement-learning-agents-openai-gym/">Train Your Reinforcement Learning Agents at the OpenAI Gym</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf">David silve’s tutorial on ICML’16</a></li>
</ul>
<h4 id="胶囊网络"><a href="#胶囊网络" class="headerlink" title="胶囊网络"></a>胶囊网络</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf">Dynamic Routing Between Capsules</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b">Part I: Intuition</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66">Part II: How Capsules Work</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418">Part III: Dynamic Routing Between Capsules</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://medium.com/@pechyonkin/part-iv-capsnet-architecture-6a64422f7dce">Part IV: CapsNet Architecture</a></li>
</ul>
<h3 id="Book"><a href="#Book" class="headerlink" title="Book"></a>Book</h3><h4 id="统计学-1"><a href="#统计学-1" class="headerlink" title="统计学"></a>统计学</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/1588297/">统计学（第二版 David Freedman 等著）</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/1230154/">统计学：基本概念与方法</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/1461381738">The Making of Statisticians</a></li>
</ul>
<h4 id="微积分-1"><a href="#微积分-1" class="headerlink" title="微积分"></a>微积分</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01M28M4G6">普林斯顿微积分读本（修订版）</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/1231399/">托马斯微积分（第 11 版）</a></li>
</ul>
<h4 id="概率论-1"><a href="#概率论-1" class="headerlink" title="概率论"></a>概率论</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B019NB0UZQ">概率导论（第二版 修订版）</a></li>
</ul>
<h4 id="机器学习-2"><a href="#机器学习-2" class="headerlink" title="机器学习"></a>机器学习</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/26708119/">机器学习（周志华 著）</a> <code>又名"西瓜书"</code></li>
</ul>
<h4 id="人工智能-1"><a href="#人工智能-1" class="headerlink" title="人工智能"></a>人工智能</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/1834728/">人工智能的未来</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://exacity.github.io/deeplearningbook-chinese/">Deep Learning（Yoshua Bengio 著）</a> <code>又名"花书"</code></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://book.douban.com/subject/26976457/">Tensorflow：实战 Google 深度学习框架</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01HQHPSS8">深度学习：21 天实战 Caffe</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B01N3KU68R">深度学习：Caffe 之经典模型详解与实战</a></li>
</ul>
<h4 id="强化学习-1"><a href="#强化学习-1" class="headerlink" title="强化学习"></a>强化学习</h4><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981">Reinforcement Learning: An Introduction</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Reinforcement-Learning-TensorFlow-Keras-Python/dp/1484232844">Reinforcement Learning: With Open AI, TensorFlow and Keras Using Python</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.amazon.com/Reinforcement-Learning-State-Art-Optimization/dp/364244685X">Reinforcement Learning State-of-the-Art</a></li>
</ul>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://nbviewer.jupyter.org/github/numenta/nupic/blob/master/examples/NuPIC%20Walkthrough.ipynb">Code for “Beginner’s Guide to NuPIC”</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://web.stanford.edu/class/cs221/">Car Tracking</a></li>
</ul>
<h3 id="Course"><a href="#Course" class="headerlink" title="Course"></a>Course</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.datacamp.com/">Learn Data Science Online</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/kailashahirwar/cheatsheets-ai">Cheat Sheets for AI</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.rstudio.com/resources/cheatsheets/">RStudio Cheat Sheets</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.fast.ai/">fast.ai</a></li>
</ul>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.kaggle.com/">Your Home for Data Science</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.data.gov/">The home of the U.S. Government´s open data</a></li>
<li>NASA + ImageNet + Google + 新闻 + 自动驾驶 + 图像 + 生物 数据集 <code>(加群后免费获取)</code></li>
</ul>
<h3 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/vinta/awesome-python#awesome-python-">A curated list of awesome Python frameworks, libraries, software and resources.</a></li>
</ul>
<h3 id="Markdown"><a href="#Markdown" class="headerlink" title="Markdown"></a>Markdown</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.zybuluo.com/codeep/note/163962">Cmd Markdown 公式指导手册</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.jianshu.com/p/7bcf4ad609cf">在 Markdown 中使用 HTML 中的特殊符号</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://detexify.kirelabs.org/classify.html">Draw Markdown</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://webdemo.myscript.com/views/math/index.html">手写生成 LaTex 代码</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://support.typora.io/Draw-Diagrams-With-Markdown/">Draw Diagrams With Markdown</a></li>
</ul>
<h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf">HIERARCHICAL TEMPORAL MEMORY including HTM Cortical Learning Algorithms</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf"><strong>AlphaGo</strong>: Mastering the Game of Go with Deep Neural Networks and Tree Search</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html"><strong>AlphaGo Zero</strong>: Mastering the game of Go without human knowledge</a> <code>(加群后免费获取)</code></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/pdf/1712.01815.pdf"><strong>AlphaZero</strong>: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/junhyukoh/deep-reinforcement-learning-papers">Deep Reinforcement Learning Papers</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/aikorea/awesome-rl">Awesome Reinforcement Learning</a></li>
</ul>
<h3 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://open.163.com/special/opencourse/daishu.html">麻省理工公开课：线性代数</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://cs229.stanford.edu/"><strong>CS229</strong>: Machine Learning</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://web.stanford.edu/class/cs224n/index.html"><strong>CS224n</strong>: Natural Language Processing with Deep Learning</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=i0o-ui1N35U"><strong>CS188</strong>: Artificial Intelligence</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=aUrX-rP_ss4"><strong>CS294</strong>: Deep Reinforcement Learning</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://katefvision.github.io/"><strong>CMU10703</strong>: Deep Reinforcement Learning and Control</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://selfdrivingcars.mit.edu/"><strong>6.S094</strong>: Deep Learning for Self-Driving Cars</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">UCL Course on RL</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.coursera.org/learn/neural-networks-deep-learning">Andrew Ng: Neural Networks and Deep Learning</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/watch?v=BCwOgbSSDM4">Beginner´s Guide to NuPIC</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.youtube.com/playlist?list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9">HTM School</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/numenta/nupic/tree/master/examples/opf/clients/hotgym/anomaly/one_gym">One Hot Gym Anomaly Tutorial</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://videolectures.net/deeplearning2016_pineau_reinforcement_learning/">Introduction to Reinforcement Learning</a></li>
</ul>
<h3 id="Tool"><a href="#Tool" class="headerlink" title="Tool"></a>Tool</h3><ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.intmath.com/functions-and-graphs/graphs-using-svg.php">Online Graphing Calculator: Plot your own SVG Math Graphs</a></li>
</ul>
<h2 id="欢迎加入我们的技术群，一起交流学习"><a href="#欢迎加入我们的技术群，一起交流学习" class="headerlink" title="欢迎加入我们的技术群，一起交流学习"></a><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/asdf2014/yuzhouwan#technical-discussion-group">欢迎加入我们的技术群，一起交流学习</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">群名称</th>
<th style="text-align:center">群号</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">人工智能（高级）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=71c6bd3fb0ff01d93abca654140387d99d3be752f92a53c1fbfd27f2dd4b4247"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1020982-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">人工智能（进阶）</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=deb268f65589a1a0a1dbaf7b72c849ed45298697805bef81e0c613dea40cd05e"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1217710-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">BigData</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=f86b3c8de20da1658a3bb42df17a2fc4eee0d75c4a130a63585fdd257e3565ed"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-1670647-blue.svg" alt></a></td>
</tr>
<tr>
<td style="text-align:center">算法</td>
<td style="text-align:center"><a target="_blank" rel="external nofollow noopener noreferrer" href="https://shang.qq.com/wpa/qunwpa?idkey=bfbcf1453371a0810fd6be235ace47147f6fb9d262fb768b497c861f50af0af4"><img data-src="https://img.shields.io/badge/QQ%E7%BE%A4-5366753-blue.svg" alt></a></td>
</tr>
</tbody>
</table>
</div>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/43687/" rel="bookmark">Python：从入门到实践</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/17444/" rel="bookmark">Qcon 2015 见闻之一：猿题库</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/4735/" rel="bookmark">Real-time ML with Spark</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/19631/" rel="bookmark">如何成为 Apache 的 PMC</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/posts/200726/" rel="bookmark">梳理微积分知识体系</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Benedict Jin
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yuzhouwan.com/posts/42737/" title="人工智能">https://yuzhouwan.com/posts/42737/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-ND</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
              <a href="/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/" rel="tag"># 微积分</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/MP-Model/" rel="tag"># MP Model</a>
              <a href="/tags/Perceptron/" rel="tag"># Perceptron</a>
              <a href="/tags/MLP/" rel="tag"># MLP</a>
              <a href="/tags/DNN/" rel="tag"># DNN</a>
              <a href="/tags/CNN/" rel="tag"># CNN</a>
              <a href="/tags/RNN/" rel="tag"># RNN</a>
              <a href="/tags/LSTM/" rel="tag"># LSTM</a>
              <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" rel="tag"># 统计学</a>
              <a href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/" rel="tag"># 线性代数</a>
              <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/" rel="tag"># 概率论</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/Caffe/" rel="tag"># Caffe</a>
              <a href="/tags/xgboost/" rel="tag"># xgboost</a>
              <a href="/tags/Apache-PredictionIO/" rel="tag"># Apache PredictionIO</a>
              <a href="/tags/NuPIC/" rel="tag"># NuPIC</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/31915/" rel="prev" title="ZooKeeper 原理与优化">
      <i class="fa fa-chevron-left"></i> ZooKeeper 原理与优化
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/45888/" rel="next" title="Apache HBase 全攻略">
      Apache HBase 全攻略 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-gitalk">Gitalk：可用 Github 账号登录</a></li>
            <li class="tab"><a href="#comment-disqus">Disqus：海外</a></li>
            <li class="tab"><a href="#comment-valine">Valine：匿名</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments" id="gitalk-container"></div>
            </div>
            <div class="tab-pane disqus" id="comment-disqus">
              
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
            </div>
            <div class="tab-pane valine" id="comment-valine">
              <div class="comments" id="valine-comments"></div>
            </div>
        </div>
      </div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="nav-number">1.</span> <span class="nav-text">什么是人工智能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E6%9C%AC%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">关于本文</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%B5%81%E6%80%9D%E6%83%B3"><span class="nav-number">3.</span> <span class="nav-text">主流思想</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BC%94%E7%BB%8E%E6%B3%95-amp-%E6%BA%AF%E5%9B%A0%E6%B3%95-amp-%E5%BD%92%E7%BA%B3%E6%B3%95"><span class="nav-number">3.1.</span> <span class="nav-text">演绎法 &amp; 溯因法 &amp; 归纳法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="nav-number">4.</span> <span class="nav-text">实用技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Occam-%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86"><span class="nav-number">4.1.</span> <span class="nav-text">Occam 剃刀原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B"><span class="nav-number">4.2.</span> <span class="nav-text">大数定律</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E8%A6%81%E5%AE%9A%E4%B9%89"><span class="nav-number">4.2.1.</span> <span class="nav-text">前要定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%B1%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B"><span class="nav-number">4.2.2.</span> <span class="nav-text">弱大数定律</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B"><span class="nav-number">4.2.3.</span> <span class="nav-text">强大数定律</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6"><span class="nav-number">5.</span> <span class="nav-text">统计学</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BB%9F%E8%AE%A1%E5%AD%A6"><span class="nav-number">5.1.</span> <span class="nav-text">什么是统计学</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="nav-number">5.2.</span> <span class="nav-text">主要思想</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%80%A7-amp-%E8%A7%84%E5%BE%8B%E6%80%A7"><span class="nav-number">5.2.1.</span> <span class="nav-text">随机性 &amp; 规律性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%8F%E8%BF%B0"><span class="nav-number">5.3.</span> <span class="nav-text">数据集描述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%AD%E5%BF%83"><span class="nav-number">5.3.1.</span> <span class="nav-text">数据分布中心</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9D%87%E5%80%BC%EF%BC%88Mean%EF%BC%89"><span class="nav-number">5.3.1.1.</span> <span class="nav-text">均值（Mean）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%AD%E4%BD%8D%E6%95%B0%EF%BC%88Median%EF%BC%89"><span class="nav-number">5.3.1.2.</span> <span class="nav-text">中位数（Median）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%97%E6%95%B0%EF%BC%88Mode%EF%BC%89"><span class="nav-number">5.3.1.3.</span> <span class="nav-text">众数（Mode）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A8%B3%E5%81%A5%E7%BB%9F%E8%AE%A1"><span class="nav-number">5.3.2.</span> <span class="nav-text">稳健统计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%80%BC%E5%9F%9F%EF%BC%88Range%EF%BC%89"><span class="nav-number">5.3.2.1.</span> <span class="nav-text">值域（Range）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%9B%E5%88%86%E4%BD%8D%E6%95%B0%EF%BC%88Quartile%EF%BC%89"><span class="nav-number">5.3.2.2.</span> <span class="nav-text">四分位数（Quartile）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%9B%E5%88%86%E4%BD%8D%E8%B7%9D%EF%BC%88IQR-Interquartile-Range%EF%BC%89"><span class="nav-number">5.3.2.3.</span> <span class="nav-text">四分位距（IQR, Interquartile Range）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%9B%E5%88%86%E4%BD%8D%E5%B7%AE%EF%BC%88QD-Quartile-Deviation%EF%BC%89"><span class="nav-number">5.3.2.4.</span> <span class="nav-text">四分位差（QD, Quartile Deviation）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE"><span class="nav-number">5.3.3.</span> <span class="nav-text">偏差</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A6%BB%E5%9D%87%E5%B7%AE%EF%BC%88Deviation-from-Average%EF%BC%89"><span class="nav-number">5.3.3.1.</span> <span class="nav-text">离均差（Deviation from Average）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B9%B3%E5%9D%87%E5%81%8F%E5%B7%AE%EF%BC%88Average-Deviation%EF%BC%89"><span class="nav-number">5.3.3.2.</span> <span class="nav-text">平均偏差（Average Deviation）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%87%E5%87%86%E5%81%8F%E5%B7%AE%EF%BC%88Standard-Deviation%EF%BC%89"><span class="nav-number">5.3.3.3.</span> <span class="nav-text">标准偏差（Standard Deviation）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B4%9D%E5%A1%9E%E5%B0%94%E6%A0%A1%E6%AD%A3"><span class="nav-number">5.3.3.4.</span> <span class="nav-text">贝塞尔校正</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="nav-number">5.4.</span> <span class="nav-text">回归分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">5.4.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%84%E6%88%90"><span class="nav-number">5.4.2.</span> <span class="nav-text">组成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A7%8D%E7%B1%BB"><span class="nav-number">5.4.3.</span> <span class="nav-text">种类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E6%96%B9%E5%BC%8F"><span class="nav-number">5.4.3.1.</span> <span class="nav-text">分类方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%B1%BB%E5%9E%8B"><span class="nav-number">5.4.3.2.</span> <span class="nav-text">常见类型</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.4.3.2.1.</span> <span class="nav-text">一元线性回归</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E9%87%8D%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.4.3.2.2.</span> <span class="nav-text">多重线性回归</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">5.4.3.2.3.</span> <span class="nav-text">多元线性回归</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="nav-number">6.</span> <span class="nav-text">微积分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">6.1.</span> <span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E9%87%8F"><span class="nav-number">6.1.1.</span> <span class="nav-text">增量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B3%E8%A1%8C%E7%BA%BF-amp-%E5%9E%82%E7%9B%B4%E7%BA%BF"><span class="nav-number">6.1.2.</span> <span class="nav-text">平行线 &amp; 垂直线</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.</span> <span class="nav-text">函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="nav-number">6.2.1.</span> <span class="nav-text">定义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E9%9B%86%E5%90%88"><span class="nav-number">6.2.1.1.</span> <span class="nav-text">常用集合</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E5%9F%9F%E5%92%8C%E5%80%BC%E5%9F%9F"><span class="nav-number">6.2.1.2.</span> <span class="nav-text">常用函数的定义域和值域</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%8D%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.2.</span> <span class="nav-text">反函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%A4%8D%E5%90%88"><span class="nav-number">6.2.3.</span> <span class="nav-text">函数复合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%81%B6%E6%80%A7"><span class="nav-number">6.2.4.</span> <span class="nav-text">奇偶性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A5%87%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.4.1.</span> <span class="nav-text">奇函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%B6%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.4.2.</span> <span class="nav-text">偶函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.5.</span> <span class="nav-text">线性函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%82%B9%E6%96%9C%E5%BC%8F"><span class="nav-number">6.2.5.1.</span> <span class="nav-text">点斜式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F"><span class="nav-number">6.2.6.</span> <span class="nav-text">多项式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.7.</span> <span class="nav-text">二次函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">6.2.7.1.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3-3x-2-5x-7-0"><span class="nav-number">6.2.7.1.1.</span> <span class="nav-text">求解 $3x^2 -5x + 7 &#x3D; 0$</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%89%E7%90%86%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.8.</span> <span class="nav-text">有理函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%95%B0%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.9.</span> <span class="nav-text">指数函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.10.</span> <span class="nav-text">对数函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%A6%E7%BB%9D%E5%AF%B9%E5%80%BC%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">6.2.11.</span> <span class="nav-text">带绝对值的函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E5%AF%BC"><span class="nav-number">6.3.</span> <span class="nav-text">求导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E6%95%B0%E6%B1%82%E5%AF%BC"><span class="nav-number">6.3.1.</span> <span class="nav-text">分数求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%8D%E5%90%88%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC"><span class="nav-number">6.3.2.</span> <span class="nav-text">复合函数求导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%81%E9%99%90"><span class="nav-number">6.4.</span> <span class="nav-text">极限</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-2"><span class="nav-number">6.4.1.</span> <span class="nav-text">定义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%82%BB%E5%9F%9F"><span class="nav-number">6.4.1.1.</span> <span class="nav-text">邻域</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E6%9E%81%E9%99%90"><span class="nav-number">6.4.1.2.</span> <span class="nav-text">函数极限</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-number">7.</span> <span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-1"><span class="nav-number">7.1.</span> <span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%8C%E5%88%97%E5%BC%8F"><span class="nav-number">7.1.1.</span> <span class="nav-text">行列式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5"><span class="nav-number">7.1.2.</span> <span class="nav-text">单位矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9D%E7%AD%89%E7%9F%A9%E9%98%B5"><span class="nav-number">7.1.3.</span> <span class="nav-text">初等矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A2%9E%E5%B9%BF%E7%9F%A9%E9%98%B5"><span class="nav-number">7.1.4.</span> <span class="nav-text">增广矩阵</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7-1"><span class="nav-number">7.2.</span> <span class="nav-text">实用技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E5%85%83%E6%B3%95"><span class="nav-number">7.2.1.</span> <span class="nav-text">消元法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%AB%98%E6%96%AF-%E7%BA%A6%E5%BD%93%EF%BC%88Gauss-Jordan%EF%BC%89%E6%B6%88%E5%85%83%E6%B3%95"><span class="nav-number">7.2.2.</span> <span class="nav-text">高斯-约当（Gauss-Jordan）消元法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%8B%E6%8B%89%E9%BB%98%E6%B3%95%E5%88%99%EF%BC%88Gramer%C2%B4s-Rule%EF%BC%89"><span class="nav-number">7.2.3.</span> <span class="nav-text">克拉默法则（Gramer´s Rule）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="nav-number">8.</span> <span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E5%90%88"><span class="nav-number">8.1.</span> <span class="nav-text">集合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.2.</span> <span class="nav-text">概率模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E6%9E%84%E6%88%90"><span class="nav-number">8.2.1.</span> <span class="nav-text">主要构成</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E7%A9%BA%E9%97%B4"><span class="nav-number">8.2.1.1.</span> <span class="nav-text">样本空间</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E5%BE%8B"><span class="nav-number">8.2.1.2.</span> <span class="nav-text">概率律</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%85%AC%E7%90%86"><span class="nav-number">8.2.1.2.1.</span> <span class="nav-text">公理</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%80%A7%E8%B4%A8"><span class="nav-number">8.2.1.2.2.</span> <span class="nav-text">性质</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="nav-number">9.</span> <span class="nav-text">技术发展史</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E6%80%BB%E5%9B%BE"><span class="nav-number">9.1.</span> <span class="nav-text">机器学习发展总图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%E6%80%BB%E5%9B%BE"><span class="nav-number">9.2.</span> <span class="nav-text">深度学习发展总图</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%BA%A6%E5%8D%A1%E6%B4%9B%E5%85%8B-%E7%9A%AE%E8%8C%A8%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B%EF%BC%88McCulloch-Pitts-Neuron-Model%EF%BC%89"><span class="nav-number">9.2.1.</span> <span class="nav-text">麦卡洛克-皮茨神经元模型（McCulloch - Pitts Neuron Model）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88Perceptron%EF%BC%89"><span class="nav-number">9.2.2.</span> <span class="nav-text">感知器（Perceptron）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%EF%BC%88MLP-Multilayer-Perceptron%EF%BC%89"><span class="nav-number">9.2.3.</span> <span class="nav-text">多层感知器（MLP, Multilayer Perceptron）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88DNN-Deep-Neural-Networks%EF%BC%89"><span class="nav-number">9.2.4.</span> <span class="nav-text">深度神经网络（DNN, Deep Neural Networks）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88CNN-Convolutional-Neural-Network%EF%BC%89"><span class="nav-number">9.2.5.</span> <span class="nav-text">卷积神经网络（CNN, Convolutional Neural Network）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN-Recurrent-Neural-Networks%EF%BC%89"><span class="nav-number">9.2.6.</span> <span class="nav-text">递归神经网络（RNN, Recurrent Neural Networks）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM-Long-Short-Term-Memory-Networks%EF%BC%89"><span class="nav-number">9.2.7.</span> <span class="nav-text">长短期记忆网络（LSTM, Long Short Term Memory Networks）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">10.</span> <span class="nav-text">机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">10.1.</span> <span class="nav-text">基础概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92"><span class="nav-number">10.1.1.</span> <span class="nav-text">分类与回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="nav-number">10.2.</span> <span class="nav-text">贝叶斯定理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%8F%E8%BF%B0"><span class="nav-number">10.2.1.</span> <span class="nav-text">描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A7%8D%E7%B1%BB-1"><span class="nav-number">10.2.2.</span> <span class="nav-text">种类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">10.2.2.1.</span> <span class="nav-text">特征独立性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E6%83%85%E5%86%B5"><span class="nav-number">10.2.2.2.</span> <span class="nav-text">分布情况</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A6%BB%E6%95%A3%E7%A8%8B%E5%BA%A6"><span class="nav-number">10.2.2.3.</span> <span class="nav-text">离散程度</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%AE%9E%E6%88%98"><span class="nav-number">10.2.3.</span> <span class="nav-text">编码实战</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85"><span class="nav-number">10.2.4.</span> <span class="nav-text">补充</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E6%A6%82%E8%A6%81"><span class="nav-number">11.1.</span> <span class="nav-text">深度网络概要</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="nav-number">11.1.1.</span> <span class="nav-text">激活函数（Activation Function）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-3"><span class="nav-number">11.1.1.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A7%8D%E7%B1%BB-2"><span class="nav-number">11.1.1.2.</span> <span class="nav-text">种类</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Sigmoid"><span class="nav-number">11.1.1.2.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#TanHyperbolic%EF%BC%88Tanh%EF%BC%89"><span class="nav-number">11.1.1.2.2.</span> <span class="nav-text">TanHyperbolic（Tanh）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#ReLU-amp-Softplus"><span class="nav-number">11.1.1.2.3.</span> <span class="nav-text">ReLU &amp; Softplus</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#LReLU-amp-PReLU-amp-RReLU"><span class="nav-number">11.1.1.2.4.</span> <span class="nav-text">LReLU &amp; PReLU &amp; RReLU</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Maxout"><span class="nav-number">11.1.1.2.5.</span> <span class="nav-text">Maxout</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Swish"><span class="nav-number">11.1.1.2.6.</span> <span class="nav-text">Swish</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88Cost-Function%EF%BC%89"><span class="nav-number">11.1.2.</span> <span class="nav-text">代价函数（Cost Function）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-4"><span class="nav-number">11.1.2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A7%8D%E7%B1%BB-3"><span class="nav-number">11.1.2.2.</span> <span class="nav-text">种类</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88Quadratic-Cost%EF%BC%89"><span class="nav-number">11.1.2.2.1.</span> <span class="nav-text">二次代价函数（Quadratic Cost）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%EF%BC%88Cross-Entropy%EF%BC%89"><span class="nav-number">11.1.2.2.2.</span> <span class="nav-text">交叉熵（Cross Entropy）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%88Log-likelihood-Cost%EF%BC%89"><span class="nav-number">11.1.2.2.3.</span> <span class="nav-text">对数似然代价函数（Log-likelihood Cost）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%88Optimizer%EF%BC%89"><span class="nav-number">11.1.3.</span> <span class="nav-text">优化器（Optimizer）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">11.2.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">11.2.1.</span> <span class="nav-text">卷积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96"><span class="nav-number">11.2.2.</span> <span class="nav-text">池化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E7%B1%BB%E5%9E%8B-1"><span class="nav-number">11.2.2.1.</span> <span class="nav-text">常见类型</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Max-Pooling"><span class="nav-number">11.2.2.1.1.</span> <span class="nav-text">Max Pooling</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Mean-Pooling"><span class="nav-number">11.2.2.1.2.</span> <span class="nav-text">Mean Pooling</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#L2-norm-pooling"><span class="nav-number">11.2.2.1.3.</span> <span class="nav-text">L2-norm pooling</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Down-Pooling"><span class="nav-number">11.2.2.1.4.</span> <span class="nav-text">Down Pooling</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Padding"><span class="nav-number">11.2.3.</span> <span class="nav-text">Padding</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Same-Padding"><span class="nav-number">11.2.3.1.</span> <span class="nav-text">Same Padding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Valid-Padding"><span class="nav-number">11.2.3.2.</span> <span class="nav-text">Valid Padding</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE"><span class="nav-number">12.</span> <span class="nav-text">开源项目</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorflow-Tensorflow"><span class="nav-number">12.1.</span> <span class="nav-text">Tensorflow &#x2F; Tensorflow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">12.1.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E6%80%A7"><span class="nav-number">12.1.1.1.</span> <span class="nav-text">特性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-2"><span class="nav-number">12.1.1.2.</span> <span class="nav-text">基本概念</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">12.1.2.</span> <span class="nav-text">安装</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Python"><span class="nav-number">12.1.2.1.</span> <span class="nav-text">Python</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Anaconda3"><span class="nav-number">12.1.2.2.</span> <span class="nav-text">Anaconda3</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TensorFlow"><span class="nav-number">12.1.2.3.</span> <span class="nav-text">TensorFlow</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98"><span class="nav-number">12.1.3.</span> <span class="nav-text">编程实战</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%98%E9%87%8F"><span class="nav-number">12.1.3.1.</span> <span class="nav-text">变量</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8A%A0%E5%87%8F"><span class="nav-number">12.1.3.1.1.</span> <span class="nav-text">加减</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%B4%AF%E8%AE%A1"><span class="nav-number">12.1.3.1.2.</span> <span class="nav-text">累计</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E7%A7%AF"><span class="nav-number">12.1.3.2.</span> <span class="nav-text">矩阵积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Fetch-amp-Feed"><span class="nav-number">12.1.3.3.</span> <span class="nav-text">Fetch &amp; Feed</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Fetch"><span class="nav-number">12.1.3.3.1.</span> <span class="nav-text">Fetch</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Feed"><span class="nav-number">12.1.3.3.2.</span> <span class="nav-text">Feed</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8C%E6%AC%A1%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-amp-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">12.1.3.4.</span> <span class="nav-text">二次代价函数 &amp; 梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">12.1.3.5.</span> <span class="nav-text">非线性回归</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">12.1.3.6.</span> <span class="nav-text">手写数字识别</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorBoard-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">12.1.4.</span> <span class="nav-text">TensorBoard 可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Scalar-amp-NameScope-amp-Embedding"><span class="nav-number">12.1.4.1.</span> <span class="nav-text">Scalar &amp; NameScope &amp; Embedding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8"><span class="nav-number">12.1.4.2.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Scalars"><span class="nav-number">12.1.4.3.</span> <span class="nav-text">Scalars</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Graphs"><span class="nav-number">12.1.4.4.</span> <span class="nav-text">Graphs</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Embedding"><span class="nav-number">12.1.4.5.</span> <span class="nav-text">Embedding</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU-%E5%8A%A0%E9%80%9F"><span class="nav-number">12.1.5.</span> <span class="nav-text">GPU 加速</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%87%86%E5%A4%87-NVIDIA-%E6%98%BE%E5%8D%A1"><span class="nav-number">12.1.5.1.</span> <span class="nav-text">准备 NVIDIA 显卡</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CUDA%EF%BC%88Compute-Unified-Device-Architecture%EF%BC%89"><span class="nav-number">12.1.5.2.</span> <span class="nav-text">CUDA（Compute Unified Device Architecture）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#cuDNN"><span class="nav-number">12.1.5.3.</span> <span class="nav-text">cuDNN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tensorflow-gpu"><span class="nav-number">12.1.5.4.</span> <span class="nav-text">tensorflow-gpu</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#nvidia-smi-%E5%91%BD%E4%BB%A4"><span class="nav-number">12.1.5.5.</span> <span class="nav-text">nvidia-smi 命令</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CNN"><span class="nav-number">12.1.6.</span> <span class="nav-text">CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CNN-%E7%89%88%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">12.1.6.1.</span> <span class="nav-text">CNN 版手写数字识别</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN"><span class="nav-number">12.1.7.</span> <span class="nav-text">RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LSTM-%E7%89%88%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">12.1.7.1.</span> <span class="nav-text">LSTM 版手写数字识别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pyTorch-pyTorch"><span class="nav-number">12.2.</span> <span class="nav-text">pyTorch &#x2F; pyTorch</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-1"><span class="nav-number">12.2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-1"><span class="nav-number">12.2.2.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98-1"><span class="nav-number">12.2.3.</span> <span class="nav-text">编程实战</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Hello-world"><span class="nav-number">12.2.3.1.</span> <span class="nav-text">Hello-world</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BVLC-Caffe"><span class="nav-number">12.3.</span> <span class="nav-text">BVLC &#x2F; Caffe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-2"><span class="nav-number">12.3.1.</span> <span class="nav-text">介绍</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dmlc-xgboost"><span class="nav-number">12.4.</span> <span class="nav-text">dmlc &#x2F; xgboost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-3"><span class="nav-number">12.4.1.</span> <span class="nav-text">介绍</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Apache-PredictionIO"><span class="nav-number">12.5.</span> <span class="nav-text">Apache &#x2F; PredictionIO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-4"><span class="nav-number">12.5.1.</span> <span class="nav-text">介绍</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numenta-NuPIC"><span class="nav-number">12.6.</span> <span class="nav-text">Numenta &#x2F; NuPIC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D-5"><span class="nav-number">12.6.1.</span> <span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E6%80%A7-1"><span class="nav-number">12.6.1.1.</span> <span class="nav-text">特性</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-2"><span class="nav-number">12.6.2.</span> <span class="nav-text">安装</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Windows"><span class="nav-number">12.6.2.1.</span> <span class="nav-text">Windows</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Linux"><span class="nav-number">12.6.2.2.</span> <span class="nav-text">Linux</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AI-%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">13.</span> <span class="nav-text">AI 到底是什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A5%97%E5%A8%83"><span class="nav-number">13.1.</span> <span class="nav-text">套娃</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E8%B1%A1%E9%99%90"><span class="nav-number">13.2.</span> <span class="nav-text">四象限</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E8%AF%AF%E5%8C%BA"><span class="nav-number">14.</span> <span class="nav-text">常见误区</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OverSampling-%E5%92%8C-Data-Augmentation-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">14.1.</span> <span class="nav-text">OverSampling 和 Data Augmentation 的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transfer-Learning-%E5%92%8C-Fine-tuning-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">14.2.</span> <span class="nav-text">Transfer Learning 和 Fine-tuning 的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%96%99"><span class="nav-number">15.</span> <span class="nav-text">资料</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Doc"><span class="nav-number">15.1.</span> <span class="nav-text">Doc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Blog"><span class="nav-number">15.2.</span> <span class="nav-text">Blog</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">15.2.1.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="nav-number">15.2.2.</span> <span class="nav-text">人工智能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">15.2.3.</span> <span class="nav-text">强化学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C"><span class="nav-number">15.2.4.</span> <span class="nav-text">胶囊网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Book"><span class="nav-number">15.3.</span> <span class="nav-text">Book</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6-1"><span class="nav-number">15.3.1.</span> <span class="nav-text">统计学</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86-1"><span class="nav-number">15.3.2.</span> <span class="nav-text">微积分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA-1"><span class="nav-number">15.3.3.</span> <span class="nav-text">概率论</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2"><span class="nav-number">15.3.4.</span> <span class="nav-text">机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-1"><span class="nav-number">15.3.5.</span> <span class="nav-text">人工智能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-1"><span class="nav-number">15.3.6.</span> <span class="nav-text">强化学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Code"><span class="nav-number">15.4.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Course"><span class="nav-number">15.5.</span> <span class="nav-text">Course</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data"><span class="nav-number">15.6.</span> <span class="nav-text">Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Resource"><span class="nav-number">15.7.</span> <span class="nav-text">Resource</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Markdown"><span class="nav-number">15.8.</span> <span class="nav-text">Markdown</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Paper"><span class="nav-number">15.9.</span> <span class="nav-text">Paper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Video"><span class="nav-number">15.10.</span> <span class="nav-text">Video</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tool"><span class="nav-number">15.11.</span> <span class="nav-text">Tool</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A2%E8%BF%8E%E5%8A%A0%E5%85%A5%E6%88%91%E4%BB%AC%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BE%A4%EF%BC%8C%E4%B8%80%E8%B5%B7%E4%BA%A4%E6%B5%81%E5%AD%A6%E4%B9%A0"><span class="nav-number">16.</span> <span class="nav-text">欢迎加入我们的技术群，一起交流学习</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Benedict Jin" src="/yuzhouwan_logo_128x128.ico">
  <p class="site-author-name" itemprop="name">Benedict Jin</p>
  <div class="site-description" itemprop="description">Benedict Jin's Blog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">163</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/asdf2014" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;asdf2014" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:asdf2014@apache.org" title="E-Mail → mailto:asdf2014@apache.org" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="/images/cc-by-nc-nd.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="external nofollow noopener noreferrer" target="_blank">苏 ICP 备 17032505号 </a>
  </div>

<div class="copyright">
  
  &copy; 2014 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yuzhouwan.com</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">21:22</span>
</div>

        
<meta name="referrer" content="always">
<div class="busuanzi-count">
  <script async src="/lib/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
<script>
var options = {
  bottom: '32px', // default: '32px'
  right: '32px', // default: '32px'
  left: 'unset', // default: 'unset'
  time: '0.5s', // default: '0.3s'
  mixColor: '#fff', // default: '#fff'
  backgroundColor: '#fff',  // default: '#fff'
  buttonColorDark: '#100f2c',  // default: '#100f2c'
  buttonColorLight: '#fff', // default: '#fff'
  saveInCookies: true, // default: true,
  label: '🌓', // default: ''
  autoMatchOsTheme: false // default: true
}
const darkmode = new Darkmode(options);
darkmode.showWidget();
</script>
<style>
button.darkmode-toggle {
  z-index: 9999;
}
img, .darkmode-ignore {
  isolation: isolate;
  display: block;
}
</style>

  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  var disqus_config = function() {
    this.page.url = "https://yuzhouwan.com/posts/42737/";
    this.page.identifier = "posts/42737/";
    this.page.title = "人工智能";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://yuzhouwan.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'e5da61069b94deb8ef3a',
      clientSecret: 'c60ad4e430be258b705027a036eeee3d71cb934b',
      repo        : 'gitment',
      owner       : 'asdf2014',
      admin       : ['asdf2014'],
      id          : '4fb8cb6afb06772ba7bf280db4fdada6',
        language: '',
      distractionFreeMode: false
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'BU4bnWsdAliFfk3fz7iMcFUU-gzGzoHsz',
      appKey     : '1xVLNFSy1qGCda3wt4GiGzHG',
      placeholder: "上述信息都不是必填的，可以直接提交评论",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
